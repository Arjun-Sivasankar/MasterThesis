{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5406343-450c-480f-8e4a-61e596222d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/software/util/JupyterLab/capella/jupyterlab-4.0.4/bin/python3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00c3cd11-9cfd-4e6b-99ba-7f66eacfc9ec",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# import networkx as nx\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "249509cb-5632-4f9d-bc10-92e311bc55cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ec28998-37c4-4766-91d8-a670caa0ecf8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mimic-iv-3.1/hosp'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_path = \"/home/th748/scratch/ehr_dataset/physionet.org/files/mimiciv/2.2/hosp\"\n",
    "data_path = \"mimic-iv-3.1/hosp\"\n",
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99833579-17d1-436a-b0f4-f4144dd8c021",
   "metadata": {},
   "outputs": [],
   "source": [
    "med_file = os.path.join(data_path, \"prescriptions.csv.gz\")\n",
    "procedure_file = os.path.join(data_path, \"procedures_icd.csv.gz\")\n",
    "diag_file = os.path.join(data_path, \"diagnoses_icd.csv.gz\")\n",
    "admission_file = os.path.join(data_path, \"admissions.csv.gz\")\n",
    "lab_test_file = os.path.join(data_path, \"labevents.csv.gz\")\n",
    "patient_file = os.path.join(data_path, \"patients.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "169d5bc7-c1cf-4355-8c39-222a90d034bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drug code mapping files from GAMENet repo\n",
    "GAMENet_path = \"/data/horse/ws/arsi805e-finetune/Thesis/GAMENet\"\n",
    "ndc2atc_file = f'{GAMENet_path}/data/ndc2atc_level4.csv' \n",
    "cid_atc = f'{GAMENet_path}/data/drug-atc.csv'\n",
    "ndc2rxnorm_file = f'{GAMENet_path}/data/ndc2rxnorm_mapping.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3450bb2-9ae2-4988-8e31-c86cc913f536",
   "metadata": {},
   "outputs": [],
   "source": [
    "icd10 = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea66018a-d8d5-48e8-8060-006f6c62593f",
   "metadata": {},
   "source": [
    "# Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5af057de-0a68-4aa7-bba9-7872cd98f4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_med():\n",
    "    med_pd = pd.read_csv(med_file, dtype={'ndc':'category'})\n",
    "    med_pd.drop(columns=['pharmacy_id', 'poe_id', 'poe_seq',\n",
    "           'order_provider_id', 'stoptime', 'drug_type', 'drug',\n",
    "           'formulary_drug_cd', 'gsn', 'prod_strength', 'form_rx',\n",
    "           'dose_val_rx', 'dose_unit_rx', 'form_val_disp', 'form_unit_disp',\n",
    "           'doses_per_24_hrs', 'route'], axis=1, inplace=True)\n",
    "    med_pd.drop(index = med_pd[med_pd['ndc'] == '0'].index, axis=0, inplace=True)\n",
    "    med_pd.fillna(method='pad', inplace=True)\n",
    "    med_pd.dropna(inplace=True)\n",
    "    med_pd.drop_duplicates(inplace=True)\n",
    "    med_pd['starttime'] = pd.to_datetime(med_pd['starttime'], format='%Y-%m-%d %H:%M:%S')    \n",
    "    med_pd.sort_values(by=['subject_id', 'hadm_id', 'starttime'], inplace=True)\n",
    "    med_pd = med_pd.reset_index(drop=True)\n",
    "    def filter_first24hour_med(med_pd):\n",
    "        med_pd_new = med_pd.drop(columns=['ndc'])\n",
    "        med_pd_new = med_pd_new.groupby(by=['subject_id','hadm_id']).head(1).reset_index(drop=True)\n",
    "        med_pd_new = pd.merge(med_pd_new, med_pd, on=['subject_id','hadm_id','starttime'])\n",
    "        med_pd_new = med_pd_new.drop(columns=['starttime'])\n",
    "        return med_pd_new\n",
    "    med_pd = filter_first24hour_med(med_pd) # or next line\n",
    "    med_pd = med_pd.drop_duplicates()\n",
    "    # visit >= 2\n",
    "#     def process_visit_lg2(med_pd):\n",
    "#         a = med_pd[['subject_id', 'hadm_id']].groupby(by='subject_id')['hadm_id'].unique().reset_index()\n",
    "#         a['hadm_id_Len'] = a['hadm_id'].map(lambda x:len(x))\n",
    "#         a = a[a['hadm_id_Len'] > 1]\n",
    "#         return a \n",
    "#     med_pd_lg2 = process_visit_lg2(med_pd).reset_index(drop=True)    \n",
    "#     med_pd = med_pd.merge(med_pd_lg2[['subject_id']], on='subject_id', how='inner')    \n",
    "    return med_pd.reset_index(drop=True)\n",
    "\n",
    "def process_procedure():\n",
    "    pro_pd = pd.read_csv(procedure_file, dtype={'icd_code':'category'})\n",
    "    if icd10:\n",
    "        pro_pd = pro_pd[pro_pd['icd_version']==10]\n",
    "    else:\n",
    "        pro_pd = pro_pd[pro_pd['icd_version']==9]\n",
    "    print(\"After filtering, pro_pd['icd_version'].unique():\", pro_pd['icd_version'].unique() if 'icd_version' in pro_pd else \"No icd_version\")\n",
    "    print(\"First 5 proc codes:\", pro_pd['icd_code'].head())\n",
    "    pro_pd.drop(columns=['chartdate', 'icd_version'], inplace=True)\n",
    "    pro_pd.drop_duplicates(inplace=True)\n",
    "    pro_pd.sort_values(by=['subject_id', 'hadm_id', 'seq_num'], inplace=True)\n",
    "    pro_pd.drop(columns=['seq_num'], inplace=True)\n",
    "    pro_pd[\"icd_code\"] = \"PRO_\" + pro_pd[\"icd_code\"].astype(str)\n",
    "    pro_pd.drop_duplicates(inplace=True)\n",
    "    pro_pd.reset_index(drop=True, inplace=True)\n",
    "    return pro_pd\n",
    "\n",
    "def process_diag():\n",
    "    diag_pd = pd.read_csv(diag_file)\n",
    "    diag_pd.dropna(inplace=True)\n",
    "    ## my addidtion:\n",
    "    if icd10:\n",
    "        diag_pd = diag_pd[diag_pd['icd_version']==10]\n",
    "    else:\n",
    "        diag_pd = diag_pd[diag_pd['icd_version']==9]\n",
    "    print(\"After filtering, diag_pd['icd_version'].unique():\", diag_pd['icd_version'].unique() if 'icd_version' in diag_pd else \"No icd_version\")\n",
    "    print(\"First 5 diag codes:\", diag_pd['icd_code'].head())\n",
    "    diag_pd.drop(columns=['seq_num','icd_version'],inplace=True)\n",
    "    diag_pd.drop_duplicates(inplace=True)\n",
    "    diag_pd.sort_values(by=['subject_id','hadm_id'], inplace=True)\n",
    "    return diag_pd.reset_index(drop=True)\n",
    "\n",
    "def process_admission():\n",
    "    ad_pd = pd.read_csv(admission_file)\n",
    "    patient_pd = pd.read_csv(patient_file)\n",
    "    \n",
    "    ad_pd.drop(columns=['admission_type', 'admit_provider_id', 'admission_location',\n",
    "       'discharge_location', 'insurance', 'language', 'marital_status', 'race',\n",
    "       'edregtime', 'edouttime', 'hospital_expire_flag'], axis=1, inplace=True)\n",
    "    patient_pd.drop(columns=['anchor_year', 'anchor_year_group'], axis=1, inplace=True)\n",
    "    \n",
    "    ad_pd[\"admittime\"] = pd.to_datetime(ad_pd['admittime'], format='%Y-%m-%d %H:%M:%S')\n",
    "    ad_pd[\"dischtime\"] = pd.to_datetime(ad_pd['dischtime'], format='%Y-%m-%d %H:%M:%S')  # time for leaving hospital\n",
    "    ad_pd = ad_pd.merge(patient_pd, on=['subject_id'], how='inner')\n",
    "    \n",
    "    # create features: age, death, number of days in this encounter, readmission (next visit)\n",
    "    ad_pd[\"age\"] = ad_pd['anchor_age']\n",
    "    # ad_pd[ad_pd[\"age\"] >= 300] = 90\n",
    "    age = ad_pd[\"age\"]\n",
    "    bins = np.linspace(age.min(), age.max(), 20 + 1)\n",
    "    ad_pd['age'] = pd.cut(age, bins=bins, labels=False, include_lowest=True)\n",
    "    ad_pd['age'] = \"age_\" + ad_pd[\"age\"].astype(\"str\")\n",
    "    \n",
    "    ad_pd[\"death\"] = ad_pd[\"dod\"].notna()\n",
    "    ad_pd[\"stay_days\"] = (ad_pd[\"dischtime\"] - ad_pd[\"admittime\"]).dt.days\n",
    "    ad_pd['admittime'] = ad_pd['admittime'].astype(str)\n",
    "    ad_pd.sort_values(by=['subject_id', 'hadm_id', 'admittime'], inplace=True)\n",
    "    ad_pd['next_visit'] = ad_pd.groupby('subject_id')['hadm_id'].shift(-1)\n",
    "    ad_pd['readmission'] = ad_pd['next_visit'].notnull().astype(int)\n",
    "    ad_pd.drop('next_visit', axis=1, inplace=True)\n",
    "    ad_pd.drop(columns=['dischtime', 'dod', 'deathtime', 'anchor_age'], axis=1, inplace=True)\n",
    "    ad_pd.drop_duplicates(inplace=True)\n",
    "    return ad_pd.reset_index(drop=True)\n",
    "\n",
    "def process_lab_test(n_bins=5):\n",
    "    lab_pd = pd.read_csv(lab_test_file)\n",
    "    lab_pd = lab_pd.groupby(by=['subject_id','itemid']).head(1).reset_index(drop=True)  # only consider the first value\n",
    "    lab_pd = lab_pd[lab_pd[\"valuenum\"].notna()]\n",
    "    lab_pd = lab_pd[lab_pd[\"hadm_id\"].notna()]\n",
    "    \n",
    "    # lab_pd.drop(columns=['ROW_ID'], axis=1, inplace=True)\n",
    "    \n",
    "    def contains_text(group):\n",
    "        for item in group:\n",
    "            try:\n",
    "                float(item)\n",
    "            except ValueError:\n",
    "                return True\n",
    "        return False\n",
    "    for itemid in lab_pd['itemid'].unique():\n",
    "        group = lab_pd[lab_pd['itemid'] == itemid]['value']\n",
    "        # if the lab test contains text value then directly copy the value\n",
    "        if contains_text(group):\n",
    "            lab_pd.loc[lab_pd['itemid'] == itemid, 'value_bin'] = group\n",
    "        else:\n",
    "            # value->numeric\n",
    "            values_numeric = pd.to_numeric(group, errors='coerce')\n",
    "            if len(values_numeric.dropna()) < n_bins:\n",
    "                lab_pd.loc[lab_pd['itemid'] == itemid, 'value_bin'] = group\n",
    "            else:\n",
    "                # cut\n",
    "    #             bins = np.linspace(values_numeric.min(), values_numeric.max(), n_bins + 1)\n",
    "    #             lab_pd.loc[df['ITEMID'] == itemid, 'value_bin'] = pd.cut(values_numeric, bins=bins, labels=False, include_lowest=True)\n",
    "                lab_pd.loc[lab_pd['itemid'] == itemid, 'value_bin'] = pd.qcut(values_numeric, q=n_bins, labels=False, duplicates='drop')\n",
    "        \n",
    "    lab_pd[\"itemid\"] = lab_pd[\"itemid\"].astype(str)\n",
    "    lab_pd[\"value_bin\"] = lab_pd[\"value_bin\"].astype(str)\n",
    "    lab_pd[\"lab_test\"] = lab_pd[[\"itemid\", \"value_bin\"]].apply(\"-\".join, axis=1)\n",
    "    \n",
    "    lab_pd.drop(columns=['labevent_id', 'specimen_id', 'itemid',\n",
    "           'order_provider_id', 'charttime', 'storetime', 'value', 'valuenum',\n",
    "           'valueuom', 'ref_range_lower', 'ref_range_upper', 'flag', 'priority',\n",
    "           'comments', 'value_bin'], axis=1, inplace=True)\n",
    "    lab_pd.drop_duplicates(inplace=True)\n",
    "    return lab_pd.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "68035637-cab8-49bf-85be-a754861c8251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndc2atc4(med_pd):\n",
    "    with open(ndc2rxnorm_file, 'r') as f:\n",
    "        ndc2rxnorm = eval(f.read())\n",
    "    med_pd['RXCUI'] = med_pd['ndc'].map(ndc2rxnorm)\n",
    "    med_pd.dropna(inplace=True)\n",
    "\n",
    "    rxnorm2atc = pd.read_csv(ndc2atc_file)\n",
    "    rxnorm2atc = rxnorm2atc.drop(columns=['YEAR','MONTH','ndc'])\n",
    "    rxnorm2atc.drop_duplicates(subset=['RXCUI'], inplace=True)\n",
    "    med_pd.drop(index = med_pd[med_pd['RXCUI'].isin([''])].index, axis=0, inplace=True)\n",
    "    \n",
    "    med_pd['RXCUI'] = med_pd['RXCUI'].astype('int64')\n",
    "    med_pd = med_pd.reset_index(drop=True)\n",
    "    med_pd = med_pd.merge(rxnorm2atc, on=['RXCUI'])\n",
    "    med_pd.drop(columns=['ndc', 'RXCUI'], inplace=True)\n",
    "    med_pd = med_pd.rename(columns={'ATC4':'ndc'})\n",
    "    med_pd['ndc'] = med_pd['ndc'].map(lambda x: x[:4])\n",
    "    med_pd = med_pd.drop_duplicates()    \n",
    "    return med_pd.reset_index(drop=True)\n",
    "\n",
    "def filter_most_pro(pro_pd, threshold=800):\n",
    "    pro_count = pro_pd.groupby(by=['icd_code']).size().reset_index().rename(columns={0:'count'}).sort_values(by=['count'],ascending=False).reset_index(drop=True)\n",
    "    pro_pd = pro_pd[pro_pd['icd_code'].isin(pro_count.loc[:threshold, 'icd_code'])]\n",
    "    return pro_pd.reset_index(drop=True)\n",
    "\n",
    "def filter_most_diag(diag_pd, threshold=2000):\n",
    "    diag_count = diag_pd.groupby(by=['icd_code']).size().reset_index().rename(columns={0:'count'}).sort_values(by=['count'],ascending=False).reset_index(drop=True)\n",
    "    diag_pd = diag_pd[diag_pd['icd_code'].isin(diag_count.loc[:threshold, 'icd_code'])]\n",
    "    return diag_pd.reset_index(drop=True)\n",
    "\n",
    "def filter_most_lab(lab_pd, threshold=1500):\n",
    "    lab_count = lab_pd.groupby(by=['lab_test']).size().reset_index().rename(columns={0:'count'}).sort_values(by=['count'],ascending=False).reset_index(drop=True)\n",
    "    lab_pd = lab_pd[lab_pd['lab_test'].isin(lab_count.loc[:threshold, 'lab_test'])]\n",
    "    return lab_pd.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a323402a-3a49-4424-9134-b1a0b6e65f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "def process_all():\n",
    "    start = time.time()\n",
    "    local_time = time.ctime(start)\n",
    "    print(\"Local time:\", local_time)\n",
    "    print('\\n')\n",
    "    \n",
    "    print('process_med')\n",
    "    med_pd = process_med()\n",
    "    med_pd = ndc2atc4(med_pd)\n",
    "\n",
    "    print('process_diag')\n",
    "    diag_pd = process_diag()\n",
    "    diag_pd = filter_most_diag(diag_pd)\n",
    "    print(\"Sample diag codes:\", diag_pd['icd_code'].unique())\n",
    "\n",
    "    print('process_pro')\n",
    "    pro_pd = process_procedure()\n",
    "    pro_pd = filter_most_pro(pro_pd)\n",
    "    print(\"Sample proc codes:\", pro_pd['icd_code'].unique())\n",
    "\n",
    "    print('process_ad')\n",
    "    ad_pd = process_admission()\n",
    "    \n",
    "    print('process_lab')\n",
    "    lab_pd = process_lab_test()\n",
    "    lab_pd = filter_most_lab(lab_pd)\n",
    "\n",
    "    print(\"Processing complete....\")\n",
    "    \n",
    "    med_pd_key = med_pd[['subject_id', 'hadm_id']].drop_duplicates()\n",
    "    diag_pd_key = diag_pd[['subject_id', 'hadm_id']].drop_duplicates()\n",
    "    pro_pd_key = pro_pd[['subject_id', 'hadm_id']].drop_duplicates()\n",
    "    lab_pd_key = lab_pd[['subject_id', 'hadm_id']].drop_duplicates()\n",
    "    ad_pd_key = ad_pd[['subject_id', 'hadm_id']].drop_duplicates()\n",
    "    \n",
    "    # filter key\n",
    "    combined_key = med_pd_key.merge(diag_pd_key, on=['subject_id', 'hadm_id'], how='inner')\n",
    "    combined_key = combined_key.merge(pro_pd_key, on=['subject_id', 'hadm_id'], how='inner')\n",
    "    combined_key = combined_key.merge(lab_pd_key, on=['subject_id', 'hadm_id'], how='inner')\n",
    "    combined_key = combined_key.merge(ad_pd_key, on=['subject_id', 'hadm_id'], how='inner')\n",
    "    diag_pd = diag_pd.merge(combined_key, on=['subject_id', 'hadm_id'], how='inner')\n",
    "    med_pd = med_pd.merge(combined_key, on=['subject_id', 'hadm_id'], how='inner')\n",
    "    pro_pd = pro_pd.merge(combined_key, on=['subject_id', 'hadm_id'], how='inner')\n",
    "    lab_pd = lab_pd.merge(combined_key, on=['subject_id', 'hadm_id'], how='inner')\n",
    "    ad_pd = ad_pd.merge(combined_key, on=['subject_id', 'hadm_id'], how='inner')\n",
    "\n",
    "    # flatten and merge\n",
    "    diag_pd = diag_pd.groupby(by=['subject_id','hadm_id'])['icd_code'].unique().reset_index()  \n",
    "    med_pd = med_pd.groupby(by=['subject_id', 'hadm_id'])['ndc'].unique().reset_index()\n",
    "    pro_pd = pro_pd.groupby(by=['subject_id','hadm_id'])['icd_code'].unique().reset_index().rename(columns={'icd_code':'pro_code'})  \n",
    "    lab_pd = lab_pd.groupby(by=['subject_id','hadm_id'])['lab_test'].unique().reset_index()\n",
    "    \n",
    "    med_pd['ndc'] = med_pd['ndc'].map(lambda x: list(x))\n",
    "    pro_pd['pro_code'] = pro_pd['pro_code'].map(lambda x: list(x))\n",
    "    lab_pd['lab_test'] = lab_pd['lab_test'].map(lambda x: list(x))\n",
    "    \n",
    "    data = diag_pd.merge(med_pd, on=['subject_id', 'hadm_id'], how='inner')\n",
    "    data = data.merge(pro_pd, on=['subject_id', 'hadm_id'], how='inner')\n",
    "    data = data.merge(lab_pd, on=['subject_id', 'hadm_id'], how='inner')\n",
    "    data = data.merge(ad_pd, on=['subject_id', 'hadm_id'], how='inner')\n",
    "#     data['icd_code_Len'] = data['icd_code'].map(lambda x: len(x))\n",
    "#     data['ndc_Len'] = data['ndc'].map(lambda x: len(x))\n",
    "\n",
    "    data = data.sort_values(by=['subject_id', 'admittime'])\n",
    "    \n",
    "    # create feature: readmission within 30/90 admittime\n",
    "    data['admittime'] = pd.to_datetime(data['admittime'])\n",
    "    data['READMISSION_1M'] = data.groupby('subject_id')['admittime'].shift(-1) - data['admittime']\n",
    "    data['READMISSION_3M'] = data['READMISSION_1M'].apply(lambda x: 1 if x <= timedelta(days=90) else 0)\n",
    "    data['READMISSION_1M'] = data['READMISSION_1M'].apply(lambda x: 1 if x <= timedelta(days=30) else 0)\n",
    "    \n",
    "    # create feature: diease in next 6/12 months    \n",
    "    data['NEXT_DIAG_6M'] = data.apply(lambda x: data[(data['subject_id'] == x['subject_id']) & \n",
    "                                              (data['admittime'] > x['admittime']) & \n",
    "                                              (data['admittime'] <= x['admittime'] + timedelta(days=180))]['icd_code'].tolist(), axis=1)\n",
    "    data['NEXT_DIAG_12M'] = data.apply(lambda x: data[(data['subject_id'] == x['subject_id']) & \n",
    "                                                   (data['admittime'] > x['admittime']) & \n",
    "                                                   (data['admittime'] <= x['admittime'] + timedelta(days=365))]['icd_code'].tolist(), axis=1)\n",
    "    data['NEXT_DIAG_6M'] = data['NEXT_DIAG_6M'].apply(lambda x: x[0] if x else float('nan'))\n",
    "    data['NEXT_DIAG_12M'] = data['NEXT_DIAG_12M'].apply(lambda x: x[0] if x else float('nan'))\n",
    "    \n",
    "    data.drop(columns=['admittime'], axis=1, inplace=True)\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    print(f\"Processing time = {end - start}s\")\n",
    "    \n",
    "    return data.reset_index(drop=True), diag_pd, pro_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "064dc0b9-94c6-4320-b093-d97fbcf3d465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "# print(f\"Start time: {start}s\")\n",
    "# print('process_med')\n",
    "# med_pd = process_med()\n",
    "# med_pd = ndc2atc4(med_pd)\n",
    "\n",
    "# print('process_diag')\n",
    "# diag_pd = process_diag()\n",
    "# diag_pd = filter_most_diag(diag_pd)\n",
    "\n",
    "# print('process_pro')\n",
    "# pro_pd = process_procedure()\n",
    "# pro_pd = filter_most_pro(pro_pd)\n",
    "\n",
    "# print('process_ad')\n",
    "# ad_pd = process_admission()\n",
    "\n",
    "# print('process_lab')\n",
    "# lab_pd = process_lab_test()\n",
    "# lab_pd = filter_most_lab(lab_pd)\n",
    "\n",
    "# end = time.time()\n",
    "\n",
    "# print(f\"Processing complete.... in {end - start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "55ba45fb-c7b5-4ebb-a6fa-5612b5e3bd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# med_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d85b52c8-551e-41e3-93ed-e7dc90cc81f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diag_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8ac3812d-6730-4230-8a67-f53486360ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ad_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "092ff24f-b28e-4f54-ada0-9c006b5288e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lab_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1e90828b-624c-482b-8cbd-cbad7c126627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pro_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "97141b27-e48f-4e46-aefa-52d10f74e506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# med_pd_key = med_pd[['subject_id', 'hadm_id']].drop_duplicates()\n",
    "# diag_pd_key = diag_pd[['subject_id', 'hadm_id']].drop_duplicates()\n",
    "# pro_pd_key = pro_pd[['subject_id', 'hadm_id']].drop_duplicates()\n",
    "# lab_pd_key = lab_pd[['subject_id', 'hadm_id']].drop_duplicates()\n",
    "# ad_pd_key = ad_pd[['subject_id', 'hadm_id']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2c5849b9-8bd4-40b5-8b5f-9e29750e89ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('MED')\n",
    "# print(med_pd_key)\n",
    "# print('\\n')\n",
    "# print('DIAG')\n",
    "# print(diag_pd_key)\n",
    "# print('\\n')\n",
    "# print('PRO')\n",
    "# print(pro_pd_key)\n",
    "# print('\\n')\n",
    "# print('LAB')\n",
    "# print(lab_pd_key)\n",
    "# print('\\n')\n",
    "# print('ADMIS')\n",
    "# print(ad_pd_key)\n",
    "# print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a708a11c-afe7-4a2b-9fd4-1be8658843be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # filter key\n",
    "# combined_key = med_pd_key.merge(diag_pd_key, on=['subject_id', 'hadm_id'], how='inner')\n",
    "# combined_key = combined_key.merge(pro_pd_key, on=['subject_id', 'hadm_id'], how='inner')\n",
    "# combined_key = combined_key.merge(lab_pd_key, on=['subject_id', 'hadm_id'], how='inner')\n",
    "# combined_key = combined_key.merge(ad_pd_key, on=['subject_id', 'hadm_id'], how='inner')\n",
    "# diag_pd = diag_pd.merge(combined_key, on=['subject_id', 'hadm_id'], how='inner')\n",
    "# med_pd = med_pd.merge(combined_key, on=['subject_id', 'hadm_id'], how='inner')\n",
    "# pro_pd = pro_pd.merge(combined_key, on=['subject_id', 'hadm_id'], how='inner')\n",
    "# lab_pd = lab_pd.merge(combined_key, on=['subject_id', 'hadm_id'], how='inner')\n",
    "# ad_pd = ad_pd.merge(combined_key, on=['subject_id', 'hadm_id'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2a4dd1b0-14dc-46c8-a5af-edbc44665a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "28d91152-7119-4425-8108-d3bf2dc6fb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('MED')\n",
    "# print(med_pd)\n",
    "# print('\\n')\n",
    "# print('DIAG')\n",
    "# print(diag_pd)\n",
    "# print('\\n')\n",
    "# print('PRO')\n",
    "# print(pro_pd)\n",
    "# print('\\n')\n",
    "# print('LAB')\n",
    "# print(lab_pd)\n",
    "# print('\\n')\n",
    "# print('ADMIS')\n",
    "# print(ad_pd)\n",
    "# print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2d44738a-5640-4195-8dc8-1db7dcd00ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistics(data):\n",
    "    print('#patients ', data['subject_id'].unique().shape)\n",
    "    print('#clinical events ', len(data))\n",
    "    \n",
    "    diag = data['icd_code'].values\n",
    "    med = data['ndc'].values\n",
    "    pro = data['pro_code'].values\n",
    "    lab_test = data['lab_test'].values\n",
    "    \n",
    "    unique_diag = set([j for i in diag for j in list(i)])\n",
    "    unique_med = set([j for i in med for j in list(i)])\n",
    "    unique_pro = set([j for i in pro for j in list(i)])\n",
    "    unique_lab = set([j for i in lab_test for j in list(i)])\n",
    "    \n",
    "    print('#diagnosis ', len(unique_diag))\n",
    "    print('#med ', len(unique_med))\n",
    "    print('#procedure', len(unique_pro))\n",
    "    print('#lab', len(unique_lab))\n",
    "    \n",
    "    avg_diag = avg_med = avg_pro = avg_lab = 0\n",
    "    max_diag = max_med = max_pro = max_lab = 0\n",
    "    cnt = max_visit = avg_visit = 0\n",
    "\n",
    "    for subject_id in data['subject_id'].unique():\n",
    "        item_data = data[data['subject_id'] == subject_id]\n",
    "        x, y, z, k = [], [], [], []\n",
    "        visit_cnt = 0\n",
    "        for index, row in item_data.iterrows():\n",
    "            visit_cnt += 1\n",
    "            cnt += 1\n",
    "            x.extend(list(row['icd_code']))\n",
    "            y.extend(list(row['ndc']))\n",
    "            z.extend(list(row['pro_code']))\n",
    "            k.extend(list(row['lab_test']))\n",
    "        x, y, z, k = set(x), set(y), set(z), set(k)\n",
    "        avg_diag += len(x)\n",
    "        avg_med += len(y)\n",
    "        avg_pro += len(z)\n",
    "        avg_lab += len(k)\n",
    "        avg_visit += visit_cnt\n",
    "        if len(x) > max_diag:\n",
    "            max_diag = len(x)\n",
    "        if len(y) > max_med:\n",
    "            max_med = len(y) \n",
    "        if len(z) > max_pro:\n",
    "            max_pro = len(z)\n",
    "        if len(k) > max_lab:\n",
    "            max_lab = len(k)\n",
    "        if visit_cnt > max_visit:\n",
    "            max_visit = visit_cnt\n",
    "\n",
    "    print('#avg of diagnoses ', avg_diag/ cnt)\n",
    "    print('#avg of medicines ', avg_med/ cnt)\n",
    "    print('#avg of procedures ', avg_pro/ cnt)\n",
    "    print('#avg of lab_test ', avg_lab/ cnt)\n",
    "    print('#avg of vists ', avg_visit/ len(data['subject_id'].unique()))\n",
    "\n",
    "    print('#max of diagnoses ', max_diag)\n",
    "    print('#max of medicines ', max_med)\n",
    "    print('#max of procedures ', max_pro)\n",
    "    print('#max of lab_test ', max_lab)\n",
    "    print('#max of visit ', max_visit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bcfbea7f-e1a5-4a84-bd68-f4fabacc901c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# med_pd = pd.read_csv(med_file, dtype={'ndc':'category'})\n",
    "# med_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c603e864-e123-4790-ba56-79760735ea29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local time: Sun Jul 13 16:24:50 2025\n",
      "\n",
      "\n",
      "process_med\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15598/3092503688.py:2: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  med_pd = pd.read_csv(med_file, dtype={'ndc':'category'})\n",
      "/tmp/ipykernel_15598/3092503688.py:9: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  med_pd.fillna(method='pad', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process_diag\n",
      "After filtering, diag_pd['icd_version'].unique(): [9]\n",
      "First 5 diag codes: 0     5723\n",
      "1    78959\n",
      "2     5715\n",
      "3    07070\n",
      "4      496\n",
      "Name: icd_code, dtype: object\n",
      "Sample diag codes: ['5723' '78959' '5715' ... '78843' '4464' '20020']\n",
      "process_pro\n",
      "After filtering, pro_pd['icd_version'].unique(): [9]\n",
      "First 5 proc codes: 0    5491\n",
      "1    5491\n",
      "2    5491\n",
      "3    8938\n",
      "5    8938\n",
      "Name: icd_code, dtype: category\n",
      "Categories (14911, object): ['0009', '0010', '0011', '0012', ..., 'DW064ZZ', 'DWY17ZZ', 'XW033C6', 'XW043W5']\n",
      "Sample proc codes: ['PRO_5491' 'PRO_8938' 'PRO_5551' 'PRO_3734' 'PRO_3728' 'PRO_3727'\n",
      " 'PRO_3893' 'PRO_4524' 'PRO_7569' 'PRO_5011' 'PRO_4562' 'PRO_5459'\n",
      " 'PRO_4513' 'PRO_0066' 'PRO_3607' 'PRO_0045' 'PRO_0041' 'PRO_3722'\n",
      " 'PRO_8856' 'PRO_0044' 'PRO_8674' 'PRO_8669' 'PRO_8672' 'PRO_0139'\n",
      " 'PRO_0331' 'PRO_3897' 'PRO_7359' 'PRO_4576' 'PRO_4525' 'PRO_734'\n",
      " 'PRO_8853' 'PRO_8855' 'PRO_8696' 'PRO_7094' 'PRO_3950' 'PRO_0055'\n",
      " 'PRO_0046' 'PRO_0040' 'PRO_3615' 'PRO_3612' 'PRO_3491' 'PRO_3606'\n",
      " 'PRO_3723' 'PRO_9925' 'PRO_9920' 'PRO_8914' 'PRO_4495' 'PRO_5371'\n",
      " 'PRO_9390' 'PRO_9672' 'PRO_9604' 'PRO_3891' 'PRO_9671' 'PRO_3409'\n",
      " 'PRO_3324' 'PRO_9915' 'PRO_966' 'PRO_7915' 'PRO_5123' 'PRO_8753'\n",
      " 'PRO_3522' 'PRO_3961' 'PRO_8622' 'PRO_7902' 'PRO_741' 'PRO_6563'\n",
      " 'PRO_6841' 'PRO_8011' 'PRO_8191' 'PRO_8321' 'PRO_8659' 'PRO_8607'\n",
      " 'PRO_3931' 'PRO_3220' 'PRO_3422' 'PRO_403' 'PRO_3323' 'PRO_4442'\n",
      " 'PRO_1733' 'PRO_5362' 'PRO_5423' 'PRO_5421' 'PRO_4242' 'PRO_4639'\n",
      " 'PRO_7936' 'PRO_7931' 'PRO_8363' 'PRO_0481' 'PRO_8891' 'PRO_4852'\n",
      " 'PRO_5369' 'PRO_9977' 'PRO_8628' 'PRO_9229' 'PRO_4523' 'PRO_4881'\n",
      " 'PRO_8601' 'PRO_3995' 'PRO_5187' 'PRO_5022' 'PRO_5169' 'PRO_5137'\n",
      " 'PRO_4029' 'PRO_1742' 'PRO_6662' 'PRO_8872' 'PRO_4443' 'PRO_9648'\n",
      " 'PRO_3321' 'PRO_311' 'PRO_4311' 'PRO_3404' 'PRO_0390' 'PRO_3322'\n",
      " 'PRO_3613' 'PRO_3521' 'PRO_3611' 'PRO_7052' 'PRO_7077' 'PRO_5979'\n",
      " 'PRO_5732' 'PRO_5523' 'PRO_8051' 'PRO_6549' 'PRO_6529' 'PRO_4686'\n",
      " 'PRO_3812' 'PRO_3241' 'PRO_4467' 'PRO_8952' 'PRO_8744' 'PRO_5185'\n",
      " 'PRO_9914' 'PRO_6952' 'PRO_8107' 'PRO_7779' 'PRO_0309' 'PRO_8162'\n",
      " 'PRO_0084' 'PRO_8154' 'PRO_8345' 'PRO_5349' 'PRO_4516' 'PRO_4542'\n",
      " 'PRO_4836' 'PRO_391' 'PRO_3895' 'PRO_9907' 'PRO_6029' 'PRO_387'\n",
      " 'PRO_8851' 'PRO_7789' 'PRO_5222' 'PRO_8874' 'PRO_864' 'PRO_4011'\n",
      " 'PRO_806' 'PRO_9468' 'PRO_064' 'PRO_0695' 'PRO_8604' 'PRO_3420'\n",
      " 'PRO_3492' 'PRO_5188' 'PRO_9462' 'PRO_8611' 'PRO_3512' 'PRO_0443'\n",
      " 'PRO_7933' 'PRO_3614' 'PRO_9659' 'PRO_0159' 'PRO_0051' 'PRO_7937'\n",
      " 'PRO_8848' 'PRO_7534' 'PRO_850' 'PRO_8534' 'PRO_3963' 'PRO_8382'\n",
      " 'PRO_7769' 'PRO_0353' 'PRO_8099' 'PRO_8104' 'PRO_8105' 'PRO_8451'\n",
      " 'PRO_8163' 'PRO_8452' 'PRO_2319' 'PRO_2309' 'PRO_4131' 'PRO_5122'\n",
      " 'PRO_5361' 'PRO_4292' 'PRO_0048' 'PRO_8151' 'PRO_2751' 'PRO_3733'\n",
      " 'PRO_3929' 'PRO_4233' 'PRO_3899' 'PRO_9906' 'PRO_9904' 'PRO_3721'\n",
      " 'PRO_8964' 'PRO_9960' 'PRO_3505' 'PRO_3772' 'PRO_3783' 'PRO_9316'\n",
      " 'PRO_3971' 'PRO_8847' 'PRO_7906' 'PRO_6632' 'PRO_7309' 'PRO_7562'\n",
      " 'PRO_9465' 'PRO_9301' 'PRO_9394' 'PRO_9605' 'PRO_8843' 'PRO_9205'\n",
      " 'PRO_7051' 'PRO_843' 'PRO_8857' 'PRO_3964' 'PRO_9919' 'PRO_8006'\n",
      " 'PRO_8456' 'PRO_9357' 'PRO_0359' 'PRO_8016' 'PRO_8086' 'PRO_9921'\n",
      " 'PRO_0080' 'PRO_8457' 'PRO_8821' 'PRO_3403' 'PRO_3452' 'PRO_3845'\n",
      " 'PRO_3959' 'PRO_0391' 'PRO_8605' 'PRO_3775' 'PRO_3726' 'PRO_7749'\n",
      " 'PRO_9425' 'PRO_8344' 'PRO_7788' 'PRO_7768' 'PRO_034' 'PRO_4709'\n",
      " 'PRO_4572' 'PRO_4382' 'PRO_4497' 'PRO_0206' 'PRO_0212' 'PRO_6541'\n",
      " 'PRO_544' 'PRO_6909' 'PRO_8854' 'PRO_9607' 'PRO_4319' 'PRO_0131'\n",
      " 'PRO_598' 'PRO_8774' 'PRO_560' 'PRO_9929' 'PRO_4573' 'PRO_5359'\n",
      " 'PRO_4620' 'PRO_4651' 'PRO_2101' 'PRO_2219' 'PRO_0124' 'PRO_4581'\n",
      " 'PRO_4869' 'PRO_4601' 'PRO_7532' 'PRO_9649' 'PRO_8703' 'PRO_9936'\n",
      " 'PRO_8103' 'PRO_5211' 'PRO_3979' 'PRO_4444' 'PRO_8864' 'PRO_9910'\n",
      " 'PRO_3990' 'PRO_8849' 'PRO_8852' 'PRO_8541' 'PRO_4023' 'PRO_9427'\n",
      " 'PRO_5451' 'PRO_5343' 'PRO_9705' 'PRO_8826' 'PRO_8838' 'PRO_8015'\n",
      " 'PRO_540' 'PRO_5293' 'PRO_5029' 'PRO_5091' 'PRO_9729' 'PRO_5419'\n",
      " 'PRO_5412' 'PRO_5425' 'PRO_8876' 'PRO_9608' 'PRO_0059' 'PRO_4241'\n",
      " 'PRO_0639' 'PRO_0094' 'PRO_8893' 'PRO_8741' 'PRO_8801' 'PRO_7770'\n",
      " 'PRO_3774' 'PRO_8949' 'PRO_6849' 'PRO_6561' 'PRO_8769' 'PRO_4543'\n",
      " 'PRO_3998' 'PRO_4823' 'PRO_8152' 'PRO_5749' 'PRO_9215' 'PRO_3761'\n",
      " 'PRO_1755' 'PRO_0017' 'PRO_0047' 'PRO_0061' 'PRO_0063' 'PRO_8841'\n",
      " 'PRO_270' 'PRO_8571' 'PRO_8595' 'PRO_0221' 'PRO_0096' 'PRO_9962'\n",
      " 'PRO_3348' 'PRO_3479' 'PRO_9952' 'PRO_8395' 'PRO_5305' 'PRO_0449'\n",
      " 'PRO_4593' 'PRO_3596' 'PRO_9359' 'PRO_3972' 'PRO_9971' 'PRO_7271'\n",
      " 'PRO_9905' 'PRO_8822' 'PRO_8842' 'PRO_0407' 'PRO_863' 'PRO_7932'\n",
      " 'PRO_8314' 'PRO_7962' 'PRO_8313' 'PRO_570' 'PRO_7867' 'PRO_7767'\n",
      " 'PRO_0074' 'PRO_6902' 'PRO_5012' 'PRO_8411' 'PRO_4574' 'PRO_415'\n",
      " 'PRO_1736' 'PRO_4611' 'PRO_713' 'PRO_9723' 'PRO_7855' 'PRO_7745'\n",
      " 'PRO_8102' 'PRO_7817' 'PRO_8472' 'PRO_835' 'PRO_7868' 'PRO_8415'\n",
      " 'PRO_7935' 'PRO_4701' 'PRO_6829' 'PRO_3736' 'PRO_4946' 'PRO_4105'\n",
      " 'PRO_0091' 'PRO_9749' 'PRO_3327' 'PRO_8523' 'PRO_8844' 'PRO_8919'\n",
      " 'PRO_3320' 'PRO_3712' 'PRO_2911' 'PRO_2121' 'PRO_3142' 'PRO_8417'\n",
      " 'PRO_5411' 'PRO_3822' 'PRO_5795' 'PRO_0014' 'PRO_4438' 'PRO_8412'\n",
      " 'PRO_8385' 'PRO_8098' 'PRO_3778' 'PRO_1756' 'PRO_0077' 'PRO_3406'\n",
      " 'PRO_8845' 'PRO_8777' 'PRO_3956' 'PRO_3951' 'PRO_0039' 'PRO_0689'\n",
      " 'PRO_4824' 'PRO_8108' 'PRO_7869' 'PRO_527' 'PRO_5024' 'PRO_5114'\n",
      " 'PRO_6651' 'PRO_8339' 'PRO_8542' 'PRO_8574' 'PRO_6825' 'PRO_9449'\n",
      " 'PRO_9802' 'PRO_4603' 'PRO_8823' 'PRO_9354' 'PRO_9755' 'PRO_9803'\n",
      " 'PRO_9961' 'PRO_5059' 'PRO_0093' 'PRO_5110' 'PRO_7987' 'PRO_7916'\n",
      " 'PRO_062' 'PRO_9353' 'PRO_8183' 'PRO_7809' 'PRO_6831' 'PRO_8192'\n",
      " 'PRO_3927' 'PRO_5101' 'PRO_5121' 'PRO_4594' 'PRO_8147' 'PRO_8088'\n",
      " 'PRO_3378' 'PRO_3201' 'PRO_9969' 'PRO_6497' 'PRO_3821' 'PRO_5183'\n",
      " 'PRO_5569' 'PRO_8968' 'PRO_5184' 'PRO_7934' 'PRO_8245' 'PRO_7819'\n",
      " 'PRO_8471' 'PRO_9627' 'PRO_5794' 'PRO_5503' 'PRO_9326' 'PRO_0113'\n",
      " 'PRO_2171' 'PRO_7672' 'PRO_5198' 'PRO_8751' 'PRO_5112' 'PRO_8754'\n",
      " 'PRO_8828' 'PRO_8873' 'PRO_8365' 'PRO_8106' 'PRO_7865' 'PRO_7765'\n",
      " 'PRO_8309' 'PRO_5303' 'PRO_503' 'PRO_0293' 'PRO_3771' 'PRO_3781'\n",
      " 'PRO_370' 'PRO_9911' 'PRO_4582' 'PRO_0395' 'PRO_6951' 'PRO_3782'\n",
      " 'PRO_2181' 'PRO_427' 'PRO_0024' 'PRO_3523' 'PRO_7939' 'PRO_740'\n",
      " 'PRO_3249' 'PRO_5631' 'PRO_554' 'PRO_3451' 'PRO_3794' 'PRO_5252'\n",
      " 'PRO_9923' 'PRO_0392' 'PRO_0092' 'PRO_8866' 'PRO_4863' 'PRO_7964'\n",
      " 'PRO_8401' 'PRO_8673' 'PRO_8244' 'PRO_8623' 'PRO_0151' 'PRO_9912'\n",
      " 'PRO_8759' 'PRO_4623' 'PRO_4399' 'PRO_4685' 'PRO_4901' 'PRO_4223'\n",
      " 'PRO_6839' 'PRO_4579' 'PRO_605' 'PRO_9762' 'PRO_8775' 'PRO_5495'\n",
      " 'PRO_9782' 'PRO_8041' 'PRO_8135' 'PRO_8164' 'PRO_736' 'PRO_5674'\n",
      " 'PRO_697' 'PRO_8724' 'PRO_3768' 'PRO_9744' 'PRO_9955' 'PRO_3511'\n",
      " 'PRO_7674' 'PRO_9312' 'PRO_5351' 'PRO_3174' 'PRO_9815' 'PRO_315'\n",
      " 'PRO_3391' 'PRO_8165' 'PRO_8879' 'PRO_8076' 'PRO_8302' 'PRO_7679'\n",
      " 'PRO_7692' 'PRO_0881' 'PRO_3533' 'PRO_7813' 'PRO_6851' 'PRO_5341'\n",
      " 'PRO_4413' 'PRO_4502' 'PRO_4675' 'PRO_4591' 'PRO_3949' 'PRO_5013'\n",
      " 'PRO_4514' 'PRO_7807' 'PRO_7966' 'PRO_7857' 'PRO_4575' 'PRO_4519'\n",
      " 'PRO_3942' 'PRO_8867' 'PRO_4673' 'PRO_7761' 'PRO_7781' 'PRO_346'\n",
      " 'PRO_3421' 'PRO_8543' 'PRO_8521' 'PRO_8364' 'PRO_7971' 'PRO_4652'\n",
      " 'PRO_4642' 'PRO_3749' 'PRO_8138' 'PRO_3571' 'PRO_8944' 'PRO_7747'\n",
      " 'PRO_8081' 'PRO_3229' 'PRO_8375' 'PRO_0479' 'PRO_4104' 'PRO_5462'\n",
      " 'PRO_8036' 'PRO_9356' 'PRO_0722' 'PRO_4610' 'PRO_5504' 'PRO_586'\n",
      " 'PRO_9761' 'PRO_3199' 'PRO_8046' 'PRO_8388' 'PRO_8683' 'PRO_3816'\n",
      " 'PRO_3818' 'PRO_3932' 'PRO_0015' 'PRO_4281' 'PRO_543' 'PRO_5472'\n",
      " 'PRO_3777' 'PRO_3779' 'PRO_0042' 'PRO_5498' 'PRO_3803' 'PRO_5493'\n",
      " 'PRO_3808' 'PRO_3943' 'PRO_4641' 'PRO_8878' 'PRO_7535' 'PRO_9703'\n",
      " 'PRO_9702' 'PRO_3230' 'PRO_4024' 'PRO_8897' 'PRO_8627' 'PRO_8137'\n",
      " 'PRO_4252' 'PRO_9979' 'PRO_5593' 'PRO_5424' 'PRO_4621' 'PRO_437'\n",
      " 'PRO_9756' 'PRO_8180' 'PRO_4439' 'PRO_7676' 'PRO_184' 'PRO_7675'\n",
      " 'PRO_9355' 'PRO_7818' 'PRO_8827' 'PRO_4342' 'PRO_8945' 'PRO_4530'\n",
      " 'PRO_3974' 'PRO_3776' 'PRO_3787' 'PRO_7901' 'PRO_5733' 'PRO_8723'\n",
      " 'PRO_4432' 'PRO_3524' 'PRO_664' 'PRO_5717' 'PRO_8591' 'PRO_6525'\n",
      " 'PRO_6812' 'PRO_8819' 'PRO_7109' 'PRO_0043' 'PRO_022' 'PRO_9771'\n",
      " 'PRO_0205' 'PRO_3975' 'PRO_0060' 'PRO_9928' 'PRO_610' 'PRO_7279'\n",
      " 'PRO_0762' 'PRO_8687' 'PRO_3552' 'PRO_280' 'PRO_3886' 'PRO_5304'\n",
      " 'PRO_8201' 'PRO_6919' 'PRO_3844' 'PRO_4041' 'PRO_7965' 'PRO_8536'\n",
      " 'PRO_758' 'PRO_3789' 'PRO_3179' 'PRO_3973' 'PRO_7301' 'PRO_2103'\n",
      " 'PRO_0110' 'PRO_8743' 'PRO_7907' 'PRO_7078' 'PRO_7050' 'PRO_7095'\n",
      " 'PRO_7909' 'PRO_0294' 'PRO_4512' 'PRO_8166' 'PRO_8596' 'PRO_4719'\n",
      " 'PRO_9322' 'PRO_0202' 'PRO_7905' 'PRO_4422' 'PRO_539' 'PRO_754'\n",
      " 'PRO_6591' 'PRO_3952' 'PRO_0114' 'PRO_3725' 'PRO_043' 'PRO_5014'\n",
      " 'PRO_6859' 'PRO_4595' 'PRO_5296' 'PRO_5164' 'PRO_7975' 'PRO_0531'\n",
      " 'PRO_0234' 'PRO_8689' 'PRO_4632' 'PRO_0239' 'PRO_0050' 'PRO_5259'\n",
      " 'PRO_6759' 'PRO_0125' 'PRO_3326' 'PRO_4693' 'PRO_1735' 'PRO_8039'\n",
      " 'PRO_4674' 'PRO_8181' 'PRO_7862' 'PRO_5781' 'PRO_8554' 'PRO_2102'\n",
      " 'PRO_9227' 'PRO_7791' 'PRO_4921' 'PRO_8184' 'PRO_3401' 'PRO_7961'\n",
      " 'PRO_9344' 'PRO_0070' 'PRO_4469' 'PRO_8229' 'PRO_3514' 'PRO_9609'\n",
      " 'PRO_8532' 'PRO_7748' 'PRO_8026' 'PRO_9656' 'PRO_5212' 'PRO_594'\n",
      " 'PRO_6021' 'PRO_3925' 'PRO_8960' 'PRO_3806' 'PRO_8594' 'PRO_8553'\n",
      " 'PRO_344' 'PRO_3228' 'PRO_9741' 'PRO_4341' 'PRO_7859' 'PRO_8589'\n",
      " 'PRO_5651' 'PRO_8332' 'PRO_8609' 'PRO_0242' 'PRO_7537' 'PRO_9981'\n",
      " 'PRO_8342' 'PRO_8005' 'PRO_5136' 'PRO_2631' 'PRO_6996' 'PRO_7863'\n",
      " 'PRO_8085' 'PRO_856' 'PRO_9638' 'PRO_7785' 'PRO_5151' 'PRO_8021'\n",
      " 'PRO_5771' 'PRO_3965' 'PRO_4389' 'PRO_6816' 'PRO_8877' 'PRO_8531'\n",
      " 'PRO_5994' 'PRO_0102' 'PRO_3978']\n",
      "process_ad\n",
      "process_lab\n",
      "Processing complete....\n",
      "Processing time = 396.4784243106842s\n"
     ]
    }
   ],
   "source": [
    "data, diag_pd, pro_pd = process_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6131ce06-0209-4532-9002-e3e84eed2b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>icd_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000032</td>\n",
       "      <td>22595853</td>\n",
       "      <td>[5723, 78959, 5715, 07070, 496, 29680, 30981, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000690</td>\n",
       "      <td>25860671</td>\n",
       "      <td>[5070, 42833, 51881, 5849, 5781, 2763, 5119, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000690</td>\n",
       "      <td>26146595</td>\n",
       "      <td>[5609, 5849, 42832, 2930, 1539, 4280, 42731, 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000826</td>\n",
       "      <td>20032235</td>\n",
       "      <td>[5712, 486, 78959, 5723, 5990, 2639, 2761, 511...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000826</td>\n",
       "      <td>21086876</td>\n",
       "      <td>[5711, 99591, 78959, 2761, 5990, 5119, 5710, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78036</th>\n",
       "      <td>19999442</td>\n",
       "      <td>26785317</td>\n",
       "      <td>[34541, 43491, 431, 3485, V6284, 11284, 5990, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78037</th>\n",
       "      <td>19999565</td>\n",
       "      <td>20486927</td>\n",
       "      <td>[82101, E8859, V0382]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78038</th>\n",
       "      <td>19999840</td>\n",
       "      <td>21033226</td>\n",
       "      <td>[3453, 51881, 5070, 5180, 42741, 43821, 43811,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78039</th>\n",
       "      <td>19999840</td>\n",
       "      <td>26071774</td>\n",
       "      <td>[43491, 43820, 34590, 43811, 4019, 2724, 3051]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78040</th>\n",
       "      <td>19999987</td>\n",
       "      <td>23865745</td>\n",
       "      <td>[431, 3485, 20280, 5849, 5990, 2449, 41401, 78...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78041 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       subject_id   hadm_id                                           icd_code\n",
       "0        10000032  22595853  [5723, 78959, 5715, 07070, 496, 29680, 30981, ...\n",
       "1        10000690  25860671  [5070, 42833, 51881, 5849, 5781, 2763, 5119, 5...\n",
       "2        10000690  26146595  [5609, 5849, 42832, 2930, 1539, 4280, 42731, 7...\n",
       "3        10000826  20032235  [5712, 486, 78959, 5723, 5990, 2639, 2761, 511...\n",
       "4        10000826  21086876  [5711, 99591, 78959, 2761, 5990, 5119, 5710, 3...\n",
       "...           ...       ...                                                ...\n",
       "78036    19999442  26785317  [34541, 43491, 431, 3485, V6284, 11284, 5990, ...\n",
       "78037    19999565  20486927                              [82101, E8859, V0382]\n",
       "78038    19999840  21033226  [3453, 51881, 5070, 5180, 42741, 43821, 43811,...\n",
       "78039    19999840  26071774     [43491, 43820, 34590, 43811, 4019, 2724, 3051]\n",
       "78040    19999987  23865745  [431, 3485, 20280, 5849, 5990, 2449, 41401, 78...\n",
       "\n",
       "[78041 rows x 3 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diag_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e6b6c493-c215-4af1-ade6-694d4f37baae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>icd_code</th>\n",
       "      <th>unusual_icd_codes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [subject_id, hadm_id, icd_code, unusual_icd_codes]\n",
       "Index: []"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diag_pd[diag_pd['subject_id']==10001401]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b1da4944-9531-4173-8baf-646db5466dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with unusual codes: 0\n",
      "Sample rows with unusual codes:\n",
      "Empty DataFrame\n",
      "Columns: [subject_id, hadm_id, icd_code, unusual_icd_codes]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def find_unusual_codes(code_list):\n",
    "    # Find codes that contain letters (A-Z/a-z), but not starting with E/V (case insensitive)\n",
    "    unusual = [code for code in code_list\n",
    "               if re.match(r'[A-DF-UW-Z]', code, re.I)]  # Any code starting with letter, but not E/V\n",
    "    return unusual\n",
    "\n",
    "# Apply to the entire DataFrame\n",
    "diag_pd['unusual_icd_codes'] = diag_pd['icd_code'].apply(find_unusual_codes)\n",
    "\n",
    "# Find rows with any unusual code\n",
    "rows_with_unusual = diag_pd[diag_pd['unusual_icd_codes'].apply(lambda x: len(x) > 0)]\n",
    "\n",
    "print(f\"Number of rows with unusual codes: {len(rows_with_unusual)}\")\n",
    "print(\"Sample rows with unusual codes:\")\n",
    "print(rows_with_unusual[['subject_id', 'hadm_id', 'icd_code', 'unusual_icd_codes']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a7adc396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>icd_code</th>\n",
       "      <th>ndc</th>\n",
       "      <th>pro_code</th>\n",
       "      <th>lab_test</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>death</th>\n",
       "      <th>stay_days</th>\n",
       "      <th>readmission</th>\n",
       "      <th>READMISSION_1M</th>\n",
       "      <th>READMISSION_3M</th>\n",
       "      <th>NEXT_DIAG_6M</th>\n",
       "      <th>NEXT_DIAG_12M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000032</td>\n",
       "      <td>22595853</td>\n",
       "      <td>[5723, 78959, 5715, 07070, 496, 29680, 30981, ...</td>\n",
       "      <td>[B01A]</td>\n",
       "      <td>[PRO_5491]</td>\n",
       "      <td>[51114-1, 51120-0]</td>\n",
       "      <td>F</td>\n",
       "      <td>age_9</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000690</td>\n",
       "      <td>25860671</td>\n",
       "      <td>[5070, 42833, 51881, 5849, 5781, 2763, 5119, 5...</td>\n",
       "      <td>[A06A, B01A]</td>\n",
       "      <td>[PRO_3893]</td>\n",
       "      <td>[51009-___]</td>\n",
       "      <td>F</td>\n",
       "      <td>age_18</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000690</td>\n",
       "      <td>26146595</td>\n",
       "      <td>[5609, 5849, 42832, 2930, 1539, 4280, 42731, 7...</td>\n",
       "      <td>[A01A, A12C, N02B, N02A, A04A, J01M]</td>\n",
       "      <td>[PRO_4524]</td>\n",
       "      <td>[50900-___]</td>\n",
       "      <td>F</td>\n",
       "      <td>age_18</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000826</td>\n",
       "      <td>20032235</td>\n",
       "      <td>[5712, 486, 78959, 5723, 5990, 2639, 2761, 511...</td>\n",
       "      <td>[B01A]</td>\n",
       "      <td>[PRO_5491]</td>\n",
       "      <td>[51118-1, 50953-0, 50960-1.5, 50970-3.3, 50995...</td>\n",
       "      <td>F</td>\n",
       "      <td>age_3</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[5711, 99591, 78959, 2761, 5990, 5119, 5710, 3...</td>\n",
       "      <td>[5711, 99591, 78959, 2761, 5990, 5119, 5710, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000826</td>\n",
       "      <td>21086876</td>\n",
       "      <td>[5711, 99591, 78959, 2761, 5990, 5119, 5710, 3...</td>\n",
       "      <td>[B03B, N07B]</td>\n",
       "      <td>[PRO_5491]</td>\n",
       "      <td>[51095-___, 51105-___]</td>\n",
       "      <td>F</td>\n",
       "      <td>age_3</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[5723, 78959, 2761, 5712, 2875, 5711, 7242, 33...</td>\n",
       "      <td>[5723, 78959, 2761, 5712, 2875, 5711, 7242, 33...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78036</th>\n",
       "      <td>19999442</td>\n",
       "      <td>26785317</td>\n",
       "      <td>[34541, 43491, 431, 3485, V6284, 11284, 5990, ...</td>\n",
       "      <td>[N06A]</td>\n",
       "      <td>[PRO_9671, PRO_9604, PRO_966]</td>\n",
       "      <td>[50910-___, 50911-4, 50802-0, 50804-23, 50808-...</td>\n",
       "      <td>M</td>\n",
       "      <td>age_6</td>\n",
       "      <td>False</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78037</th>\n",
       "      <td>19999565</td>\n",
       "      <td>20486927</td>\n",
       "      <td>[82101, E8859, V0382]</td>\n",
       "      <td>[J01M, B05C, A04A, A06A, N02B]</td>\n",
       "      <td>[PRO_7935, PRO_9955]</td>\n",
       "      <td>[50893-8.6, 50960-1.9, 50970-3.1]</td>\n",
       "      <td>F</td>\n",
       "      <td>age_13</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78038</th>\n",
       "      <td>19999840</td>\n",
       "      <td>26071774</td>\n",
       "      <td>[43491, 43820, 34590, 43811, 4019, 2724, 3051]</td>\n",
       "      <td>[N02B]</td>\n",
       "      <td>[PRO_8891, PRO_8841]</td>\n",
       "      <td>[50868-16, 50882-20, 50902-106, 50908-0, 50910...</td>\n",
       "      <td>M</td>\n",
       "      <td>age_10</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[3453, 51881, 5070, 5180, 42741, 43821, 43811,...</td>\n",
       "      <td>[3453, 51881, 5070, 5180, 42741, 43821, 43811,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78039</th>\n",
       "      <td>19999840</td>\n",
       "      <td>21033226</td>\n",
       "      <td>[3453, 51881, 5070, 5180, 42741, 43821, 43811,...</td>\n",
       "      <td>[A01A, H02A, C10A]</td>\n",
       "      <td>[PRO_9604, PRO_9672, PRO_966, PRO_0331]</td>\n",
       "      <td>[51099-1, 50862-4.3, 50883-0.3, 50884-1, 50930...</td>\n",
       "      <td>M</td>\n",
       "      <td>age_10</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78040</th>\n",
       "      <td>19999987</td>\n",
       "      <td>23865745</td>\n",
       "      <td>[431, 3485, 20280, 5849, 5990, 2449, 41401, 78...</td>\n",
       "      <td>[C02D, A12C, N03A]</td>\n",
       "      <td>[PRO_8841, PRO_9671]</td>\n",
       "      <td>[50868-13, 50882-28, 50893-8.5, 50902-109, 509...</td>\n",
       "      <td>F</td>\n",
       "      <td>age_10</td>\n",
       "      <td>False</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78041 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       subject_id   hadm_id  \\\n",
       "0        10000032  22595853   \n",
       "1        10000690  25860671   \n",
       "2        10000690  26146595   \n",
       "3        10000826  20032235   \n",
       "4        10000826  21086876   \n",
       "...           ...       ...   \n",
       "78036    19999442  26785317   \n",
       "78037    19999565  20486927   \n",
       "78038    19999840  26071774   \n",
       "78039    19999840  21033226   \n",
       "78040    19999987  23865745   \n",
       "\n",
       "                                                icd_code  \\\n",
       "0      [5723, 78959, 5715, 07070, 496, 29680, 30981, ...   \n",
       "1      [5070, 42833, 51881, 5849, 5781, 2763, 5119, 5...   \n",
       "2      [5609, 5849, 42832, 2930, 1539, 4280, 42731, 7...   \n",
       "3      [5712, 486, 78959, 5723, 5990, 2639, 2761, 511...   \n",
       "4      [5711, 99591, 78959, 2761, 5990, 5119, 5710, 3...   \n",
       "...                                                  ...   \n",
       "78036  [34541, 43491, 431, 3485, V6284, 11284, 5990, ...   \n",
       "78037                              [82101, E8859, V0382]   \n",
       "78038     [43491, 43820, 34590, 43811, 4019, 2724, 3051]   \n",
       "78039  [3453, 51881, 5070, 5180, 42741, 43821, 43811,...   \n",
       "78040  [431, 3485, 20280, 5849, 5990, 2449, 41401, 78...   \n",
       "\n",
       "                                        ndc  \\\n",
       "0                                    [B01A]   \n",
       "1                              [A06A, B01A]   \n",
       "2      [A01A, A12C, N02B, N02A, A04A, J01M]   \n",
       "3                                    [B01A]   \n",
       "4                              [B03B, N07B]   \n",
       "...                                     ...   \n",
       "78036                                [N06A]   \n",
       "78037        [J01M, B05C, A04A, A06A, N02B]   \n",
       "78038                                [N02B]   \n",
       "78039                    [A01A, H02A, C10A]   \n",
       "78040                    [C02D, A12C, N03A]   \n",
       "\n",
       "                                      pro_code  \\\n",
       "0                                   [PRO_5491]   \n",
       "1                                   [PRO_3893]   \n",
       "2                                   [PRO_4524]   \n",
       "3                                   [PRO_5491]   \n",
       "4                                   [PRO_5491]   \n",
       "...                                        ...   \n",
       "78036            [PRO_9671, PRO_9604, PRO_966]   \n",
       "78037                     [PRO_7935, PRO_9955]   \n",
       "78038                     [PRO_8891, PRO_8841]   \n",
       "78039  [PRO_9604, PRO_9672, PRO_966, PRO_0331]   \n",
       "78040                     [PRO_8841, PRO_9671]   \n",
       "\n",
       "                                                lab_test gender     age  \\\n",
       "0                                     [51114-1, 51120-0]      F   age_9   \n",
       "1                                            [51009-___]      F  age_18   \n",
       "2                                            [50900-___]      F  age_18   \n",
       "3      [51118-1, 50953-0, 50960-1.5, 50970-3.3, 50995...      F   age_3   \n",
       "4                                 [51095-___, 51105-___]      F   age_3   \n",
       "...                                                  ...    ...     ...   \n",
       "78036  [50910-___, 50911-4, 50802-0, 50804-23, 50808-...      M   age_6   \n",
       "78037                  [50893-8.6, 50960-1.9, 50970-3.1]      F  age_13   \n",
       "78038  [50868-16, 50882-20, 50902-106, 50908-0, 50910...      M  age_10   \n",
       "78039  [51099-1, 50862-4.3, 50883-0.3, 50884-1, 50930...      M  age_10   \n",
       "78040  [50868-13, 50882-28, 50893-8.5, 50902-109, 509...      F  age_10   \n",
       "\n",
       "       death  stay_days  readmission  READMISSION_1M  READMISSION_3M  \\\n",
       "0       True          0            1               0               0   \n",
       "1       True          9            1               0               0   \n",
       "2       True          1            1               0               0   \n",
       "3      False          6            1               1               1   \n",
       "4      False          6            1               1               1   \n",
       "...      ...        ...          ...             ...             ...   \n",
       "78036  False         15            0               0               0   \n",
       "78037  False          3            0               0               0   \n",
       "78038   True          3            0               0               1   \n",
       "78039   True          6            1               0               0   \n",
       "78040  False          8            0               0               0   \n",
       "\n",
       "                                            NEXT_DIAG_6M  \\\n",
       "0                                                    NaN   \n",
       "1                                                    NaN   \n",
       "2                                                    NaN   \n",
       "3      [5711, 99591, 78959, 2761, 5990, 5119, 5710, 3...   \n",
       "4      [5723, 78959, 2761, 5712, 2875, 5711, 7242, 33...   \n",
       "...                                                  ...   \n",
       "78036                                                NaN   \n",
       "78037                                                NaN   \n",
       "78038  [3453, 51881, 5070, 5180, 42741, 43821, 43811,...   \n",
       "78039                                                NaN   \n",
       "78040                                                NaN   \n",
       "\n",
       "                                           NEXT_DIAG_12M  \n",
       "0                                                    NaN  \n",
       "1                                                    NaN  \n",
       "2                                                    NaN  \n",
       "3      [5711, 99591, 78959, 2761, 5990, 5119, 5710, 3...  \n",
       "4      [5723, 78959, 2761, 5712, 2875, 5711, 7242, 33...  \n",
       "...                                                  ...  \n",
       "78036                                                NaN  \n",
       "78037                                                NaN  \n",
       "78038  [3453, 51881, 5070, 5180, 42741, 43821, 43811,...  \n",
       "78039                                                NaN  \n",
       "78040                                                NaN  \n",
       "\n",
       "[78041 rows x 15 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "815ee7d6-fdb4-482a-a5b2-e2e868617a6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>icd_code</th>\n",
       "      <th>ndc</th>\n",
       "      <th>pro_code</th>\n",
       "      <th>lab_test</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>death</th>\n",
       "      <th>stay_days</th>\n",
       "      <th>readmission</th>\n",
       "      <th>READMISSION_1M</th>\n",
       "      <th>READMISSION_3M</th>\n",
       "      <th>NEXT_DIAG_6M</th>\n",
       "      <th>NEXT_DIAG_12M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000032</td>\n",
       "      <td>22595853</td>\n",
       "      <td>[5723, 78959, 5715, 07070, 496, 29680, 30981, ...</td>\n",
       "      <td>[B01A]</td>\n",
       "      <td>[PRO_5491]</td>\n",
       "      <td>[51114-1, 51120-0]</td>\n",
       "      <td>F</td>\n",
       "      <td>age_9</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000690</td>\n",
       "      <td>25860671</td>\n",
       "      <td>[5070, 42833, 51881, 5849, 5781, 2763, 5119, 5...</td>\n",
       "      <td>[A06A, B01A]</td>\n",
       "      <td>[PRO_3893]</td>\n",
       "      <td>[51009-___]</td>\n",
       "      <td>F</td>\n",
       "      <td>age_18</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000690</td>\n",
       "      <td>26146595</td>\n",
       "      <td>[5609, 5849, 42832, 2930, 1539, 4280, 42731, 7...</td>\n",
       "      <td>[A01A, A12C, N02B, N02A, A04A, J01M]</td>\n",
       "      <td>[PRO_4524]</td>\n",
       "      <td>[50900-___]</td>\n",
       "      <td>F</td>\n",
       "      <td>age_18</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000826</td>\n",
       "      <td>20032235</td>\n",
       "      <td>[5712, 486, 78959, 5723, 5990, 2639, 2761, 511...</td>\n",
       "      <td>[B01A]</td>\n",
       "      <td>[PRO_5491]</td>\n",
       "      <td>[51118-1, 50953-0, 50960-1.5, 50970-3.3, 50995...</td>\n",
       "      <td>F</td>\n",
       "      <td>age_3</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[5711, 99591, 78959, 2761, 5990, 5119, 5710, 3...</td>\n",
       "      <td>[5711, 99591, 78959, 2761, 5990, 5119, 5710, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000826</td>\n",
       "      <td>21086876</td>\n",
       "      <td>[5711, 99591, 78959, 2761, 5990, 5119, 5710, 3...</td>\n",
       "      <td>[B03B, N07B]</td>\n",
       "      <td>[PRO_5491]</td>\n",
       "      <td>[51095-___, 51105-___]</td>\n",
       "      <td>F</td>\n",
       "      <td>age_3</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[5723, 78959, 2761, 5712, 2875, 5711, 7242, 33...</td>\n",
       "      <td>[5723, 78959, 2761, 5712, 2875, 5711, 7242, 33...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10000826</td>\n",
       "      <td>28289260</td>\n",
       "      <td>[5723, 78959, 2761, 5712, 2875, 5711, 7242, 33...</td>\n",
       "      <td>[N05B, N02A]</td>\n",
       "      <td>[PRO_5491]</td>\n",
       "      <td>[51114-1, 51067-___, 51094-1]</td>\n",
       "      <td>F</td>\n",
       "      <td>age_3</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10000935</td>\n",
       "      <td>25849114</td>\n",
       "      <td>[1970, 34830, 1977, 1539, 2762, 5780, 2869, 27...</td>\n",
       "      <td>[N02B, A06A]</td>\n",
       "      <td>[PRO_5011]</td>\n",
       "      <td>[50963-___, 50802--2, 50804-22, 50818-33, 5082...</td>\n",
       "      <td>F</td>\n",
       "      <td>age_9</td>\n",
       "      <td>True</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10000980</td>\n",
       "      <td>26913865</td>\n",
       "      <td>[41071, 42823, 41412, 5854, 4240, 4280, 41401,...</td>\n",
       "      <td>[A12C, V03A, A02B, C03C, C10A, B01A, C08C, H04...</td>\n",
       "      <td>[PRO_0066, PRO_3607, PRO_0045, PRO_0041, PRO_3...</td>\n",
       "      <td>[50863-61, 50883-0.1, 50884-0, 50885-0.3]</td>\n",
       "      <td>F</td>\n",
       "      <td>age_15</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10000980</td>\n",
       "      <td>25242409</td>\n",
       "      <td>[45342, 53551, 5854, 25040, 5849, 3572, 42832,...</td>\n",
       "      <td>[C07A, B01A, C08C, N06A, M04A, N02B, A06A, C10...</td>\n",
       "      <td>[PRO_4513]</td>\n",
       "      <td>[50810-25]</td>\n",
       "      <td>F</td>\n",
       "      <td>age_15</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10001217</td>\n",
       "      <td>24597018</td>\n",
       "      <td>[3484, 3485, 5180, 340, 04109, 3051, 4019, V16...</td>\n",
       "      <td>[A06A, N03A, N02A, N02B]</td>\n",
       "      <td>[PRO_0139, PRO_0331, PRO_3897]</td>\n",
       "      <td>[52264-___, 52272-0, 52281-___, 52286-___, 509...</td>\n",
       "      <td>F</td>\n",
       "      <td>age_10</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10001338</td>\n",
       "      <td>22119639</td>\n",
       "      <td>[56211, 5849, 5695, 99859, 6822, 04111, E8786,...</td>\n",
       "      <td>[D07A, N06A]</td>\n",
       "      <td>[PRO_4576]</td>\n",
       "      <td>[51237-1.1, 51274-___, 51275-28.4, 51143-0, 51...</td>\n",
       "      <td>F</td>\n",
       "      <td>age_6</td>\n",
       "      <td>False</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10001492</td>\n",
       "      <td>27463908</td>\n",
       "      <td>[41071, 42983, 2449, 4240]</td>\n",
       "      <td>[H03A]</td>\n",
       "      <td>[PRO_3722, PRO_8853, PRO_8855]</td>\n",
       "      <td>[50908-3, 50903-1, 51000-___, 50893-8.4, 50960...</td>\n",
       "      <td>F</td>\n",
       "      <td>age_14</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10001725</td>\n",
       "      <td>25563031</td>\n",
       "      <td>[78829, 9950, 49390, 53081, 30000, 311, 7291, ...</td>\n",
       "      <td>[N05B, M01A, D07A, C07A, C03B, J01D, N02A, C01...</td>\n",
       "      <td>[PRO_8696, PRO_7094]</td>\n",
       "      <td>[50868-14, 50882-24, 50893-9.1, 50912-0.8, 509...</td>\n",
       "      <td>F</td>\n",
       "      <td>age_7</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10002013</td>\n",
       "      <td>21975601</td>\n",
       "      <td>[99672, 42832, 4111, 5849, 41401, E8781, 4280,...</td>\n",
       "      <td>[N02B, H04A, B05C, R03A, A02B]</td>\n",
       "      <td>[PRO_0066, PRO_3607, PRO_3722, PRO_0045, PRO_0...</td>\n",
       "      <td>[51482-1]</td>\n",
       "      <td>F</td>\n",
       "      <td>age_9</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[41401, 42832, 5180, 4280, 4139, 4400, 4019, 3...</td>\n",
       "      <td>[41401, 42832, 5180, 4280, 4139, 4400, 4019, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10002013</td>\n",
       "      <td>23581541</td>\n",
       "      <td>[41401, 42832, 5180, 4280, 4139, 4400, 4019, 3...</td>\n",
       "      <td>[C10A, B01A]</td>\n",
       "      <td>[PRO_3615, PRO_3612]</td>\n",
       "      <td>[50802-0, 50804-27, 50806-101, 50808-1.16, 508...</td>\n",
       "      <td>F</td>\n",
       "      <td>age_9</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10002155</td>\n",
       "      <td>28439444</td>\n",
       "      <td>[43491, 41400, V4581, 2720, 3051]</td>\n",
       "      <td>[C10A, B01A]</td>\n",
       "      <td>[PRO_8914]</td>\n",
       "      <td>[50903-2, 50904-44, 50993-2.4, 50995-2, 51000-...</td>\n",
       "      <td>F</td>\n",
       "      <td>age_16</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10002155</td>\n",
       "      <td>23822395</td>\n",
       "      <td>[41011, 486, 42821, 41402, 99672, 7455, 1628, ...</td>\n",
       "      <td>[B01A, C10A, A01A]</td>\n",
       "      <td>[PRO_0066, PRO_3606, PRO_3723, PRO_8856, PRO_0...</td>\n",
       "      <td>[50908-4, 50953-2]</td>\n",
       "      <td>F</td>\n",
       "      <td>age_16</td>\n",
       "      <td>True</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10002155</td>\n",
       "      <td>20345487</td>\n",
       "      <td>[1628, 486, 51884, 5781, 2851, 2761, 49121, 41...</td>\n",
       "      <td>[C10A, B03B]</td>\n",
       "      <td>[PRO_3491]</td>\n",
       "      <td>[50808-1.11, 51479-1, 51482-1, 51453-___]</td>\n",
       "      <td>F</td>\n",
       "      <td>age_16</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10002428</td>\n",
       "      <td>28662225</td>\n",
       "      <td>[0383, 78552, 5184, 5845, 34831, 486, 51881, 0...</td>\n",
       "      <td>[A02B, H03A]</td>\n",
       "      <td>[PRO_9604, PRO_9671, PRO_3893, PRO_3891, PRO_3...</td>\n",
       "      <td>[50817-95, 50802-1, 50804-29, 50818-49, 50820-...</td>\n",
       "      <td>F</td>\n",
       "      <td>age_16</td>\n",
       "      <td>False</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[03843, 51881, 42843, 5990, 00845, 99591, 4280...</td>\n",
       "      <td>[03843, 51881, 42843, 5990, 00845, 99591, 4280...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10002428</td>\n",
       "      <td>23473524</td>\n",
       "      <td>[03843, 51881, 42843, 5990, 00845, 99591, 4280...</td>\n",
       "      <td>[H03A]</td>\n",
       "      <td>[PRO_9672, PRO_9604, PRO_3891, PRO_3897]</td>\n",
       "      <td>[51283-1.4]</td>\n",
       "      <td>F</td>\n",
       "      <td>age_16</td>\n",
       "      <td>False</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    subject_id   hadm_id                                           icd_code  \\\n",
       "0     10000032  22595853  [5723, 78959, 5715, 07070, 496, 29680, 30981, ...   \n",
       "1     10000690  25860671  [5070, 42833, 51881, 5849, 5781, 2763, 5119, 5...   \n",
       "2     10000690  26146595  [5609, 5849, 42832, 2930, 1539, 4280, 42731, 7...   \n",
       "3     10000826  20032235  [5712, 486, 78959, 5723, 5990, 2639, 2761, 511...   \n",
       "4     10000826  21086876  [5711, 99591, 78959, 2761, 5990, 5119, 5710, 3...   \n",
       "5     10000826  28289260  [5723, 78959, 2761, 5712, 2875, 5711, 7242, 33...   \n",
       "6     10000935  25849114  [1970, 34830, 1977, 1539, 2762, 5780, 2869, 27...   \n",
       "7     10000980  26913865  [41071, 42823, 41412, 5854, 4240, 4280, 41401,...   \n",
       "8     10000980  25242409  [45342, 53551, 5854, 25040, 5849, 3572, 42832,...   \n",
       "9     10001217  24597018  [3484, 3485, 5180, 340, 04109, 3051, 4019, V16...   \n",
       "10    10001338  22119639  [56211, 5849, 5695, 99859, 6822, 04111, E8786,...   \n",
       "11    10001492  27463908                         [41071, 42983, 2449, 4240]   \n",
       "12    10001725  25563031  [78829, 9950, 49390, 53081, 30000, 311, 7291, ...   \n",
       "13    10002013  21975601  [99672, 42832, 4111, 5849, 41401, E8781, 4280,...   \n",
       "14    10002013  23581541  [41401, 42832, 5180, 4280, 4139, 4400, 4019, 3...   \n",
       "15    10002155  28439444                  [43491, 41400, V4581, 2720, 3051]   \n",
       "16    10002155  23822395  [41011, 486, 42821, 41402, 99672, 7455, 1628, ...   \n",
       "17    10002155  20345487  [1628, 486, 51884, 5781, 2851, 2761, 49121, 41...   \n",
       "18    10002428  28662225  [0383, 78552, 5184, 5845, 34831, 486, 51881, 0...   \n",
       "19    10002428  23473524  [03843, 51881, 42843, 5990, 00845, 99591, 4280...   \n",
       "\n",
       "                                                  ndc  \\\n",
       "0                                              [B01A]   \n",
       "1                                        [A06A, B01A]   \n",
       "2                [A01A, A12C, N02B, N02A, A04A, J01M]   \n",
       "3                                              [B01A]   \n",
       "4                                        [B03B, N07B]   \n",
       "5                                        [N05B, N02A]   \n",
       "6                                        [N02B, A06A]   \n",
       "7   [A12C, V03A, A02B, C03C, C10A, B01A, C08C, H04...   \n",
       "8   [C07A, B01A, C08C, N06A, M04A, N02B, A06A, C10...   \n",
       "9                            [A06A, N03A, N02A, N02B]   \n",
       "10                                       [D07A, N06A]   \n",
       "11                                             [H03A]   \n",
       "12  [N05B, M01A, D07A, C07A, C03B, J01D, N02A, C01...   \n",
       "13                     [N02B, H04A, B05C, R03A, A02B]   \n",
       "14                                       [C10A, B01A]   \n",
       "15                                       [C10A, B01A]   \n",
       "16                                 [B01A, C10A, A01A]   \n",
       "17                                       [C10A, B03B]   \n",
       "18                                       [A02B, H03A]   \n",
       "19                                             [H03A]   \n",
       "\n",
       "                                             pro_code  \\\n",
       "0                                          [PRO_5491]   \n",
       "1                                          [PRO_3893]   \n",
       "2                                          [PRO_4524]   \n",
       "3                                          [PRO_5491]   \n",
       "4                                          [PRO_5491]   \n",
       "5                                          [PRO_5491]   \n",
       "6                                          [PRO_5011]   \n",
       "7   [PRO_0066, PRO_3607, PRO_0045, PRO_0041, PRO_3...   \n",
       "8                                          [PRO_4513]   \n",
       "9                      [PRO_0139, PRO_0331, PRO_3897]   \n",
       "10                                         [PRO_4576]   \n",
       "11                     [PRO_3722, PRO_8853, PRO_8855]   \n",
       "12                               [PRO_8696, PRO_7094]   \n",
       "13  [PRO_0066, PRO_3607, PRO_3722, PRO_0045, PRO_0...   \n",
       "14                               [PRO_3615, PRO_3612]   \n",
       "15                                         [PRO_8914]   \n",
       "16  [PRO_0066, PRO_3606, PRO_3723, PRO_8856, PRO_0...   \n",
       "17                                         [PRO_3491]   \n",
       "18  [PRO_9604, PRO_9671, PRO_3893, PRO_3891, PRO_3...   \n",
       "19           [PRO_9672, PRO_9604, PRO_3891, PRO_3897]   \n",
       "\n",
       "                                             lab_test gender     age  death  \\\n",
       "0                                  [51114-1, 51120-0]      F   age_9   True   \n",
       "1                                         [51009-___]      F  age_18   True   \n",
       "2                                         [50900-___]      F  age_18   True   \n",
       "3   [51118-1, 50953-0, 50960-1.5, 50970-3.3, 50995...      F   age_3  False   \n",
       "4                              [51095-___, 51105-___]      F   age_3  False   \n",
       "5                       [51114-1, 51067-___, 51094-1]      F   age_3  False   \n",
       "6   [50963-___, 50802--2, 50804-22, 50818-33, 5082...      F   age_9   True   \n",
       "7           [50863-61, 50883-0.1, 50884-0, 50885-0.3]      F  age_15   True   \n",
       "8                                          [50810-25]      F  age_15   True   \n",
       "9   [52264-___, 52272-0, 52281-___, 52286-___, 509...      F  age_10  False   \n",
       "10  [51237-1.1, 51274-___, 51275-28.4, 51143-0, 51...      F   age_6  False   \n",
       "11  [50908-3, 50903-1, 51000-___, 50893-8.4, 50960...      F  age_14  False   \n",
       "12  [50868-14, 50882-24, 50893-9.1, 50912-0.8, 509...      F   age_7  False   \n",
       "13                                          [51482-1]      F   age_9  False   \n",
       "14  [50802-0, 50804-27, 50806-101, 50808-1.16, 508...      F   age_9  False   \n",
       "15  [50903-2, 50904-44, 50993-2.4, 50995-2, 51000-...      F  age_16   True   \n",
       "16                                 [50908-4, 50953-2]      F  age_16   True   \n",
       "17          [50808-1.11, 51479-1, 51482-1, 51453-___]      F  age_16   True   \n",
       "18  [50817-95, 50802-1, 50804-29, 50818-49, 50820-...      F  age_16  False   \n",
       "19                                        [51283-1.4]      F  age_16  False   \n",
       "\n",
       "    stay_days  readmission  READMISSION_1M  READMISSION_3M  \\\n",
       "0           0            1               0               0   \n",
       "1           9            1               0               0   \n",
       "2           1            1               0               0   \n",
       "3           6            1               1               1   \n",
       "4           6            1               1               1   \n",
       "5           2            0               0               0   \n",
       "6          15            1               0               0   \n",
       "7           5            1               0               0   \n",
       "8           7            1               0               0   \n",
       "9           6            1               0               0   \n",
       "10         17            1               0               0   \n",
       "11          1            0               0               0   \n",
       "12          2            0               0               0   \n",
       "13          2            1               0               0   \n",
       "14          5            1               0               0   \n",
       "15          2            1               0               0   \n",
       "16         14            1               0               0   \n",
       "17          0            1               0               0   \n",
       "18         17            1               1               1   \n",
       "19         10            1               0               0   \n",
       "\n",
       "                                         NEXT_DIAG_6M  \\\n",
       "0                                                 NaN   \n",
       "1                                                 NaN   \n",
       "2                                                 NaN   \n",
       "3   [5711, 99591, 78959, 2761, 5990, 5119, 5710, 3...   \n",
       "4   [5723, 78959, 2761, 5712, 2875, 5711, 7242, 33...   \n",
       "5                                                 NaN   \n",
       "6                                                 NaN   \n",
       "7                                                 NaN   \n",
       "8                                                 NaN   \n",
       "9                                                 NaN   \n",
       "10                                                NaN   \n",
       "11                                                NaN   \n",
       "12                                                NaN   \n",
       "13  [41401, 42832, 5180, 4280, 4139, 4400, 4019, 3...   \n",
       "14                                                NaN   \n",
       "15                                                NaN   \n",
       "16                                                NaN   \n",
       "17                                                NaN   \n",
       "18  [03843, 51881, 42843, 5990, 00845, 99591, 4280...   \n",
       "19                                                NaN   \n",
       "\n",
       "                                        NEXT_DIAG_12M  \n",
       "0                                                 NaN  \n",
       "1                                                 NaN  \n",
       "2                                                 NaN  \n",
       "3   [5711, 99591, 78959, 2761, 5990, 5119, 5710, 3...  \n",
       "4   [5723, 78959, 2761, 5712, 2875, 5711, 7242, 33...  \n",
       "5                                                 NaN  \n",
       "6                                                 NaN  \n",
       "7                                                 NaN  \n",
       "8                                                 NaN  \n",
       "9                                                 NaN  \n",
       "10                                                NaN  \n",
       "11                                                NaN  \n",
       "12                                                NaN  \n",
       "13  [41401, 42832, 5180, 4280, 4139, 4400, 4019, 3...  \n",
       "14                                                NaN  \n",
       "15                                                NaN  \n",
       "16                                                NaN  \n",
       "17                                                NaN  \n",
       "18  [03843, 51881, 42843, 5990, 00845, 99591, 4280...  \n",
       "19                                                NaN  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e2a99ac4-fa20-4048-8e7c-d36c00c5b301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample codes in final merged data: ['5723' '78959' '5715' '07070' '496' '29680' '30981' 'V1582' '5070'\n",
      " '42833' '51881' '5849' '5781' '2763' '5119' '5180' 'V850' '4280' '42731'\n",
      " '42781']\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample codes in final merged data:\", data['icd_code'].explode().unique()[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4cf9925e-00f0-4d15-89b5-7f468aa4e9a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subject_id                                                 10001725\n",
       "hadm_id                                                    25563031\n",
       "icd_code          [78829, 9950, 49390, 53081, 30000, 311, 7291, ...\n",
       "ndc               [N05B, M01A, D07A, C07A, C03B, J01D, N02A, C01...\n",
       "pro_code                                       [PRO_8696, PRO_7094]\n",
       "lab_test          [50868-14, 50882-24, 50893-9.1, 50912-0.8, 509...\n",
       "gender                                                            F\n",
       "age                                                           age_7\n",
       "death                                                         False\n",
       "stay_days                                                         2\n",
       "readmission                                                       0\n",
       "READMISSION_1M                                                    0\n",
       "READMISSION_3M                                                    0\n",
       "NEXT_DIAG_6M                                                    NaN\n",
       "NEXT_DIAG_12M                                                   NaN\n",
       "Name: 12, dtype: object"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "129743a6-a13a-45c6-9784-a4494b68bbf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#patients  (58894,)\n",
      "#clinical events  78041\n",
      "#diagnosis  2001\n",
      "#med  140\n",
      "#procedure 801\n",
      "#lab 1387\n",
      "#avg of diagnoses  10.567137786548097\n",
      "#avg of medicines  3.017093579016158\n",
      "#avg of procedures  2.9668507579349317\n",
      "#avg of lab_test  10.313130277674556\n",
      "#avg of vists  1.3251095187964819\n",
      "#max of diagnoses  114\n",
      "#max of medicines  27\n",
      "#max of procedures  41\n",
      "#max of lab_test  71\n",
      "#max of visit  12\n"
     ]
    }
   ],
   "source": [
    "statistics(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1515d807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICD flag:  False\n",
      "Data stored in: <_io.BufferedWriter name='./dataset/mimic.pkl'>\n"
     ]
    }
   ],
   "source": [
    "print(\"ICD flag: \", icd10)\n",
    "if icd10:\n",
    "    with open(\"./dataset/mimic_icd10.pkl\", \"wb\") as outfile:\n",
    "        pickle.dump(data, outfile)\n",
    "else:\n",
    "    with open(\"./dataset/mimic.pkl\", \"wb\") as outfile:\n",
    "        pickle.dump(data, outfile)\n",
    "\n",
    "print(f\"Data stored in: {outfile}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377f2c71-fc38-4eac-be3c-dc8607291480",
   "metadata": {},
   "source": [
    "# Dataset Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2127798-1b2e-4363-9fa6-8012c87942dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mimic.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['subject_id', 'hadm_id', 'icd_code', 'ndc', 'pro_code', 'lab_test',\n",
       "       'gender', 'age', 'death', 'stay_days', 'readmission', 'READMISSION_1M',\n",
       "       'READMISSION_3M', 'NEXT_DIAG_6M', 'NEXT_DIAG_12M'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if icd10:\n",
    "    dataset = \"mimic_icd10.pkl\"\n",
    "else:\n",
    "    dataset = \"mimic.pkl\"\n",
    "print(dataset)\n",
    "mimic_data = pickle.load(open(\"dataset/mimic.pkl\", 'rb'))\n",
    "mimic_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b13001a-aed7-499c-9799-a9c78c8c29bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>icd_code</th>\n",
       "      <th>ndc</th>\n",
       "      <th>pro_code</th>\n",
       "      <th>lab_test</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>death</th>\n",
       "      <th>stay_days</th>\n",
       "      <th>readmission</th>\n",
       "      <th>READMISSION_1M</th>\n",
       "      <th>READMISSION_3M</th>\n",
       "      <th>NEXT_DIAG_6M</th>\n",
       "      <th>NEXT_DIAG_12M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000032</td>\n",
       "      <td>22595853</td>\n",
       "      <td>[5723, 78959, 5715, 07070, 496, 29680, 30981, ...</td>\n",
       "      <td>[B01A]</td>\n",
       "      <td>[PRO_5491]</td>\n",
       "      <td>[51114-1, 51120-0]</td>\n",
       "      <td>F</td>\n",
       "      <td>age_9</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000690</td>\n",
       "      <td>25860671</td>\n",
       "      <td>[5070, 42833, 51881, 5849, 5781, 2763, 5119, 5...</td>\n",
       "      <td>[A06A, B01A]</td>\n",
       "      <td>[PRO_3893]</td>\n",
       "      <td>[51009-___]</td>\n",
       "      <td>F</td>\n",
       "      <td>age_18</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000690</td>\n",
       "      <td>26146595</td>\n",
       "      <td>[5609, 5849, 42832, 2930, 1539, 4280, 42731, 7...</td>\n",
       "      <td>[A01A, A12C, N02B, N02A, A04A, J01M]</td>\n",
       "      <td>[PRO_4524]</td>\n",
       "      <td>[50900-___]</td>\n",
       "      <td>F</td>\n",
       "      <td>age_18</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000826</td>\n",
       "      <td>20032235</td>\n",
       "      <td>[5712, 486, 78959, 5723, 5990, 2639, 2761, 511...</td>\n",
       "      <td>[B01A]</td>\n",
       "      <td>[PRO_5491]</td>\n",
       "      <td>[51118-1, 50953-0, 50960-1.5, 50970-3.3, 50995...</td>\n",
       "      <td>F</td>\n",
       "      <td>age_3</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[5711, 99591, 78959, 2761, 5990, 5119, 5710, 3...</td>\n",
       "      <td>[5711, 99591, 78959, 2761, 5990, 5119, 5710, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000826</td>\n",
       "      <td>21086876</td>\n",
       "      <td>[5711, 99591, 78959, 2761, 5990, 5119, 5710, 3...</td>\n",
       "      <td>[B03B, N07B]</td>\n",
       "      <td>[PRO_5491]</td>\n",
       "      <td>[51095-___, 51105-___]</td>\n",
       "      <td>F</td>\n",
       "      <td>age_3</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[5723, 78959, 2761, 5712, 2875, 5711, 7242, 33...</td>\n",
       "      <td>[5723, 78959, 2761, 5712, 2875, 5711, 7242, 33...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10000826</td>\n",
       "      <td>28289260</td>\n",
       "      <td>[5723, 78959, 2761, 5712, 2875, 5711, 7242, 33...</td>\n",
       "      <td>[N05B, N02A]</td>\n",
       "      <td>[PRO_5491]</td>\n",
       "      <td>[51114-1, 51067-___, 51094-1]</td>\n",
       "      <td>F</td>\n",
       "      <td>age_3</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10000935</td>\n",
       "      <td>25849114</td>\n",
       "      <td>[1970, 34830, 1977, 1539, 2762, 5780, 2869, 27...</td>\n",
       "      <td>[N02B, A06A]</td>\n",
       "      <td>[PRO_5011]</td>\n",
       "      <td>[50963-___, 50802--2, 50804-22, 50818-33, 5082...</td>\n",
       "      <td>F</td>\n",
       "      <td>age_9</td>\n",
       "      <td>True</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10000980</td>\n",
       "      <td>26913865</td>\n",
       "      <td>[41071, 42823, 41412, 5854, 4240, 4280, 41401,...</td>\n",
       "      <td>[A12C, V03A, A02B, C03C, C10A, B01A, C08C, H04...</td>\n",
       "      <td>[PRO_0066, PRO_3607, PRO_0045, PRO_0041, PRO_3...</td>\n",
       "      <td>[50863-61, 50883-0.1, 50884-0, 50885-0.3]</td>\n",
       "      <td>F</td>\n",
       "      <td>age_15</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10000980</td>\n",
       "      <td>25242409</td>\n",
       "      <td>[45342, 53551, 5854, 25040, 5849, 3572, 42832,...</td>\n",
       "      <td>[C07A, B01A, C08C, N06A, M04A, N02B, A06A, C10...</td>\n",
       "      <td>[PRO_4513]</td>\n",
       "      <td>[50810-25]</td>\n",
       "      <td>F</td>\n",
       "      <td>age_15</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10001217</td>\n",
       "      <td>24597018</td>\n",
       "      <td>[3484, 3485, 5180, 340, 04109, 3051, 4019, V16...</td>\n",
       "      <td>[A06A, N03A, N02A, N02B]</td>\n",
       "      <td>[PRO_0139, PRO_0331, PRO_3897]</td>\n",
       "      <td>[52264-___, 52272-0, 52281-___, 52286-___, 509...</td>\n",
       "      <td>F</td>\n",
       "      <td>age_10</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10001338</td>\n",
       "      <td>22119639</td>\n",
       "      <td>[56211, 5849, 5695, 99859, 6822, 04111, E8786,...</td>\n",
       "      <td>[D07A, N06A]</td>\n",
       "      <td>[PRO_4576]</td>\n",
       "      <td>[51237-1.1, 51274-___, 51275-28.4, 51143-0, 51...</td>\n",
       "      <td>F</td>\n",
       "      <td>age_6</td>\n",
       "      <td>False</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10001492</td>\n",
       "      <td>27463908</td>\n",
       "      <td>[41071, 42983, 2449, 4240]</td>\n",
       "      <td>[H03A]</td>\n",
       "      <td>[PRO_3722, PRO_8853, PRO_8855]</td>\n",
       "      <td>[50908-3, 50903-1, 51000-___, 50893-8.4, 50960...</td>\n",
       "      <td>F</td>\n",
       "      <td>age_14</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10001725</td>\n",
       "      <td>25563031</td>\n",
       "      <td>[78829, 9950, 49390, 53081, 30000, 311, 7291, ...</td>\n",
       "      <td>[N05B, M01A, D07A, C07A, C03B, J01D, N02A, C01...</td>\n",
       "      <td>[PRO_8696, PRO_7094]</td>\n",
       "      <td>[50868-14, 50882-24, 50893-9.1, 50912-0.8, 509...</td>\n",
       "      <td>F</td>\n",
       "      <td>age_7</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10002013</td>\n",
       "      <td>21975601</td>\n",
       "      <td>[99672, 42832, 4111, 5849, 41401, E8781, 4280,...</td>\n",
       "      <td>[N02B, H04A, B05C, R03A, A02B]</td>\n",
       "      <td>[PRO_0066, PRO_3607, PRO_3722, PRO_0045, PRO_0...</td>\n",
       "      <td>[51482-1]</td>\n",
       "      <td>F</td>\n",
       "      <td>age_9</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[41401, 42832, 5180, 4280, 4139, 4400, 4019, 3...</td>\n",
       "      <td>[41401, 42832, 5180, 4280, 4139, 4400, 4019, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10002013</td>\n",
       "      <td>23581541</td>\n",
       "      <td>[41401, 42832, 5180, 4280, 4139, 4400, 4019, 3...</td>\n",
       "      <td>[C10A, B01A]</td>\n",
       "      <td>[PRO_3615, PRO_3612]</td>\n",
       "      <td>[50802-0, 50804-27, 50806-101, 50808-1.16, 508...</td>\n",
       "      <td>F</td>\n",
       "      <td>age_9</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10002155</td>\n",
       "      <td>28439444</td>\n",
       "      <td>[43491, 41400, V4581, 2720, 3051]</td>\n",
       "      <td>[C10A, B01A]</td>\n",
       "      <td>[PRO_8914]</td>\n",
       "      <td>[50903-2, 50904-44, 50993-2.4, 50995-2, 51000-...</td>\n",
       "      <td>F</td>\n",
       "      <td>age_16</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10002155</td>\n",
       "      <td>23822395</td>\n",
       "      <td>[41011, 486, 42821, 41402, 99672, 7455, 1628, ...</td>\n",
       "      <td>[B01A, C10A, A01A]</td>\n",
       "      <td>[PRO_0066, PRO_3606, PRO_3723, PRO_8856, PRO_0...</td>\n",
       "      <td>[50908-4, 50953-2]</td>\n",
       "      <td>F</td>\n",
       "      <td>age_16</td>\n",
       "      <td>True</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10002155</td>\n",
       "      <td>20345487</td>\n",
       "      <td>[1628, 486, 51884, 5781, 2851, 2761, 49121, 41...</td>\n",
       "      <td>[C10A, B03B]</td>\n",
       "      <td>[PRO_3491]</td>\n",
       "      <td>[50808-1.11, 51479-1, 51482-1, 51453-___]</td>\n",
       "      <td>F</td>\n",
       "      <td>age_16</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10002428</td>\n",
       "      <td>28662225</td>\n",
       "      <td>[0383, 78552, 5184, 5845, 34831, 486, 51881, 0...</td>\n",
       "      <td>[A02B, H03A]</td>\n",
       "      <td>[PRO_9604, PRO_9671, PRO_3893, PRO_3891, PRO_3...</td>\n",
       "      <td>[50817-95, 50802-1, 50804-29, 50818-49, 50820-...</td>\n",
       "      <td>F</td>\n",
       "      <td>age_16</td>\n",
       "      <td>False</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[03843, 51881, 42843, 5990, 00845, 99591, 4280...</td>\n",
       "      <td>[03843, 51881, 42843, 5990, 00845, 99591, 4280...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10002428</td>\n",
       "      <td>23473524</td>\n",
       "      <td>[03843, 51881, 42843, 5990, 00845, 99591, 4280...</td>\n",
       "      <td>[H03A]</td>\n",
       "      <td>[PRO_9672, PRO_9604, PRO_3891, PRO_3897]</td>\n",
       "      <td>[51283-1.4]</td>\n",
       "      <td>F</td>\n",
       "      <td>age_16</td>\n",
       "      <td>False</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    subject_id   hadm_id                                           icd_code  \\\n",
       "0     10000032  22595853  [5723, 78959, 5715, 07070, 496, 29680, 30981, ...   \n",
       "1     10000690  25860671  [5070, 42833, 51881, 5849, 5781, 2763, 5119, 5...   \n",
       "2     10000690  26146595  [5609, 5849, 42832, 2930, 1539, 4280, 42731, 7...   \n",
       "3     10000826  20032235  [5712, 486, 78959, 5723, 5990, 2639, 2761, 511...   \n",
       "4     10000826  21086876  [5711, 99591, 78959, 2761, 5990, 5119, 5710, 3...   \n",
       "5     10000826  28289260  [5723, 78959, 2761, 5712, 2875, 5711, 7242, 33...   \n",
       "6     10000935  25849114  [1970, 34830, 1977, 1539, 2762, 5780, 2869, 27...   \n",
       "7     10000980  26913865  [41071, 42823, 41412, 5854, 4240, 4280, 41401,...   \n",
       "8     10000980  25242409  [45342, 53551, 5854, 25040, 5849, 3572, 42832,...   \n",
       "9     10001217  24597018  [3484, 3485, 5180, 340, 04109, 3051, 4019, V16...   \n",
       "10    10001338  22119639  [56211, 5849, 5695, 99859, 6822, 04111, E8786,...   \n",
       "11    10001492  27463908                         [41071, 42983, 2449, 4240]   \n",
       "12    10001725  25563031  [78829, 9950, 49390, 53081, 30000, 311, 7291, ...   \n",
       "13    10002013  21975601  [99672, 42832, 4111, 5849, 41401, E8781, 4280,...   \n",
       "14    10002013  23581541  [41401, 42832, 5180, 4280, 4139, 4400, 4019, 3...   \n",
       "15    10002155  28439444                  [43491, 41400, V4581, 2720, 3051]   \n",
       "16    10002155  23822395  [41011, 486, 42821, 41402, 99672, 7455, 1628, ...   \n",
       "17    10002155  20345487  [1628, 486, 51884, 5781, 2851, 2761, 49121, 41...   \n",
       "18    10002428  28662225  [0383, 78552, 5184, 5845, 34831, 486, 51881, 0...   \n",
       "19    10002428  23473524  [03843, 51881, 42843, 5990, 00845, 99591, 4280...   \n",
       "\n",
       "                                                  ndc  \\\n",
       "0                                              [B01A]   \n",
       "1                                        [A06A, B01A]   \n",
       "2                [A01A, A12C, N02B, N02A, A04A, J01M]   \n",
       "3                                              [B01A]   \n",
       "4                                        [B03B, N07B]   \n",
       "5                                        [N05B, N02A]   \n",
       "6                                        [N02B, A06A]   \n",
       "7   [A12C, V03A, A02B, C03C, C10A, B01A, C08C, H04...   \n",
       "8   [C07A, B01A, C08C, N06A, M04A, N02B, A06A, C10...   \n",
       "9                            [A06A, N03A, N02A, N02B]   \n",
       "10                                       [D07A, N06A]   \n",
       "11                                             [H03A]   \n",
       "12  [N05B, M01A, D07A, C07A, C03B, J01D, N02A, C01...   \n",
       "13                     [N02B, H04A, B05C, R03A, A02B]   \n",
       "14                                       [C10A, B01A]   \n",
       "15                                       [C10A, B01A]   \n",
       "16                                 [B01A, C10A, A01A]   \n",
       "17                                       [C10A, B03B]   \n",
       "18                                       [A02B, H03A]   \n",
       "19                                             [H03A]   \n",
       "\n",
       "                                             pro_code  \\\n",
       "0                                          [PRO_5491]   \n",
       "1                                          [PRO_3893]   \n",
       "2                                          [PRO_4524]   \n",
       "3                                          [PRO_5491]   \n",
       "4                                          [PRO_5491]   \n",
       "5                                          [PRO_5491]   \n",
       "6                                          [PRO_5011]   \n",
       "7   [PRO_0066, PRO_3607, PRO_0045, PRO_0041, PRO_3...   \n",
       "8                                          [PRO_4513]   \n",
       "9                      [PRO_0139, PRO_0331, PRO_3897]   \n",
       "10                                         [PRO_4576]   \n",
       "11                     [PRO_3722, PRO_8853, PRO_8855]   \n",
       "12                               [PRO_8696, PRO_7094]   \n",
       "13  [PRO_0066, PRO_3607, PRO_3722, PRO_0045, PRO_0...   \n",
       "14                               [PRO_3615, PRO_3612]   \n",
       "15                                         [PRO_8914]   \n",
       "16  [PRO_0066, PRO_3606, PRO_3723, PRO_8856, PRO_0...   \n",
       "17                                         [PRO_3491]   \n",
       "18  [PRO_9604, PRO_9671, PRO_3893, PRO_3891, PRO_3...   \n",
       "19           [PRO_9672, PRO_9604, PRO_3891, PRO_3897]   \n",
       "\n",
       "                                             lab_test gender     age  death  \\\n",
       "0                                  [51114-1, 51120-0]      F   age_9   True   \n",
       "1                                         [51009-___]      F  age_18   True   \n",
       "2                                         [50900-___]      F  age_18   True   \n",
       "3   [51118-1, 50953-0, 50960-1.5, 50970-3.3, 50995...      F   age_3  False   \n",
       "4                              [51095-___, 51105-___]      F   age_3  False   \n",
       "5                       [51114-1, 51067-___, 51094-1]      F   age_3  False   \n",
       "6   [50963-___, 50802--2, 50804-22, 50818-33, 5082...      F   age_9   True   \n",
       "7           [50863-61, 50883-0.1, 50884-0, 50885-0.3]      F  age_15   True   \n",
       "8                                          [50810-25]      F  age_15   True   \n",
       "9   [52264-___, 52272-0, 52281-___, 52286-___, 509...      F  age_10  False   \n",
       "10  [51237-1.1, 51274-___, 51275-28.4, 51143-0, 51...      F   age_6  False   \n",
       "11  [50908-3, 50903-1, 51000-___, 50893-8.4, 50960...      F  age_14  False   \n",
       "12  [50868-14, 50882-24, 50893-9.1, 50912-0.8, 509...      F   age_7  False   \n",
       "13                                          [51482-1]      F   age_9  False   \n",
       "14  [50802-0, 50804-27, 50806-101, 50808-1.16, 508...      F   age_9  False   \n",
       "15  [50903-2, 50904-44, 50993-2.4, 50995-2, 51000-...      F  age_16   True   \n",
       "16                                 [50908-4, 50953-2]      F  age_16   True   \n",
       "17          [50808-1.11, 51479-1, 51482-1, 51453-___]      F  age_16   True   \n",
       "18  [50817-95, 50802-1, 50804-29, 50818-49, 50820-...      F  age_16  False   \n",
       "19                                        [51283-1.4]      F  age_16  False   \n",
       "\n",
       "    stay_days  readmission  READMISSION_1M  READMISSION_3M  \\\n",
       "0           0            1               0               0   \n",
       "1           9            1               0               0   \n",
       "2           1            1               0               0   \n",
       "3           6            1               1               1   \n",
       "4           6            1               1               1   \n",
       "5           2            0               0               0   \n",
       "6          15            1               0               0   \n",
       "7           5            1               0               0   \n",
       "8           7            1               0               0   \n",
       "9           6            1               0               0   \n",
       "10         17            1               0               0   \n",
       "11          1            0               0               0   \n",
       "12          2            0               0               0   \n",
       "13          2            1               0               0   \n",
       "14          5            1               0               0   \n",
       "15          2            1               0               0   \n",
       "16         14            1               0               0   \n",
       "17          0            1               0               0   \n",
       "18         17            1               1               1   \n",
       "19         10            1               0               0   \n",
       "\n",
       "                                         NEXT_DIAG_6M  \\\n",
       "0                                                 NaN   \n",
       "1                                                 NaN   \n",
       "2                                                 NaN   \n",
       "3   [5711, 99591, 78959, 2761, 5990, 5119, 5710, 3...   \n",
       "4   [5723, 78959, 2761, 5712, 2875, 5711, 7242, 33...   \n",
       "5                                                 NaN   \n",
       "6                                                 NaN   \n",
       "7                                                 NaN   \n",
       "8                                                 NaN   \n",
       "9                                                 NaN   \n",
       "10                                                NaN   \n",
       "11                                                NaN   \n",
       "12                                                NaN   \n",
       "13  [41401, 42832, 5180, 4280, 4139, 4400, 4019, 3...   \n",
       "14                                                NaN   \n",
       "15                                                NaN   \n",
       "16                                                NaN   \n",
       "17                                                NaN   \n",
       "18  [03843, 51881, 42843, 5990, 00845, 99591, 4280...   \n",
       "19                                                NaN   \n",
       "\n",
       "                                        NEXT_DIAG_12M  \n",
       "0                                                 NaN  \n",
       "1                                                 NaN  \n",
       "2                                                 NaN  \n",
       "3   [5711, 99591, 78959, 2761, 5990, 5119, 5710, 3...  \n",
       "4   [5723, 78959, 2761, 5712, 2875, 5711, 7242, 33...  \n",
       "5                                                 NaN  \n",
       "6                                                 NaN  \n",
       "7                                                 NaN  \n",
       "8                                                 NaN  \n",
       "9                                                 NaN  \n",
       "10                                                NaN  \n",
       "11                                                NaN  \n",
       "12                                                NaN  \n",
       "13  [41401, 42832, 5180, 4280, 4139, 4400, 4019, 3...  \n",
       "14                                                NaN  \n",
       "15                                                NaN  \n",
       "16                                                NaN  \n",
       "17                                                NaN  \n",
       "18  [03843, 51881, 42843, 5990, 00845, 99591, 4280...  \n",
       "19                                                NaN  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mimic_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "779ec51f-da3e-424b-959e-e9851f59040f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78041"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mimic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b01a4cea-b23b-4d89-a724-66246c5f23fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subject_id                                                 10000032\n",
       "hadm_id                                                    22595853\n",
       "icd_code          [5723, 78959, 5715, 07070, 496, 29680, 30981, ...\n",
       "ndc                                                          [B01A]\n",
       "pro_code                                                 [PRO_5491]\n",
       "lab_test                                         [51114-1, 51120-0]\n",
       "gender                                                            F\n",
       "age                                                           age_9\n",
       "death                                                          True\n",
       "stay_days                                                         0\n",
       "readmission                                                       1\n",
       "READMISSION_1M                                                    0\n",
       "READMISSION_3M                                                    0\n",
       "NEXT_DIAG_6M                                                    NaN\n",
       "NEXT_DIAG_12M                                                   NaN\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mimic_data.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c046494b-65eb-4546-b9ce-100a6e82db39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>icd_code</th>\n",
       "      <th>ndc</th>\n",
       "      <th>pro_code</th>\n",
       "      <th>lab_test</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>death</th>\n",
       "      <th>stay_days</th>\n",
       "      <th>readmission</th>\n",
       "      <th>READMISSION_1M</th>\n",
       "      <th>READMISSION_3M</th>\n",
       "      <th>NEXT_DIAG_6M</th>\n",
       "      <th>NEXT_DIAG_12M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000032</td>\n",
       "      <td>22595853</td>\n",
       "      <td>[5723, 78959, 5715, 07070, 496, 29680, 30981, ...</td>\n",
       "      <td>[B01A]</td>\n",
       "      <td>[PRO_5491]</td>\n",
       "      <td>[51114-1, 51120-0]</td>\n",
       "      <td>F</td>\n",
       "      <td>age_9</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id   hadm_id                                           icd_code  \\\n",
       "0    10000032  22595853  [5723, 78959, 5715, 07070, 496, 29680, 30981, ...   \n",
       "\n",
       "      ndc    pro_code            lab_test gender    age  death  stay_days  \\\n",
       "0  [B01A]  [PRO_5491]  [51114-1, 51120-0]      F  age_9   True          0   \n",
       "\n",
       "   readmission  READMISSION_1M  READMISSION_3M NEXT_DIAG_6M NEXT_DIAG_12M  \n",
       "0            1               0               0          NaN           NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mimic_data[mimic_data['subject_id']==10000032]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0943309c-03b1-4a46-8601-58ed55ece703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>icd_code</th>\n",
       "      <th>ndc</th>\n",
       "      <th>pro_code</th>\n",
       "      <th>lab_test</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>death</th>\n",
       "      <th>stay_days</th>\n",
       "      <th>readmission</th>\n",
       "      <th>READMISSION_1M</th>\n",
       "      <th>READMISSION_3M</th>\n",
       "      <th>NEXT_DIAG_6M</th>\n",
       "      <th>NEXT_DIAG_12M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000826</td>\n",
       "      <td>20032235</td>\n",
       "      <td>[5712, 486, 78959, 5723, 5990, 2639, 2761, 511...</td>\n",
       "      <td>[B01A]</td>\n",
       "      <td>[PRO_5491]</td>\n",
       "      <td>[51118-1, 50953-0, 50960-1.5, 50970-3.3, 50995...</td>\n",
       "      <td>F</td>\n",
       "      <td>age_3</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[5711, 99591, 78959, 2761, 5990, 5119, 5710, 3...</td>\n",
       "      <td>[5711, 99591, 78959, 2761, 5990, 5119, 5710, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000826</td>\n",
       "      <td>21086876</td>\n",
       "      <td>[5711, 99591, 78959, 2761, 5990, 5119, 5710, 3...</td>\n",
       "      <td>[B03B, N07B]</td>\n",
       "      <td>[PRO_5491]</td>\n",
       "      <td>[51095-___, 51105-___]</td>\n",
       "      <td>F</td>\n",
       "      <td>age_3</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[5723, 78959, 2761, 5712, 2875, 5711, 7242, 33...</td>\n",
       "      <td>[5723, 78959, 2761, 5712, 2875, 5711, 7242, 33...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10002013</td>\n",
       "      <td>21975601</td>\n",
       "      <td>[99672, 42832, 4111, 5849, 41401, E8781, 4280,...</td>\n",
       "      <td>[N02B, H04A, B05C, R03A, A02B]</td>\n",
       "      <td>[PRO_0066, PRO_3607, PRO_3722, PRO_0045, PRO_0...</td>\n",
       "      <td>[51482-1]</td>\n",
       "      <td>F</td>\n",
       "      <td>age_9</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[41401, 42832, 5180, 4280, 4139, 4400, 4019, 3...</td>\n",
       "      <td>[41401, 42832, 5180, 4280, 4139, 4400, 4019, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10002428</td>\n",
       "      <td>28662225</td>\n",
       "      <td>[0383, 78552, 5184, 5845, 34831, 486, 51881, 0...</td>\n",
       "      <td>[A02B, H03A]</td>\n",
       "      <td>[PRO_9604, PRO_9671, PRO_3893, PRO_3891, PRO_3...</td>\n",
       "      <td>[50817-95, 50802-1, 50804-29, 50818-49, 50820-...</td>\n",
       "      <td>F</td>\n",
       "      <td>age_16</td>\n",
       "      <td>False</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[03843, 51881, 42843, 5990, 00845, 99591, 4280...</td>\n",
       "      <td>[03843, 51881, 42843, 5990, 00845, 99591, 4280...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>10003400</td>\n",
       "      <td>29483621</td>\n",
       "      <td>[28412, 20300, 4589, 42731, 5853, 2851, V5861,...</td>\n",
       "      <td>[B01A, J01F, C05A, C07A]</td>\n",
       "      <td>[PRO_4523]</td>\n",
       "      <td>[51143-0, 51144-5, 51251-1, 51255-0, 50900-___...</td>\n",
       "      <td>F</td>\n",
       "      <td>age_14</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[2866, 51881, 5845, 20300, 2639, 99809, 2930, ...</td>\n",
       "      <td>[2866, 51881, 5845, 20300, 2639, 99809, 2930, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78012</th>\n",
       "      <td>19997367</td>\n",
       "      <td>25038503</td>\n",
       "      <td>[5781, 2851, 5723, 27900, 3481, 42832, 45621, ...</td>\n",
       "      <td>[B03B, C10A, A01A]</td>\n",
       "      <td>[PRO_4513, PRO_4523, PRO_8872]</td>\n",
       "      <td>[50883-0.4, 50884-3, 50864-___, 50802-0, 50804...</td>\n",
       "      <td>F</td>\n",
       "      <td>age_12</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[78551, 03849, 99592, 51851, 5722, 45620, 5070...</td>\n",
       "      <td>[78551, 03849, 99592, 51851, 5722, 45620, 5070...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78018</th>\n",
       "      <td>19998330</td>\n",
       "      <td>24492004</td>\n",
       "      <td>[51881, 42833, 486, 5849, 49121, 4280, 40390, ...</td>\n",
       "      <td>[S01E, C08C, C10A, A07E, C09A]</td>\n",
       "      <td>[PRO_9671, PRO_9604]</td>\n",
       "      <td>[50808-1.18, 51283-1.3, 50953-3]</td>\n",
       "      <td>F</td>\n",
       "      <td>age_14</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[49121, 42833, 51881, 4280, 42731, 32723, 2852...</td>\n",
       "      <td>[49121, 42833, 51881, 4280, 42731, 32723, 2852...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78022</th>\n",
       "      <td>19998497</td>\n",
       "      <td>28129567</td>\n",
       "      <td>[41401, 5184, 4111, 25000, 2809, 2724, 45829, ...</td>\n",
       "      <td>[A06A, B01A]</td>\n",
       "      <td>[PRO_0066, PRO_3607, PRO_3606, PRO_3723, PRO_8...</td>\n",
       "      <td>[50885-0.3]</td>\n",
       "      <td>F</td>\n",
       "      <td>age_17</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[41401, 5845, 2762, 4111, 25000, 28521, 5854, ...</td>\n",
       "      <td>[41401, 5845, 2762, 4111, 25000, 28521, 5854, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78031</th>\n",
       "      <td>19999287</td>\n",
       "      <td>22997012</td>\n",
       "      <td>[1629, 486, 7907, 25000, 496, V0481, 2724, 716...</td>\n",
       "      <td>[R03A, R01A]</td>\n",
       "      <td>[PRO_3391, PRO_3324]</td>\n",
       "      <td>[51476-2, 51493-2, 51516-1]</td>\n",
       "      <td>F</td>\n",
       "      <td>age_14</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[51884, 4829, 9341, 496, V462, 1625, 99659, 25...</td>\n",
       "      <td>[51884, 4829, 9341, 496, V462, 1625, 99659, 25...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78038</th>\n",
       "      <td>19999840</td>\n",
       "      <td>26071774</td>\n",
       "      <td>[43491, 43820, 34590, 43811, 4019, 2724, 3051]</td>\n",
       "      <td>[N02B]</td>\n",
       "      <td>[PRO_8891, PRO_8841]</td>\n",
       "      <td>[50868-16, 50882-20, 50902-106, 50908-0, 50910...</td>\n",
       "      <td>M</td>\n",
       "      <td>age_10</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[3453, 51881, 5070, 5180, 42741, 43821, 43811,...</td>\n",
       "      <td>[3453, 51881, 5070, 5180, 42741, 43821, 43811,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10895 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       subject_id   hadm_id  \\\n",
       "3        10000826  20032235   \n",
       "4        10000826  21086876   \n",
       "13       10002013  21975601   \n",
       "18       10002428  28662225   \n",
       "31       10003400  29483621   \n",
       "...           ...       ...   \n",
       "78012    19997367  25038503   \n",
       "78018    19998330  24492004   \n",
       "78022    19998497  28129567   \n",
       "78031    19999287  22997012   \n",
       "78038    19999840  26071774   \n",
       "\n",
       "                                                icd_code  \\\n",
       "3      [5712, 486, 78959, 5723, 5990, 2639, 2761, 511...   \n",
       "4      [5711, 99591, 78959, 2761, 5990, 5119, 5710, 3...   \n",
       "13     [99672, 42832, 4111, 5849, 41401, E8781, 4280,...   \n",
       "18     [0383, 78552, 5184, 5845, 34831, 486, 51881, 0...   \n",
       "31     [28412, 20300, 4589, 42731, 5853, 2851, V5861,...   \n",
       "...                                                  ...   \n",
       "78012  [5781, 2851, 5723, 27900, 3481, 42832, 45621, ...   \n",
       "78018  [51881, 42833, 486, 5849, 49121, 4280, 40390, ...   \n",
       "78022  [41401, 5184, 4111, 25000, 2809, 2724, 45829, ...   \n",
       "78031  [1629, 486, 7907, 25000, 496, V0481, 2724, 716...   \n",
       "78038     [43491, 43820, 34590, 43811, 4019, 2724, 3051]   \n",
       "\n",
       "                                  ndc  \\\n",
       "3                              [B01A]   \n",
       "4                        [B03B, N07B]   \n",
       "13     [N02B, H04A, B05C, R03A, A02B]   \n",
       "18                       [A02B, H03A]   \n",
       "31           [B01A, J01F, C05A, C07A]   \n",
       "...                               ...   \n",
       "78012              [B03B, C10A, A01A]   \n",
       "78018  [S01E, C08C, C10A, A07E, C09A]   \n",
       "78022                    [A06A, B01A]   \n",
       "78031                    [R03A, R01A]   \n",
       "78038                          [N02B]   \n",
       "\n",
       "                                                pro_code  \\\n",
       "3                                             [PRO_5491]   \n",
       "4                                             [PRO_5491]   \n",
       "13     [PRO_0066, PRO_3607, PRO_3722, PRO_0045, PRO_0...   \n",
       "18     [PRO_9604, PRO_9671, PRO_3893, PRO_3891, PRO_3...   \n",
       "31                                            [PRO_4523]   \n",
       "...                                                  ...   \n",
       "78012                     [PRO_4513, PRO_4523, PRO_8872]   \n",
       "78018                               [PRO_9671, PRO_9604]   \n",
       "78022  [PRO_0066, PRO_3607, PRO_3606, PRO_3723, PRO_8...   \n",
       "78031                               [PRO_3391, PRO_3324]   \n",
       "78038                               [PRO_8891, PRO_8841]   \n",
       "\n",
       "                                                lab_test gender     age  \\\n",
       "3      [51118-1, 50953-0, 50960-1.5, 50970-3.3, 50995...      F   age_3   \n",
       "4                                 [51095-___, 51105-___]      F   age_3   \n",
       "13                                             [51482-1]      F   age_9   \n",
       "18     [50817-95, 50802-1, 50804-29, 50818-49, 50820-...      F  age_16   \n",
       "31     [51143-0, 51144-5, 51251-1, 51255-0, 50900-___...      F  age_14   \n",
       "...                                                  ...    ...     ...   \n",
       "78012  [50883-0.4, 50884-3, 50864-___, 50802-0, 50804...      F  age_12   \n",
       "78018                   [50808-1.18, 51283-1.3, 50953-3]      F  age_14   \n",
       "78022                                        [50885-0.3]      F  age_17   \n",
       "78031                        [51476-2, 51493-2, 51516-1]      F  age_14   \n",
       "78038  [50868-16, 50882-20, 50902-106, 50908-0, 50910...      M  age_10   \n",
       "\n",
       "       death  stay_days  readmission  READMISSION_1M  READMISSION_3M  \\\n",
       "3      False          6            1               1               1   \n",
       "4      False          6            1               1               1   \n",
       "13     False          2            1               0               0   \n",
       "18     False         17            1               1               1   \n",
       "31      True          7            0               0               0   \n",
       "...      ...        ...          ...             ...             ...   \n",
       "78012  False          4            1               0               1   \n",
       "78018   True          7            1               0               1   \n",
       "78022   True          3            1               0               0   \n",
       "78031   True          5            1               1               1   \n",
       "78038   True          3            0               0               1   \n",
       "\n",
       "                                            NEXT_DIAG_6M  \\\n",
       "3      [5711, 99591, 78959, 2761, 5990, 5119, 5710, 3...   \n",
       "4      [5723, 78959, 2761, 5712, 2875, 5711, 7242, 33...   \n",
       "13     [41401, 42832, 5180, 4280, 4139, 4400, 4019, 3...   \n",
       "18     [03843, 51881, 42843, 5990, 00845, 99591, 4280...   \n",
       "31     [2866, 51881, 5845, 20300, 2639, 99809, 2930, ...   \n",
       "...                                                  ...   \n",
       "78012  [78551, 03849, 99592, 51851, 5722, 45620, 5070...   \n",
       "78018  [49121, 42833, 51881, 4280, 42731, 32723, 2852...   \n",
       "78022  [41401, 5845, 2762, 4111, 25000, 28521, 5854, ...   \n",
       "78031  [51884, 4829, 9341, 496, V462, 1625, 99659, 25...   \n",
       "78038  [3453, 51881, 5070, 5180, 42741, 43821, 43811,...   \n",
       "\n",
       "                                           NEXT_DIAG_12M  \n",
       "3      [5711, 99591, 78959, 2761, 5990, 5119, 5710, 3...  \n",
       "4      [5723, 78959, 2761, 5712, 2875, 5711, 7242, 33...  \n",
       "13     [41401, 42832, 5180, 4280, 4139, 4400, 4019, 3...  \n",
       "18     [03843, 51881, 42843, 5990, 00845, 99591, 4280...  \n",
       "31     [2866, 51881, 5845, 20300, 2639, 99809, 2930, ...  \n",
       "...                                                  ...  \n",
       "78012  [78551, 03849, 99592, 51851, 5722, 45620, 5070...  \n",
       "78018  [49121, 42833, 51881, 4280, 42731, 32723, 2852...  \n",
       "78022  [41401, 5845, 2762, 4111, 25000, 28521, 5854, ...  \n",
       "78031  [51884, 4829, 9341, 496, V462, 1625, 99659, 25...  \n",
       "78038  [3453, 51881, 5070, 5180, 42741, 43821, 43811,...  \n",
       "\n",
       "[10895 rows x 15 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# next diagnosis (6M) is not Nan\n",
    "mimic_data[mimic_data[\"NEXT_DIAG_6M\"].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873e5894-7d44-4cfd-b3ad-c33fa48cf4ed",
   "metadata": {},
   "source": [
    "This has duplicates as well. hence it is 4733 rows. Without duplicates it is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4811e538-c83e-4822-a32d-e6bca72db4b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8151"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(mimic_data[mimic_data[\"NEXT_DIAG_6M\"].notna()][\"subject_id\"].values.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a80d6a3d-1e9a-4451-ba8e-437e8691d8e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58894"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mimic_data[\"subject_id\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c0af11-1a21-4c8f-ac22-c3c2a47fd528",
   "metadata": {},
   "source": [
    "38760 unique patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d364854f-c4de-4414-a4ae-dec863cf92db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>icd_code</th>\n",
       "      <th>ndc</th>\n",
       "      <th>pro_code</th>\n",
       "      <th>lab_test</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>death</th>\n",
       "      <th>stay_days</th>\n",
       "      <th>readmission</th>\n",
       "      <th>READMISSION_1M</th>\n",
       "      <th>READMISSION_3M</th>\n",
       "      <th>NEXT_DIAG_6M</th>\n",
       "      <th>NEXT_DIAG_12M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000826</td>\n",
       "      <td>20032235</td>\n",
       "      <td>[5712, 486, 78959, 5723, 5990, 2639, 2761, 511...</td>\n",
       "      <td>[B01A]</td>\n",
       "      <td>[PRO_5491]</td>\n",
       "      <td>[51118-1, 50953-0, 50960-1.5, 50970-3.3, 50995...</td>\n",
       "      <td>F</td>\n",
       "      <td>age_3</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[5711, 99591, 78959, 2761, 5990, 5119, 5710, 3...</td>\n",
       "      <td>[5711, 99591, 78959, 2761, 5990, 5119, 5710, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000826</td>\n",
       "      <td>21086876</td>\n",
       "      <td>[5711, 99591, 78959, 2761, 5990, 5119, 5710, 3...</td>\n",
       "      <td>[B03B, N07B]</td>\n",
       "      <td>[PRO_5491]</td>\n",
       "      <td>[51095-___, 51105-___]</td>\n",
       "      <td>F</td>\n",
       "      <td>age_3</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[5723, 78959, 2761, 5712, 2875, 5711, 7242, 33...</td>\n",
       "      <td>[5723, 78959, 2761, 5712, 2875, 5711, 7242, 33...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10002013</td>\n",
       "      <td>21975601</td>\n",
       "      <td>[99672, 42832, 4111, 5849, 41401, E8781, 4280,...</td>\n",
       "      <td>[N02B, H04A, B05C, R03A, A02B]</td>\n",
       "      <td>[PRO_0066, PRO_3607, PRO_3722, PRO_0045, PRO_0...</td>\n",
       "      <td>[51482-1]</td>\n",
       "      <td>F</td>\n",
       "      <td>age_9</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[41401, 42832, 5180, 4280, 4139, 4400, 4019, 3...</td>\n",
       "      <td>[41401, 42832, 5180, 4280, 4139, 4400, 4019, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10002428</td>\n",
       "      <td>28662225</td>\n",
       "      <td>[0383, 78552, 5184, 5845, 34831, 486, 51881, 0...</td>\n",
       "      <td>[A02B, H03A]</td>\n",
       "      <td>[PRO_9604, PRO_9671, PRO_3893, PRO_3891, PRO_3...</td>\n",
       "      <td>[50817-95, 50802-1, 50804-29, 50818-49, 50820-...</td>\n",
       "      <td>F</td>\n",
       "      <td>age_16</td>\n",
       "      <td>False</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[03843, 51881, 42843, 5990, 00845, 99591, 4280...</td>\n",
       "      <td>[03843, 51881, 42843, 5990, 00845, 99591, 4280...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>10003400</td>\n",
       "      <td>29483621</td>\n",
       "      <td>[28412, 20300, 4589, 42731, 5853, 2851, V5861,...</td>\n",
       "      <td>[B01A, J01F, C05A, C07A]</td>\n",
       "      <td>[PRO_4523]</td>\n",
       "      <td>[51143-0, 51144-5, 51251-1, 51255-0, 50900-___...</td>\n",
       "      <td>F</td>\n",
       "      <td>age_14</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[2866, 51881, 5845, 20300, 2639, 99809, 2930, ...</td>\n",
       "      <td>[2866, 51881, 5845, 20300, 2639, 99809, 2930, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78013</th>\n",
       "      <td>19997367</td>\n",
       "      <td>20617667</td>\n",
       "      <td>[78551, 03849, 99592, 51851, 5722, 45620, 5070...</td>\n",
       "      <td>[C07A, C10A, A01A, B03B]</td>\n",
       "      <td>[PRO_3523, PRO_3723, PRO_3733, PRO_3961, PRO_8...</td>\n",
       "      <td>[51491-7.0, 51498-1.008, 50806-108, 50808-1.15...</td>\n",
       "      <td>F</td>\n",
       "      <td>age_12</td>\n",
       "      <td>False</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[0380, 78552, 51881, 5722, 4210, 5119, 20500, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78018</th>\n",
       "      <td>19998330</td>\n",
       "      <td>24492004</td>\n",
       "      <td>[51881, 42833, 486, 5849, 49121, 4280, 40390, ...</td>\n",
       "      <td>[S01E, C08C, C10A, A07E, C09A]</td>\n",
       "      <td>[PRO_9671, PRO_9604]</td>\n",
       "      <td>[50808-1.18, 51283-1.3, 50953-3]</td>\n",
       "      <td>F</td>\n",
       "      <td>age_14</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[49121, 42833, 51881, 4280, 42731, 32723, 2852...</td>\n",
       "      <td>[49121, 42833, 51881, 4280, 42731, 32723, 2852...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78022</th>\n",
       "      <td>19998497</td>\n",
       "      <td>28129567</td>\n",
       "      <td>[41401, 5184, 4111, 25000, 2809, 2724, 45829, ...</td>\n",
       "      <td>[A06A, B01A]</td>\n",
       "      <td>[PRO_0066, PRO_3607, PRO_3606, PRO_3723, PRO_8...</td>\n",
       "      <td>[50885-0.3]</td>\n",
       "      <td>F</td>\n",
       "      <td>age_17</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[41401, 5845, 2762, 4111, 25000, 28521, 5854, ...</td>\n",
       "      <td>[41401, 5845, 2762, 4111, 25000, 28521, 5854, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78031</th>\n",
       "      <td>19999287</td>\n",
       "      <td>22997012</td>\n",
       "      <td>[1629, 486, 7907, 25000, 496, V0481, 2724, 716...</td>\n",
       "      <td>[R03A, R01A]</td>\n",
       "      <td>[PRO_3391, PRO_3324]</td>\n",
       "      <td>[51476-2, 51493-2, 51516-1]</td>\n",
       "      <td>F</td>\n",
       "      <td>age_14</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[51884, 4829, 9341, 496, V462, 1625, 99659, 25...</td>\n",
       "      <td>[51884, 4829, 9341, 496, V462, 1625, 99659, 25...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78038</th>\n",
       "      <td>19999840</td>\n",
       "      <td>26071774</td>\n",
       "      <td>[43491, 43820, 34590, 43811, 4019, 2724, 3051]</td>\n",
       "      <td>[N02B]</td>\n",
       "      <td>[PRO_8891, PRO_8841]</td>\n",
       "      <td>[50868-16, 50882-20, 50902-106, 50908-0, 50910...</td>\n",
       "      <td>M</td>\n",
       "      <td>age_10</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[3453, 51881, 5070, 5180, 42741, 43821, 43811,...</td>\n",
       "      <td>[3453, 51881, 5070, 5180, 42741, 43821, 43811,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13417 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       subject_id   hadm_id  \\\n",
       "3        10000826  20032235   \n",
       "4        10000826  21086876   \n",
       "13       10002013  21975601   \n",
       "18       10002428  28662225   \n",
       "31       10003400  29483621   \n",
       "...           ...       ...   \n",
       "78013    19997367  20617667   \n",
       "78018    19998330  24492004   \n",
       "78022    19998497  28129567   \n",
       "78031    19999287  22997012   \n",
       "78038    19999840  26071774   \n",
       "\n",
       "                                                icd_code  \\\n",
       "3      [5712, 486, 78959, 5723, 5990, 2639, 2761, 511...   \n",
       "4      [5711, 99591, 78959, 2761, 5990, 5119, 5710, 3...   \n",
       "13     [99672, 42832, 4111, 5849, 41401, E8781, 4280,...   \n",
       "18     [0383, 78552, 5184, 5845, 34831, 486, 51881, 0...   \n",
       "31     [28412, 20300, 4589, 42731, 5853, 2851, V5861,...   \n",
       "...                                                  ...   \n",
       "78013  [78551, 03849, 99592, 51851, 5722, 45620, 5070...   \n",
       "78018  [51881, 42833, 486, 5849, 49121, 4280, 40390, ...   \n",
       "78022  [41401, 5184, 4111, 25000, 2809, 2724, 45829, ...   \n",
       "78031  [1629, 486, 7907, 25000, 496, V0481, 2724, 716...   \n",
       "78038     [43491, 43820, 34590, 43811, 4019, 2724, 3051]   \n",
       "\n",
       "                                  ndc  \\\n",
       "3                              [B01A]   \n",
       "4                        [B03B, N07B]   \n",
       "13     [N02B, H04A, B05C, R03A, A02B]   \n",
       "18                       [A02B, H03A]   \n",
       "31           [B01A, J01F, C05A, C07A]   \n",
       "...                               ...   \n",
       "78013        [C07A, C10A, A01A, B03B]   \n",
       "78018  [S01E, C08C, C10A, A07E, C09A]   \n",
       "78022                    [A06A, B01A]   \n",
       "78031                    [R03A, R01A]   \n",
       "78038                          [N02B]   \n",
       "\n",
       "                                                pro_code  \\\n",
       "3                                             [PRO_5491]   \n",
       "4                                             [PRO_5491]   \n",
       "13     [PRO_0066, PRO_3607, PRO_3722, PRO_0045, PRO_0...   \n",
       "18     [PRO_9604, PRO_9671, PRO_3893, PRO_3891, PRO_3...   \n",
       "31                                            [PRO_4523]   \n",
       "...                                                  ...   \n",
       "78013  [PRO_3523, PRO_3723, PRO_3733, PRO_3961, PRO_8...   \n",
       "78018                               [PRO_9671, PRO_9604]   \n",
       "78022  [PRO_0066, PRO_3607, PRO_3606, PRO_3723, PRO_8...   \n",
       "78031                               [PRO_3391, PRO_3324]   \n",
       "78038                               [PRO_8891, PRO_8841]   \n",
       "\n",
       "                                                lab_test gender     age  \\\n",
       "3      [51118-1, 50953-0, 50960-1.5, 50970-3.3, 50995...      F   age_3   \n",
       "4                                 [51095-___, 51105-___]      F   age_3   \n",
       "13                                             [51482-1]      F   age_9   \n",
       "18     [50817-95, 50802-1, 50804-29, 50818-49, 50820-...      F  age_16   \n",
       "31     [51143-0, 51144-5, 51251-1, 51255-0, 50900-___...      F  age_14   \n",
       "...                                                  ...    ...     ...   \n",
       "78013  [51491-7.0, 51498-1.008, 50806-108, 50808-1.15...      F  age_12   \n",
       "78018                   [50808-1.18, 51283-1.3, 50953-3]      F  age_14   \n",
       "78022                                        [50885-0.3]      F  age_17   \n",
       "78031                        [51476-2, 51493-2, 51516-1]      F  age_14   \n",
       "78038  [50868-16, 50882-20, 50902-106, 50908-0, 50910...      M  age_10   \n",
       "\n",
       "       death  stay_days  readmission  READMISSION_1M  READMISSION_3M  \\\n",
       "3      False          6            1               1               1   \n",
       "4      False          6            1               1               1   \n",
       "13     False          2            1               0               0   \n",
       "18     False         17            1               1               1   \n",
       "31      True          7            0               0               0   \n",
       "...      ...        ...          ...             ...             ...   \n",
       "78013  False         29            1               0               0   \n",
       "78018   True          7            1               0               1   \n",
       "78022   True          3            1               0               0   \n",
       "78031   True          5            1               1               1   \n",
       "78038   True          3            0               0               1   \n",
       "\n",
       "                                            NEXT_DIAG_6M  \\\n",
       "3      [5711, 99591, 78959, 2761, 5990, 5119, 5710, 3...   \n",
       "4      [5723, 78959, 2761, 5712, 2875, 5711, 7242, 33...   \n",
       "13     [41401, 42832, 5180, 4280, 4139, 4400, 4019, 3...   \n",
       "18     [03843, 51881, 42843, 5990, 00845, 99591, 4280...   \n",
       "31     [2866, 51881, 5845, 20300, 2639, 99809, 2930, ...   \n",
       "...                                                  ...   \n",
       "78013                                                NaN   \n",
       "78018  [49121, 42833, 51881, 4280, 42731, 32723, 2852...   \n",
       "78022  [41401, 5845, 2762, 4111, 25000, 28521, 5854, ...   \n",
       "78031  [51884, 4829, 9341, 496, V462, 1625, 99659, 25...   \n",
       "78038  [3453, 51881, 5070, 5180, 42741, 43821, 43811,...   \n",
       "\n",
       "                                           NEXT_DIAG_12M  \n",
       "3      [5711, 99591, 78959, 2761, 5990, 5119, 5710, 3...  \n",
       "4      [5723, 78959, 2761, 5712, 2875, 5711, 7242, 33...  \n",
       "13     [41401, 42832, 5180, 4280, 4139, 4400, 4019, 3...  \n",
       "18     [03843, 51881, 42843, 5990, 00845, 99591, 4280...  \n",
       "31     [2866, 51881, 5845, 20300, 2639, 99809, 2930, ...  \n",
       "...                                                  ...  \n",
       "78013  [0380, 78552, 51881, 5722, 4210, 5119, 20500, ...  \n",
       "78018  [49121, 42833, 51881, 4280, 42731, 32723, 2852...  \n",
       "78022  [41401, 5845, 2762, 4111, 25000, 28521, 5854, ...  \n",
       "78031  [51884, 4829, 9341, 496, V462, 1625, 99659, 25...  \n",
       "78038  [3453, 51881, 5070, 5180, 42741, 43821, 43811,...  \n",
       "\n",
       "[13417 rows x 15 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mimic_data[mimic_data[\"NEXT_DIAG_12M\"].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f3e3a6-df15-41d4-b81c-82683ece899d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2fe055b9-da11-4b2a-a913-1324bd92ce03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3706\n",
      "8151\n",
      "9591\n",
      "15608\n"
     ]
    }
   ],
   "source": [
    "# get the patient id with the labels\n",
    "pat_readmission = set(mimic_data[mimic_data[\"READMISSION_1M\"] == 1][\"subject_id\"].values.tolist())\n",
    "print(len(pat_readmission))\n",
    "pat_nextdiag_6m = set(mimic_data[mimic_data[\"NEXT_DIAG_6M\"].notna()][\"subject_id\"].values.tolist())\n",
    "print(len(pat_nextdiag_6m))\n",
    "pat_nextdiag_12m = set(mimic_data[mimic_data[\"NEXT_DIAG_12M\"].notna()][\"subject_id\"].values.tolist())\n",
    "print(len(pat_nextdiag_12m))\n",
    "pat_death = set(mimic_data[mimic_data[\"death\"]][\"subject_id\"].values.tolist())\n",
    "print(len(pat_death))\n",
    "# pat_all_label = list(pat_readmission | pat_nextdiag_6m | pat_nextdiag_12m | pat_death)\n",
    "pat_all_label = list(pat_readmission | pat_nextdiag_6m | pat_nextdiag_12m | pat_death)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6540f6fa-0b0c-49c5-99fe-98641514882e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20867"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pat_all_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15642830-e8fb-44c8-a2af-cf95338a31f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_all = mimic_data[\"subject_id\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a5c00bd-ad08-4d3c-837e-ba59c80adf83",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot take a larger sample than population when 'replace=False'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m n_pretrain_patient = \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(pat_all) * \u001b[32m0.7\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# pretrain_patient = np.random.choice(list(set(pat_all) - set(pat_all_label)), n_pretrain_patient, replace=False).tolist()\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m pretrain_patient = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpat_all\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpat_all_label\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_pretrain_patient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m.tolist()\n\u001b[32m      4\u001b[39m downstream_patient = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(pat_all) - \u001b[38;5;28mset\u001b[39m(pretrain_patient))\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(pretrain_patient), \u001b[38;5;28mlen\u001b[39m(downstream_patient))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mnumpy/random/mtrand.pyx:1020\u001b[39m, in \u001b[36mnumpy.random.mtrand.RandomState.choice\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: Cannot take a larger sample than population when 'replace=False'"
     ]
    }
   ],
   "source": [
    "n_pretrain_patient = int(len(pat_all) * 0.7)\n",
    "# pretrain_patient = np.random.choice(list(set(pat_all) - set(pat_all_label)), n_pretrain_patient, replace=False).tolist()\n",
    "pretrain_patient = np.random.choice(list(set(pat_all) - set(pat_all_label)), n_pretrain_patient, replace=False).tolist()\n",
    "downstream_patient = list(set(pat_all) - set(pretrain_patient))\n",
    "print(len(pretrain_patient), len(downstream_patient))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21381a27-e6e6-48ac-8316-22945f64566e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20867"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pat_all_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1f1cb61-fcba-46ac-ab92-7f30660f5e50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38027"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_free_patients = list(set(pat_all) - set(pat_all_label))\n",
    "len(label_free_patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f928a79-77ea-488e-a446-111834abe7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26618 32276\n",
      "Total unique patients in split: 58894\n",
      "Total unique in original: 58894\n"
     ]
    }
   ],
   "source": [
    "# Calculate the pool you want to sample from:\n",
    "label_free_patients = list(set(pat_all) - set(pat_all_label))\n",
    "n_pretrain_patient = int(len(label_free_patients) * 0.7)  # 70% of only those WITHOUT labels\n",
    "\n",
    "# Now, you can safely sample without replacement:\n",
    "pretrain_patient = np.random.choice(label_free_patients, n_pretrain_patient, replace=False).tolist()\n",
    "downstream_patient = list(set(pat_all) - set(pretrain_patient))\n",
    "\n",
    "print(len(pretrain_patient), len(downstream_patient))\n",
    "print(\"Total unique patients in split:\", len(set(pretrain_patient) | set(downstream_patient)))\n",
    "print(\"Total unique in original:\", len(set(pat_all)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b070a9ec-b18d-4763-9664-131bec34325d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26618"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_pretrain_patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7aa847e7-23d7-4183-9949-d0abe2139486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19053975,\n",
       " 11991967,\n",
       " 19013037,\n",
       " 11576270,\n",
       " 12589387,\n",
       " 12978079,\n",
       " 14745196,\n",
       " 13032235,\n",
       " 17997063,\n",
       " 12714566]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_patient[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b9539242-39cf-42c0-90d8-a49d9ceea64c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18874374,\n",
       " 10878995,\n",
       " 11272213,\n",
       " 18350105,\n",
       " 12189736,\n",
       " 13893673,\n",
       " 10747946,\n",
       " 11403312,\n",
       " 18612273,\n",
       " 19398714]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downstream_patient[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "11136ebe-3ee4-4ed1-acd9-305ee07243e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>icd_code</th>\n",
       "      <th>ndc</th>\n",
       "      <th>pro_code</th>\n",
       "      <th>lab_test</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>death</th>\n",
       "      <th>stay_days</th>\n",
       "      <th>readmission</th>\n",
       "      <th>READMISSION_1M</th>\n",
       "      <th>READMISSION_3M</th>\n",
       "      <th>NEXT_DIAG_6M</th>\n",
       "      <th>NEXT_DIAG_12M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10001338</td>\n",
       "      <td>22119639</td>\n",
       "      <td>[56211, 5849, 5695, 99859, 6822, 04111, E8786,...</td>\n",
       "      <td>[D07A, N06A]</td>\n",
       "      <td>[PRO_4576]</td>\n",
       "      <td>[51237-1.1, 51274-___, 51275-28.4, 51143-0, 51...</td>\n",
       "      <td>F</td>\n",
       "      <td>age_6</td>\n",
       "      <td>False</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10001492</td>\n",
       "      <td>27463908</td>\n",
       "      <td>[41071, 42983, 2449, 4240]</td>\n",
       "      <td>[H03A]</td>\n",
       "      <td>[PRO_3722, PRO_8853, PRO_8855]</td>\n",
       "      <td>[50908-3, 50903-1, 51000-___, 50893-8.4, 50960...</td>\n",
       "      <td>F</td>\n",
       "      <td>age_14</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10002870</td>\n",
       "      <td>25351634</td>\n",
       "      <td>[220, 2180, 2449, 2720, 3051]</td>\n",
       "      <td>[A12A, M01A]</td>\n",
       "      <td>[PRO_6563, PRO_6841]</td>\n",
       "      <td>[51146-0.5, 51200-1.9, 51254-5.6, 50960-1.9, 5...</td>\n",
       "      <td>F</td>\n",
       "      <td>age_10</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10002976</td>\n",
       "      <td>27179825</td>\n",
       "      <td>[7291, 5849, V5867, 412, 41401, 4019, V4582, 2...</td>\n",
       "      <td>[C07A, A01A]</td>\n",
       "      <td>[PRO_8321]</td>\n",
       "      <td>[50889-___, 50893-8.6, 50960-2.4, 50970-2.4, 5...</td>\n",
       "      <td>M</td>\n",
       "      <td>age_14</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>10003019</td>\n",
       "      <td>23693618</td>\n",
       "      <td>[135, 7856, 51889, 2724, V4579]</td>\n",
       "      <td>[N02B, A04A, A12A, N02A]</td>\n",
       "      <td>[PRO_3220, PRO_3422, PRO_403, PRO_3323]</td>\n",
       "      <td>[50960-1.8, 50893-8.8, 50970-3.6]</td>\n",
       "      <td>M</td>\n",
       "      <td>age_13</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78005</th>\n",
       "      <td>19996219</td>\n",
       "      <td>22115476</td>\n",
       "      <td>[57400, 5531, 2720]</td>\n",
       "      <td>[C10A]</td>\n",
       "      <td>[PRO_5123, PRO_5349]</td>\n",
       "      <td>[50893-8.6, 50960-2.1, 50970-3.2]</td>\n",
       "      <td>M</td>\n",
       "      <td>age_12</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78006</th>\n",
       "      <td>19997062</td>\n",
       "      <td>28549819</td>\n",
       "      <td>[57400, 5761, 5715, 57410]</td>\n",
       "      <td>[B01A, A07E, A05A]</td>\n",
       "      <td>[PRO_5123, PRO_5014]</td>\n",
       "      <td>[50893-9.4, 50956-23, 50960-2.1, 50970-2.5, 51...</td>\n",
       "      <td>M</td>\n",
       "      <td>age_8</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78026</th>\n",
       "      <td>19998626</td>\n",
       "      <td>24639135</td>\n",
       "      <td>[566, 56942]</td>\n",
       "      <td>[C01B]</td>\n",
       "      <td>[PRO_4901]</td>\n",
       "      <td>[50868-13, 50882-27, 50893-8.7, 50902-104, 509...</td>\n",
       "      <td>F</td>\n",
       "      <td>age_4</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78036</th>\n",
       "      <td>19999442</td>\n",
       "      <td>26785317</td>\n",
       "      <td>[34541, 43491, 431, 3485, V6284, 11284, 5990, ...</td>\n",
       "      <td>[N06A]</td>\n",
       "      <td>[PRO_9671, PRO_9604, PRO_966]</td>\n",
       "      <td>[50910-___, 50911-4, 50802-0, 50804-23, 50808-...</td>\n",
       "      <td>M</td>\n",
       "      <td>age_6</td>\n",
       "      <td>False</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78040</th>\n",
       "      <td>19999987</td>\n",
       "      <td>23865745</td>\n",
       "      <td>[431, 3485, 20280, 5849, 5990, 2449, 41401, 78...</td>\n",
       "      <td>[C02D, A12C, N03A]</td>\n",
       "      <td>[PRO_8841, PRO_9671]</td>\n",
       "      <td>[50868-13, 50882-28, 50893-8.5, 50902-109, 509...</td>\n",
       "      <td>F</td>\n",
       "      <td>age_10</td>\n",
       "      <td>False</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28156 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       subject_id   hadm_id  \\\n",
       "10       10001338  22119639   \n",
       "11       10001492  27463908   \n",
       "24       10002870  25351634   \n",
       "25       10002976  27179825   \n",
       "26       10003019  23693618   \n",
       "...           ...       ...   \n",
       "78005    19996219  22115476   \n",
       "78006    19997062  28549819   \n",
       "78026    19998626  24639135   \n",
       "78036    19999442  26785317   \n",
       "78040    19999987  23865745   \n",
       "\n",
       "                                                icd_code  \\\n",
       "10     [56211, 5849, 5695, 99859, 6822, 04111, E8786,...   \n",
       "11                            [41071, 42983, 2449, 4240]   \n",
       "24                         [220, 2180, 2449, 2720, 3051]   \n",
       "25     [7291, 5849, V5867, 412, 41401, 4019, V4582, 2...   \n",
       "26                       [135, 7856, 51889, 2724, V4579]   \n",
       "...                                                  ...   \n",
       "78005                                [57400, 5531, 2720]   \n",
       "78006                         [57400, 5761, 5715, 57410]   \n",
       "78026                                       [566, 56942]   \n",
       "78036  [34541, 43491, 431, 3485, V6284, 11284, 5990, ...   \n",
       "78040  [431, 3485, 20280, 5849, 5990, 2449, 41401, 78...   \n",
       "\n",
       "                            ndc                                 pro_code  \\\n",
       "10                 [D07A, N06A]                               [PRO_4576]   \n",
       "11                       [H03A]           [PRO_3722, PRO_8853, PRO_8855]   \n",
       "24                 [A12A, M01A]                     [PRO_6563, PRO_6841]   \n",
       "25                 [C07A, A01A]                               [PRO_8321]   \n",
       "26     [N02B, A04A, A12A, N02A]  [PRO_3220, PRO_3422, PRO_403, PRO_3323]   \n",
       "...                         ...                                      ...   \n",
       "78005                    [C10A]                     [PRO_5123, PRO_5349]   \n",
       "78006        [B01A, A07E, A05A]                     [PRO_5123, PRO_5014]   \n",
       "78026                    [C01B]                               [PRO_4901]   \n",
       "78036                    [N06A]            [PRO_9671, PRO_9604, PRO_966]   \n",
       "78040        [C02D, A12C, N03A]                     [PRO_8841, PRO_9671]   \n",
       "\n",
       "                                                lab_test gender     age  \\\n",
       "10     [51237-1.1, 51274-___, 51275-28.4, 51143-0, 51...      F   age_6   \n",
       "11     [50908-3, 50903-1, 51000-___, 50893-8.4, 50960...      F  age_14   \n",
       "24     [51146-0.5, 51200-1.9, 51254-5.6, 50960-1.9, 5...      F  age_10   \n",
       "25     [50889-___, 50893-8.6, 50960-2.4, 50970-2.4, 5...      M  age_14   \n",
       "26                     [50960-1.8, 50893-8.8, 50970-3.6]      M  age_13   \n",
       "...                                                  ...    ...     ...   \n",
       "78005                  [50893-8.6, 50960-2.1, 50970-3.2]      M  age_12   \n",
       "78006  [50893-9.4, 50956-23, 50960-2.1, 50970-2.5, 51...      M   age_8   \n",
       "78026  [50868-13, 50882-27, 50893-8.7, 50902-104, 509...      F   age_4   \n",
       "78036  [50910-___, 50911-4, 50802-0, 50804-23, 50808-...      M   age_6   \n",
       "78040  [50868-13, 50882-28, 50893-8.5, 50902-109, 509...      F  age_10   \n",
       "\n",
       "       death  stay_days  readmission  READMISSION_1M  READMISSION_3M  \\\n",
       "10     False         17            1               0               0   \n",
       "11     False          1            0               0               0   \n",
       "24     False          2            0               0               0   \n",
       "25     False          4            0               0               0   \n",
       "26     False          1            1               0               0   \n",
       "...      ...        ...          ...             ...             ...   \n",
       "78005  False          1            0               0               0   \n",
       "78006  False          2            0               0               0   \n",
       "78026  False          2            0               0               0   \n",
       "78036  False         15            0               0               0   \n",
       "78040  False          8            0               0               0   \n",
       "\n",
       "      NEXT_DIAG_6M NEXT_DIAG_12M  \n",
       "10             NaN           NaN  \n",
       "11             NaN           NaN  \n",
       "24             NaN           NaN  \n",
       "25             NaN           NaN  \n",
       "26             NaN           NaN  \n",
       "...            ...           ...  \n",
       "78005          NaN           NaN  \n",
       "78006          NaN           NaN  \n",
       "78026          NaN           NaN  \n",
       "78036          NaN           NaN  \n",
       "78040          NaN           NaN  \n",
       "\n",
       "[28156 rows x 15 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_dataset = mimic_data[mimic_data[\"subject_id\"].isin(set(pretrain_patient))]\n",
    "pretrain_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "886492de-5261-4420-b956-7b161eed9232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Pretrain dataset stored in: <_io.BufferedWriter name='./dataset/mimic_pretrain.pkl'>\n"
     ]
    }
   ],
   "source": [
    "if icd10:\n",
    "    with open(\"./dataset/mimic_pretrain_icd10.pkl\", \"wb\") as outfile:\n",
    "        pickle.dump(pretrain_dataset, outfile)\n",
    "else:\n",
    "    with open(\"./dataset/mimic_pretrain.pkl\", \"wb\") as outfile:\n",
    "        pickle.dump(pretrain_dataset, outfile)\n",
    "\n",
    "print(icd10)\n",
    "print(f\"Pretrain dataset stored in: {outfile}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e6274ad-ad8a-4198-a09d-bb03f82fd4aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>icd_code</th>\n",
       "      <th>ndc</th>\n",
       "      <th>pro_code</th>\n",
       "      <th>lab_test</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>death</th>\n",
       "      <th>stay_days</th>\n",
       "      <th>readmission</th>\n",
       "      <th>READMISSION_1M</th>\n",
       "      <th>READMISSION_3M</th>\n",
       "      <th>NEXT_DIAG_6M</th>\n",
       "      <th>NEXT_DIAG_12M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10001338</td>\n",
       "      <td>22119639</td>\n",
       "      <td>[56211, 5849, 5695, 99859, 6822, 04111, E8786,...</td>\n",
       "      <td>[D07A, N06A]</td>\n",
       "      <td>[PRO_4576]</td>\n",
       "      <td>[51237-1.1, 51274-___, 51275-28.4, 51143-0, 51...</td>\n",
       "      <td>F</td>\n",
       "      <td>age_6</td>\n",
       "      <td>False</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10001492</td>\n",
       "      <td>27463908</td>\n",
       "      <td>[41071, 42983, 2449, 4240]</td>\n",
       "      <td>[H03A]</td>\n",
       "      <td>[PRO_3722, PRO_8853, PRO_8855]</td>\n",
       "      <td>[50908-3, 50903-1, 51000-___, 50893-8.4, 50960...</td>\n",
       "      <td>F</td>\n",
       "      <td>age_14</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10002870</td>\n",
       "      <td>25351634</td>\n",
       "      <td>[220, 2180, 2449, 2720, 3051]</td>\n",
       "      <td>[A12A, M01A]</td>\n",
       "      <td>[PRO_6563, PRO_6841]</td>\n",
       "      <td>[51146-0.5, 51200-1.9, 51254-5.6, 50960-1.9, 5...</td>\n",
       "      <td>F</td>\n",
       "      <td>age_10</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10002976</td>\n",
       "      <td>27179825</td>\n",
       "      <td>[7291, 5849, V5867, 412, 41401, 4019, V4582, 2...</td>\n",
       "      <td>[C07A, A01A]</td>\n",
       "      <td>[PRO_8321]</td>\n",
       "      <td>[50889-___, 50893-8.6, 50960-2.4, 50970-2.4, 5...</td>\n",
       "      <td>M</td>\n",
       "      <td>age_14</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>10003019</td>\n",
       "      <td>23693618</td>\n",
       "      <td>[135, 7856, 51889, 2724, V4579]</td>\n",
       "      <td>[N02B, A04A, A12A, N02A]</td>\n",
       "      <td>[PRO_3220, PRO_3422, PRO_403, PRO_3323]</td>\n",
       "      <td>[50960-1.8, 50893-8.8, 50970-3.6]</td>\n",
       "      <td>M</td>\n",
       "      <td>age_13</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    subject_id   hadm_id                                           icd_code  \\\n",
       "10    10001338  22119639  [56211, 5849, 5695, 99859, 6822, 04111, E8786,...   \n",
       "11    10001492  27463908                         [41071, 42983, 2449, 4240]   \n",
       "24    10002870  25351634                      [220, 2180, 2449, 2720, 3051]   \n",
       "25    10002976  27179825  [7291, 5849, V5867, 412, 41401, 4019, V4582, 2...   \n",
       "26    10003019  23693618                    [135, 7856, 51889, 2724, V4579]   \n",
       "\n",
       "                         ndc                                 pro_code  \\\n",
       "10              [D07A, N06A]                               [PRO_4576]   \n",
       "11                    [H03A]           [PRO_3722, PRO_8853, PRO_8855]   \n",
       "24              [A12A, M01A]                     [PRO_6563, PRO_6841]   \n",
       "25              [C07A, A01A]                               [PRO_8321]   \n",
       "26  [N02B, A04A, A12A, N02A]  [PRO_3220, PRO_3422, PRO_403, PRO_3323]   \n",
       "\n",
       "                                             lab_test gender     age  death  \\\n",
       "10  [51237-1.1, 51274-___, 51275-28.4, 51143-0, 51...      F   age_6  False   \n",
       "11  [50908-3, 50903-1, 51000-___, 50893-8.4, 50960...      F  age_14  False   \n",
       "24  [51146-0.5, 51200-1.9, 51254-5.6, 50960-1.9, 5...      F  age_10  False   \n",
       "25  [50889-___, 50893-8.6, 50960-2.4, 50970-2.4, 5...      M  age_14  False   \n",
       "26                  [50960-1.8, 50893-8.8, 50970-3.6]      M  age_13  False   \n",
       "\n",
       "    stay_days  readmission  READMISSION_1M  READMISSION_3M NEXT_DIAG_6M  \\\n",
       "10         17            1               0               0          NaN   \n",
       "11          1            0               0               0          NaN   \n",
       "24          2            0               0               0          NaN   \n",
       "25          4            0               0               0          NaN   \n",
       "26          1            1               0               0          NaN   \n",
       "\n",
       "   NEXT_DIAG_12M  \n",
       "10           NaN  \n",
       "11           NaN  \n",
       "24           NaN  \n",
       "25           NaN  \n",
       "26           NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "pretrain_data = pickle.load(open(\"dataset/mimic_pretrain.pkl\", 'rb'))\n",
    "pretrain_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "92915943-0e63-475b-9794-1c477a770be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FineTune dataset for 6M diagnosis prediction = 8767\n",
      "Val. dataset for 6M diagnosis prediction = 6524\n",
      "Test dataset for 6M diagnosis prediction = 6526\n"
     ]
    }
   ],
   "source": [
    "train_ratio, val_ratio = 0.4, 0.3\n",
    "\n",
    "#6m\n",
    "n_finetune_pat, n_val_pat = int(len(pat_nextdiag_6m) * train_ratio), int(len(pat_nextdiag_6m) * val_ratio)\n",
    "pat_nextdiag_6m = list(pat_nextdiag_6m)\n",
    "np.random.shuffle(pat_nextdiag_6m)\n",
    "finetune_pat, val_pat, test_pat = pat_nextdiag_6m[:n_finetune_pat], \\\n",
    "                                    pat_nextdiag_6m[n_finetune_pat:n_finetune_pat+n_val_pat], \\\n",
    "                                    pat_nextdiag_6m[n_finetune_pat+n_val_pat:]\n",
    "finetune_dataset6m = mimic_data[mimic_data[\"subject_id\"].isin(set(finetune_pat))]\n",
    "print(f\"FineTune dataset for 6M diagnosis prediction = {len(finetune_dataset6m)}\")\n",
    "val_dataset6m = mimic_data[mimic_data[\"subject_id\"].isin(set(val_pat))]\n",
    "print(f\"Val. dataset for 6M diagnosis prediction = {len(val_dataset6m)}\")\n",
    "test_dataset6m = mimic_data[mimic_data[\"subject_id\"].isin(set(test_pat))]\n",
    "print(f\"Test dataset for 6M diagnosis prediction = {len(test_dataset6m)}\")\n",
    "\n",
    "# 12m\n",
    "n_finetune_pat, n_val_pat = int(len(pat_nextdiag_12m) * train_ratio), int(len(pat_nextdiag_12m) * val_ratio)\n",
    "pat_nextdiag_12m = list(pat_nextdiag_12m)\n",
    "np.random.shuffle(pat_nextdiag_12m)\n",
    "finetune_pat, val_pat, test_pat = pat_nextdiag_12m[:n_finetune_pat], \\\n",
    "                                    pat_nextdiag_12m[n_finetune_pat:n_finetune_pat+n_val_pat], \\\n",
    "                                    pat_nextdiag_12m[n_finetune_pat+n_val_pat:]\n",
    "finetune_dataset12m = mimic_data[mimic_data[\"subject_id\"].isin(set(finetune_pat))]\n",
    "val_dataset12m = mimic_data[mimic_data[\"subject_id\"].isin(set(val_pat))]\n",
    "test_dataset12m = mimic_data[mimic_data[\"subject_id\"].isin(set(test_pat))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cc961f74-c09b-42d3-be9d-a9048747be6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.BufferedWriter name='./dataset/mimic_nextdiag_12m.pkl'>\n"
     ]
    }
   ],
   "source": [
    "if icd10:\n",
    "    with open(\"./dataset/mimic_nextdiag_6m_icd10.pkl\", \"wb\") as outfile:\n",
    "        pickle.dump([finetune_dataset6m, val_dataset6m, test_dataset6m], outfile)\n",
    "    with open(\"./dataset/mimic_nextdiag_12m_icd10.pkl\", \"wb\") as outfile:\n",
    "        pickle.dump([finetune_dataset12m, val_dataset12m, test_dataset12m], outfile)\n",
    "else:\n",
    "    with open(\"./dataset/mimic_nextdiag_6m.pkl\", \"wb\") as outfile:\n",
    "        pickle.dump([finetune_dataset6m, val_dataset6m, test_dataset6m], outfile)\n",
    "    with open(\"./dataset/mimic_nextdiag_12m.pkl\", \"wb\") as outfile:\n",
    "        pickle.dump([finetune_dataset12m, val_dataset12m, test_dataset12m], outfile)\n",
    "\n",
    "print(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "def6d3e6-0e9d-493c-aef9-7db5358de1d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>icd_code</th>\n",
       "      <th>ndc</th>\n",
       "      <th>pro_code</th>\n",
       "      <th>lab_test</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>death</th>\n",
       "      <th>stay_days</th>\n",
       "      <th>readmission</th>\n",
       "      <th>READMISSION_1M</th>\n",
       "      <th>READMISSION_3M</th>\n",
       "      <th>NEXT_DIAG_6M</th>\n",
       "      <th>NEXT_DIAG_12M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>10008924</td>\n",
       "      <td>27441295</td>\n",
       "      <td>[56723, 5849, 78959, 5723, 2761, 2867, 5712, 3...</td>\n",
       "      <td>[N02B]</td>\n",
       "      <td>[PRO_4516, PRO_5491]</td>\n",
       "      <td>[50893-8.8, 50960-1.5, 50970-4.6, 50953-0, 508...</td>\n",
       "      <td>F</td>\n",
       "      <td>age_7</td>\n",
       "      <td>True</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[99811, 5724, 5845, 5722, 56723, 2851, 5723, 7...</td>\n",
       "      <td>[99811, 5724, 5845, 5722, 56723, 2851, 5723, 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>10008924</td>\n",
       "      <td>23676183</td>\n",
       "      <td>[99811, 5724, 5845, 5722, 56723, 2851, 5723, 7...</td>\n",
       "      <td>[B03B]</td>\n",
       "      <td>[PRO_9462, PRO_5491, PRO_3893]</td>\n",
       "      <td>[50813-1.7, 51099-0, 50802-1, 50804-27, 50818-...</td>\n",
       "      <td>F</td>\n",
       "      <td>age_7</td>\n",
       "      <td>True</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>10016991</td>\n",
       "      <td>24172189</td>\n",
       "      <td>[1536, 1962, 5990, 0416]</td>\n",
       "      <td>[A02B, M01A, A04A, A06A, A03F, N02B, A12A]</td>\n",
       "      <td>[PRO_1733]</td>\n",
       "      <td>[50900-___, 50960-1.8, 50893-8.3, 50970-3.0, 5...</td>\n",
       "      <td>M</td>\n",
       "      <td>age_7</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[V5811, 1536]</td>\n",
       "      <td>[V5811, 1536]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>10016991</td>\n",
       "      <td>27389040</td>\n",
       "      <td>[V5811, 1536]</td>\n",
       "      <td>[L01B, A04A, A01A]</td>\n",
       "      <td>[PRO_9925, PRO_8914]</td>\n",
       "      <td>[51237-1.0, 51274-11.5]</td>\n",
       "      <td>M</td>\n",
       "      <td>age_7</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>10018052</td>\n",
       "      <td>27285907</td>\n",
       "      <td>[1977, 78951, 7994, 2536, 7892, 1749, 53081, 2...</td>\n",
       "      <td>[B01A]</td>\n",
       "      <td>[PRO_5491]</td>\n",
       "      <td>[50893-8.5, 50960-2.0, 50970-3.4]</td>\n",
       "      <td>F</td>\n",
       "      <td>age_7</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[1749, 1977, 78951, 2767, V860, V4986]</td>\n",
       "      <td>[1749, 1977, 78951, 2767, V860, V4986]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77925</th>\n",
       "      <td>19986198</td>\n",
       "      <td>25207205</td>\n",
       "      <td>[431, 99702, 6822, 2768, 3485, 3439, 2449, 34590]</td>\n",
       "      <td>[J01D, A12C]</td>\n",
       "      <td>[PRO_966, PRO_3893]</td>\n",
       "      <td>[51144-0, 50889-___, 50813-0.9, 51491-7.0, 514...</td>\n",
       "      <td>F</td>\n",
       "      <td>age_3</td>\n",
       "      <td>False</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78009</th>\n",
       "      <td>19997293</td>\n",
       "      <td>27119651</td>\n",
       "      <td>[34839, 42823, 5849, 3241, 4260, 73008, 70703,...</td>\n",
       "      <td>[C01B]</td>\n",
       "      <td>[PRO_8628]</td>\n",
       "      <td>[50995-1]</td>\n",
       "      <td>M</td>\n",
       "      <td>age_15</td>\n",
       "      <td>True</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[4260, 486, 51881, 42823, 70703, 496, 3441, 27...</td>\n",
       "      <td>[4260, 486, 51881, 42823, 70703, 496, 3441, 27...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78010</th>\n",
       "      <td>19997293</td>\n",
       "      <td>28847872</td>\n",
       "      <td>[4260, 486, 51881, 42823, 70703, 496, 3441, 27...</td>\n",
       "      <td>[A06A, N05C]</td>\n",
       "      <td>[PRO_3771, PRO_3781, PRO_9671, PRO_3778, PRO_8...</td>\n",
       "      <td>[50908-1, 50823-4]</td>\n",
       "      <td>M</td>\n",
       "      <td>age_15</td>\n",
       "      <td>True</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78018</th>\n",
       "      <td>19998330</td>\n",
       "      <td>24492004</td>\n",
       "      <td>[51881, 42833, 486, 5849, 49121, 4280, 40390, ...</td>\n",
       "      <td>[S01E, C08C, C10A, A07E, C09A]</td>\n",
       "      <td>[PRO_9671, PRO_9604]</td>\n",
       "      <td>[50808-1.18, 51283-1.3, 50953-3]</td>\n",
       "      <td>F</td>\n",
       "      <td>age_14</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[49121, 42833, 51881, 4280, 42731, 32723, 2852...</td>\n",
       "      <td>[49121, 42833, 51881, 4280, 42731, 32723, 2852...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78019</th>\n",
       "      <td>19998330</td>\n",
       "      <td>24096838</td>\n",
       "      <td>[49121, 42833, 51881, 4280, 42731, 32723, 2852...</td>\n",
       "      <td>[A06A, N01A]</td>\n",
       "      <td>[PRO_9671, PRO_9604]</td>\n",
       "      <td>[50817-96]</td>\n",
       "      <td>F</td>\n",
       "      <td>age_14</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6526 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       subject_id   hadm_id  \\\n",
       "72       10008924  27441295   \n",
       "73       10008924  23676183   \n",
       "138      10016991  24172189   \n",
       "139      10016991  27389040   \n",
       "154      10018052  27285907   \n",
       "...           ...       ...   \n",
       "77925    19986198  25207205   \n",
       "78009    19997293  27119651   \n",
       "78010    19997293  28847872   \n",
       "78018    19998330  24492004   \n",
       "78019    19998330  24096838   \n",
       "\n",
       "                                                icd_code  \\\n",
       "72     [56723, 5849, 78959, 5723, 2761, 2867, 5712, 3...   \n",
       "73     [99811, 5724, 5845, 5722, 56723, 2851, 5723, 7...   \n",
       "138                             [1536, 1962, 5990, 0416]   \n",
       "139                                        [V5811, 1536]   \n",
       "154    [1977, 78951, 7994, 2536, 7892, 1749, 53081, 2...   \n",
       "...                                                  ...   \n",
       "77925  [431, 99702, 6822, 2768, 3485, 3439, 2449, 34590]   \n",
       "78009  [34839, 42823, 5849, 3241, 4260, 73008, 70703,...   \n",
       "78010  [4260, 486, 51881, 42823, 70703, 496, 3441, 27...   \n",
       "78018  [51881, 42833, 486, 5849, 49121, 4280, 40390, ...   \n",
       "78019  [49121, 42833, 51881, 4280, 42731, 32723, 2852...   \n",
       "\n",
       "                                              ndc  \\\n",
       "72                                         [N02B]   \n",
       "73                                         [B03B]   \n",
       "138    [A02B, M01A, A04A, A06A, A03F, N02B, A12A]   \n",
       "139                            [L01B, A04A, A01A]   \n",
       "154                                        [B01A]   \n",
       "...                                           ...   \n",
       "77925                                [J01D, A12C]   \n",
       "78009                                      [C01B]   \n",
       "78010                                [A06A, N05C]   \n",
       "78018              [S01E, C08C, C10A, A07E, C09A]   \n",
       "78019                                [A06A, N01A]   \n",
       "\n",
       "                                                pro_code  \\\n",
       "72                                  [PRO_4516, PRO_5491]   \n",
       "73                        [PRO_9462, PRO_5491, PRO_3893]   \n",
       "138                                           [PRO_1733]   \n",
       "139                                 [PRO_9925, PRO_8914]   \n",
       "154                                           [PRO_5491]   \n",
       "...                                                  ...   \n",
       "77925                                [PRO_966, PRO_3893]   \n",
       "78009                                         [PRO_8628]   \n",
       "78010  [PRO_3771, PRO_3781, PRO_9671, PRO_3778, PRO_8...   \n",
       "78018                               [PRO_9671, PRO_9604]   \n",
       "78019                               [PRO_9671, PRO_9604]   \n",
       "\n",
       "                                                lab_test gender     age  \\\n",
       "72     [50893-8.8, 50960-1.5, 50970-4.6, 50953-0, 508...      F   age_7   \n",
       "73     [50813-1.7, 51099-0, 50802-1, 50804-27, 50818-...      F   age_7   \n",
       "138    [50900-___, 50960-1.8, 50893-8.3, 50970-3.0, 5...      M   age_7   \n",
       "139                              [51237-1.0, 51274-11.5]      M   age_7   \n",
       "154                    [50893-8.5, 50960-2.0, 50970-3.4]      F   age_7   \n",
       "...                                                  ...    ...     ...   \n",
       "77925  [51144-0, 50889-___, 50813-0.9, 51491-7.0, 514...      F   age_3   \n",
       "78009                                          [50995-1]      M  age_15   \n",
       "78010                                 [50908-1, 50823-4]      M  age_15   \n",
       "78018                   [50808-1.18, 51283-1.3, 50953-3]      F  age_14   \n",
       "78019                                         [50817-96]      F  age_14   \n",
       "\n",
       "       death  stay_days  readmission  READMISSION_1M  READMISSION_3M  \\\n",
       "72      True          8            0               0               1   \n",
       "73      True         20            1               0               0   \n",
       "138    False          4            1               0               0   \n",
       "139    False          0            0               0               0   \n",
       "154     True          1            0               1               1   \n",
       "...      ...        ...          ...             ...             ...   \n",
       "77925  False         13            1               0               0   \n",
       "78009   True         10            1               0               1   \n",
       "78010   True         12            0               0               0   \n",
       "78018   True          7            1               0               1   \n",
       "78019   True          3            1               0               0   \n",
       "\n",
       "                                            NEXT_DIAG_6M  \\\n",
       "72     [99811, 5724, 5845, 5722, 56723, 2851, 5723, 7...   \n",
       "73                                                   NaN   \n",
       "138                                        [V5811, 1536]   \n",
       "139                                                  NaN   \n",
       "154               [1749, 1977, 78951, 2767, V860, V4986]   \n",
       "...                                                  ...   \n",
       "77925                                                NaN   \n",
       "78009  [4260, 486, 51881, 42823, 70703, 496, 3441, 27...   \n",
       "78010                                                NaN   \n",
       "78018  [49121, 42833, 51881, 4280, 42731, 32723, 2852...   \n",
       "78019                                                NaN   \n",
       "\n",
       "                                           NEXT_DIAG_12M  \n",
       "72     [99811, 5724, 5845, 5722, 56723, 2851, 5723, 7...  \n",
       "73                                                   NaN  \n",
       "138                                        [V5811, 1536]  \n",
       "139                                                  NaN  \n",
       "154               [1749, 1977, 78951, 2767, V860, V4986]  \n",
       "...                                                  ...  \n",
       "77925                                                NaN  \n",
       "78009  [4260, 486, 51881, 42823, 70703, 496, 3441, 27...  \n",
       "78010                                                NaN  \n",
       "78018  [49121, 42833, 51881, 4280, 42731, 32723, 2852...  \n",
       "78019                                                NaN  \n",
       "\n",
       "[6526 rows x 15 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mimic_nextdiag_6m = pickle.load(open(\"dataset/mimic_nextdiag_6m.pkl\", 'rb'))\n",
    "mimic_nextdiag_6m[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d92347-8216-4c8d-b650-2515e181855c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_nextdiag_12m = pickle.load(open(\"dataset/mimic_nextdiag_12m.pkl\", 'rb'))\n",
    "mimic_nextdiag_12m.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f880241-dea2-4231-a611-5d821ff4ec6c",
   "metadata": {},
   "source": [
    "# PRETRAINING TRIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "992bcdc7-cecf-4db8-a244-547b8f8b9093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_level2():\n",
    "    level2 = ['001-009', '010-018', '020-027', '030-041', '042', '045-049', '050-059', '060-066', '070-079', '080-088',\n",
    "              '090-099', '100-104', '110-118', '120-129', '130-136', '137-139', '140-149', '150-159', '160-165',\n",
    "              '170-176',\n",
    "              '176', '179-189', '190-199', '200-208', '209', '210-229', '230-234', '235-238', '239', '240-246',\n",
    "              '249-259',\n",
    "              '260-269', '270-279', '280-289', '290-294', '295-299', '300-316', '317-319', '320-327', '330-337', '338',\n",
    "              '339', '340-349', '350-359', '360-379', '380-389', '390-392', '393-398', '401-405', '410-414', '415-417',\n",
    "              '420-429', '430-438', '440-449', '451-459', '460-466', '470-478', '480-488', '490-496', '500-508',\n",
    "              '510-519',\n",
    "              '520-529', '530-539', '540-543', '550-553', '555-558', '560-569', '570-579', '580-589', '590-599',\n",
    "              '600-608',\n",
    "              '610-611', '614-616', '617-629', '630-639', '640-649', '650-659', '660-669', '670-677', '678-679',\n",
    "              '680-686',\n",
    "              '690-698', '700-709', '710-719', '720-724', '725-729', '730-739', '740-759', '760-763', '764-779',\n",
    "              '780-789',\n",
    "              '790-796', '797-799', '800-804', '805-809', '810-819', '820-829', '830-839', '840-848', '850-854',\n",
    "              '860-869',\n",
    "              '870-879', '880-887', '890-897', '900-904', '905-909', '910-919', '920-924', '925-929', '930-939',\n",
    "              '940-949',\n",
    "              '950-957', '958-959', '960-979', '980-989', '990-995', '996-999', 'V01-V91', 'V01-V09', 'V10-V19',\n",
    "              'V20-V29',\n",
    "              'V30-V39', 'V40-V49', 'V50-V59', 'V60-V69', 'V70-V82', 'V83-V84', 'V85', 'V86', 'V87', 'V88', 'V89',\n",
    "              'V90',\n",
    "              'V91', 'E000-E899', 'E000', 'E001-E030', 'E800-E807', 'E810-E819', 'E820-E825', 'E826-E829', 'E830-E838',\n",
    "              'E840-E845', 'E846-E849', 'E850-E858', 'E860-E869', 'E870-E876', 'E878-E879', 'E880-E888', 'E890-E899',\n",
    "              'E900-E909', 'E910-E915', 'E916-E928', 'E929', 'E930-E949', 'E950-E959', 'E960-E969', 'E970-E978',\n",
    "              'E980-E989', 'E990-E999']\n",
    "\n",
    "    level2_expand = {}\n",
    "    for i in level2:\n",
    "        tokens = i.split('-')\n",
    "        if i[0] == 'V':\n",
    "            if len(tokens) == 1:\n",
    "                level2_expand[i] = i\n",
    "            else:\n",
    "                for j in range(int(tokens[0][1:]), int(tokens[1][1:]) + 1):\n",
    "                    level2_expand[\"V%02d\" % j] = i\n",
    "        elif i[0] == 'E':\n",
    "            if len(tokens) == 1:\n",
    "                level2_expand[i] = i\n",
    "            else:\n",
    "                for j in range(int(tokens[0][1:]), int(tokens[1][1:]) + 1):\n",
    "                    level2_expand[\"E%03d\" % j] = i\n",
    "        else:\n",
    "            if len(tokens) == 1:\n",
    "                level2_expand[i] = i\n",
    "            else:\n",
    "                for j in range(int(tokens[0]), int(tokens[1]) + 1):\n",
    "                    level2_expand[\"%03d\" % j] = i\n",
    "    return level2_expand\n",
    "\n",
    "\n",
    "# for diagnosis\n",
    "def build_icd9_tree(graph_voc, unique_codes):\n",
    "    code2sentence = {}\n",
    "    # graph_voc = Voc()\n",
    "\n",
    "    root_node = 'icd9_root'\n",
    "    level3_dict = expand_level2()\n",
    "    for code in unique_codes:\n",
    "        level1 = code\n",
    "        level2 = level1[:4] if level1[0] == 'E' else level1[:3]\n",
    "        level3 = level3_dict[level2]\n",
    "        level4 = root_node\n",
    "\n",
    "        # sample = [level1, level2, level3, level4]\n",
    "        sample = [level1, level2, level3]\n",
    "\n",
    "        graph_voc.add_sentence(sample)\n",
    "        code2sentence[code] = sample\n",
    "\n",
    "    return code2sentence, graph_voc\n",
    "\n",
    "\n",
    "# for medication\n",
    "def build_atc_tree(graph_voc, unique_codes):\n",
    "    code2sentence = {}\n",
    "\n",
    "    root_node = 'atc_root'\n",
    "    for code in unique_codes:\n",
    "        # sample = [code] + [code[:i] for i in [4, 3, 1]] + [root_node]\n",
    "        sample = [code[:i] for i in [4, 3, 1]]\n",
    "        graph_voc.add_sentence(sample)\n",
    "        code2sentence[code] = sample\n",
    "    \n",
    "    return code2sentence, graph_voc\n",
    "\n",
    "\n",
    "## MY ADDITION (for diagnosis):\n",
    "def expand_icd10_blocks():\n",
    "    icd10_blocks = [\n",
    "        # Chapter I: Certain infectious and parasitic diseases (A00â€“B99)\n",
    "        'A00-A09','A15-A19','A20-A28','A30-A49','A50-A64','A65-A69','A70-A74',\n",
    "        'A75-A79','A80-A89','A90-A99',\n",
    "        'B00-B09','B15-B19','B20-B24','B25-B34','B35-B49','B50-B64','B65-B83',\n",
    "        'B85-B89','B90-B94','B95-B97','B99',\n",
    "        # Chapter II: Neoplasms (C00â€“D49)\n",
    "        'C00-C14','C15-C26','C30-C39','C40-C41','C43-C44','C45-C49','C50','C51-C58',\n",
    "        'C60-C63','C64-C68','C69-C72','C73-C75','C76-C80','C81-C85','C88-C90',\n",
    "        'C91-C95','C96-C97','D00-D09','D10-D36','D37-D48',\n",
    "        # Chapter III: Blood/immune (D50â€“D89)\n",
    "        'D50-D53','D55-D59','D60-D64','D65-D69','D70-D77','D80-D89',\n",
    "        # Chapter IV: Endocrine/nutritional/metabolic (E00â€“E89)\n",
    "        'E00-E07','E08-E13','E15-E16','E20-E35','E36','E40-E46','E50-E64',\n",
    "        'E65-E68','E70-E88','E89',\n",
    "        # Chapter V: Mental/behavioural (F00-F99)\n",
    "        'F00-F09','F10-F19','F20-F29','F30-F39','F40-F48','F50-F59',\n",
    "        'F60-F69','F70-F79','F80-F89','F90-F98','F99',\n",
    "        # Chapter VI: Nervous system (G00-G99)\n",
    "        'G00-G09','G10-G14','G20-G26','G30-G32','G35-G37','G40-G47',\n",
    "        'G50-G59','G60-G64','G70-G73','G80-G83','G90-G99',\n",
    "        # Chapter VIIâ€“VIII: Eye/ear (H00-H95)\n",
    "        'H00-H05','H10-H13','H15-H22','H25-H28','H30-H36','H40-H42',\n",
    "        'H43-H45','H46-H48','H49-H52','H53-H54','H55-H59',\n",
    "        'H60-H62','H65-H75','H80-H83','H90-H95',\n",
    "        # Chapter IX, X, XI, XII, XIII, XIV\n",
    "        'I00-I02','I05-I09','I10-I15','I20-I25','I26-I28','I30-I52','I60-I69',\n",
    "        'I70-I79','I80-I89','I95-I99',\n",
    "        'J00-J06','J09-J18','J20-J22','J30-J39','J40-J47','J60-J70','J80-J84',\n",
    "        'J85-J86','J90-J94','J95-J99',\n",
    "        'K00-K14','K20-K31','K35-K38','K40-K46','K50-K52','K55-K64','K65-K68',\n",
    "        'K70-K77','K80-K87','K90-K95',\n",
    "        'L00-L08','L10-L14','L20-L30','L40-L45','L50-L54','L55-L59','L60-L75',\n",
    "        'L80-L99',\n",
    "        'M00-M03','M05-M14','M15-M19','M20-M25','M26-M27','M30-M36','M40-M54',\n",
    "        'M60-M63','M65-M68','M70-M79','M80-M85','M86-M90','M91-M94','M95-M99',\n",
    "        'N00-N08','N10-N16','N17-N19','N20-N23','N25-N29','N30-N39','N40-N51',\n",
    "        'N60-N64','N70-N77','N80-N98','N99',\n",
    "        # Chapter XVâ€“XXI: O00-O99, P00-P96, Q00-Q99, R00-R99, S00-T98, V01-Y98, Z00-Z99\n",
    "        'O00-O08','O09-O09','O10-O16','O20-O29','O30-O48','O60-O75','O80-O84',\n",
    "        'O85-O92','O94-O99',\n",
    "        'P00-P04','P05-P08','P10-P15','P20-P29','P35-P39','P50-P61','P70-P74',\n",
    "        'P76-P78','P80-P83','P90-P96',\n",
    "        'Q00-Q07','Q10-Q18','Q20-Q28','Q30-Q34','Q35-Q37','Q38-Q45','Q50-Q56',\n",
    "        'Q60-Q64','Q65-Q79','Q80-Q89','Q90-Q99',\n",
    "        'R00-R09','R10-R19','R20-R23','R25-R29','R30-R39','R40-R46','R47-R49',\n",
    "        'R50-R69','R70-R79','R80-R82','R83-R89','R90-R99',\n",
    "        'S00-S09','S10-S19','S20-S29','S30-S39','S40-S49','S50-S59','S60-S69',\n",
    "        'S70-S79','S80-S89','S90-S99','T00-T07','T08-T14','T15-T19','T20-T32',\n",
    "        'T33-T35','T36-T50','T51-T65','T66-T78','T79-T79','T80-T88','T90-T98',\n",
    "        'V01-V09','V10-V19','V20-V29','V30-V39','V40-V49','V50-V59','V60-V69',\n",
    "        'V70-V79','V80-V89','V90-V99','Y00-Y34','Y35-Y38','Y40-Y84','Y85-Y89',\n",
    "        'Y90-Y98','Z00-Z13','Z14-Z15','Z16','Z17','Z18','Z19','Z20-Z29','Z30-Z39',\n",
    "        'Z40-Z53','Z54','Z55-Z65','Z66','Z67','Z68','Z69-Z76','Z77-Z99',\n",
    "    ]\n",
    "    block_expand = {}\n",
    "    for block in icd10_blocks:\n",
    "        parts = block.split('-')\n",
    "        if len(parts) == 1:\n",
    "            block_expand[parts[0]] = block\n",
    "        else:\n",
    "            start_letter, start_num = parts[0][0], int(parts[0][1:])\n",
    "            end_num = int(parts[1][1:])\n",
    "            for i in range(start_num, end_num+1):\n",
    "                code3 = f\"{start_letter}{i:02d}\"\n",
    "                block_expand[code3] = block\n",
    "    return block_expand\n",
    "\n",
    "\n",
    "def build_icd10_tree(graph_voc, unique_codes):\n",
    "    code2sentence = {}\n",
    "    block_dict = expand_icd10_blocks()\n",
    "    for code in unique_codes:\n",
    "        short = code.split('.')[0][:3]\n",
    "        block = block_dict.get(short, 'Unknown')\n",
    "        sample = [code, short, block]\n",
    "        graph_voc.add_sentence(sample)\n",
    "        code2sentence[code] = sample\n",
    "    return code2sentence, graph_voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8208bf17-75a6-47ba-8d40-f36f6535b190",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Voc(object):\n",
    "    def __init__(self):\n",
    "        self.idx2word = {}\n",
    "        self.word2idx = {}\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence:\n",
    "            if word not in self.word2idx:\n",
    "                self.idx2word[len(self.word2idx)] = word\n",
    "                self.word2idx[word] = len(self.word2idx)\n",
    "\n",
    "\n",
    "class EHRTokenizer(object):\n",
    "    def __init__(self, diag_sentences, med_sentences, lab_sentences, pro_sentences, gender_set, age_set, age_gender_set=None, special_tokens=(\"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\")):\n",
    "\n",
    "        self.vocab = Voc()\n",
    "\n",
    "        # special tokens\n",
    "        self.vocab.add_sentence(special_tokens)\n",
    "        self.n_special_tokens = len(special_tokens)\n",
    "        self.age_voc = self.add_vocab(age_set)\n",
    "        self.diag_voc = self.add_vocab(diag_sentences)\n",
    "        self.med_voc = self.add_vocab(med_sentences)\n",
    "        self.lab_voc = self.add_vocab(lab_sentences)\n",
    "        if pro_sentences is not None:\n",
    "            self.pro_voc = self.add_vocab(pro_sentences)\n",
    "        else:\n",
    "            self.pro_voc = Voc()\n",
    "        self.gender_voc = self.add_vocab(gender_set)\n",
    "        if age_gender_set is not None:\n",
    "            self.age_gender_voc = self.add_vocab(age_gender_set)\n",
    "        else:\n",
    "            self.age_gender_voc = Voc()\n",
    "\n",
    "        assert len(special_tokens) + len(self.age_voc.idx2word) + len(self.diag_voc.idx2word) + len(self.med_voc.idx2word) + \\\n",
    "                len(self.lab_voc.idx2word) + len(self.pro_voc.idx2word) + len(self.gender_voc.idx2word) + len(self.age_gender_voc.idx2word) == len(self.vocab.idx2word)\n",
    "\n",
    "    def build_tree(self):\n",
    "        # create tree for diagnosis and medication\n",
    "        diag2tree, self.diag_tree_voc = build_icd9_tree(Voc(), list(self.diag_voc.idx2word.values()))\n",
    "        med2tree, self.med_tree_voc = build_atc_tree(Voc(), list(self.med_voc.idx2word.values()))\n",
    "        \n",
    "        diag_tree_table = []\n",
    "        for diag_id in range(len(self.diag_voc.idx2word)):\n",
    "            diag_tree = diag2tree[self.diag_voc.idx2word[diag_id]]  # [code1, code2, ...]\n",
    "            diag_tree_table.append([self.diag_tree_voc.word2idx[code] for code in diag_tree])\n",
    "        \n",
    "        med_tree_table = []\n",
    "        for med_id in range(len(self.med_voc.idx2word)):\n",
    "            med_tree = med2tree[self.med_voc.idx2word[med_id]]  # [code1, code2, ...]\n",
    "            med_tree_table.append([self.med_tree_voc.word2idx[code] for code in med_tree])\n",
    "        \n",
    "        # [n_diag/med, n_level]\n",
    "        self.diag_tree_table, self.med_tree_table = torch.tensor(diag_tree_table), torch.tensor(med_tree_table)\n",
    "\n",
    "    def add_vocab(self, sentences):\n",
    "        voc = self.vocab\n",
    "        specific_voc = Voc()\n",
    "        for sentence in sentences:\n",
    "            voc.add_sentence(sentence)\n",
    "            specific_voc.add_sentence(sentence)\n",
    "        return specific_voc\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens, voc_type=\"all\"):\n",
    "        \"\"\"Converts a sequence of tokens into ids using the vocab.\"\"\"\n",
    "        ids = []\n",
    "        for token in tokens:\n",
    "            if voc_type == \"all\":\n",
    "                ids.append(self.vocab.word2idx[token])\n",
    "            elif voc_type == \"diag\":\n",
    "                ids.append(self.diag_voc.word2idx[token])\n",
    "            elif voc_type == \"med\":\n",
    "                ids.append(self.med_voc.word2idx[token])\n",
    "            elif voc_type == \"lab\":\n",
    "                ids.append(self.lab_voc.word2idx[token])\n",
    "            elif voc_type == \"pro\":\n",
    "                ids.append(self.pro_voc.word2idx[token])\n",
    "        return ids\n",
    "\n",
    "    def convert_ids_to_tokens(self, ids, voc_type=\"all\"):\n",
    "        \"\"\"Converts a sequence of ids in wordpiece tokens using the vocab.\"\"\"\n",
    "        tokens = []\n",
    "        for i in ids:\n",
    "            if voc_type == \"all\":\n",
    "                tokens.append(self.vocab.idx2word[i])\n",
    "            elif voc_type == \"diag\":\n",
    "                tokens.append(self.diag_voc.idx2word[i])\n",
    "            elif voc_type == \"med\":\n",
    "                tokens.append(self.med_voc.idx2word[i])\n",
    "            elif voc_type == \"lab\":\n",
    "                tokens.append(self.lab_voc.idx2word[i])\n",
    "            elif voc_type == \"pro\":\n",
    "                tokens.append(self.pro_voc.idx2word[i])\n",
    "        return tokens\n",
    "    \n",
    "    def token_id_range(self, voc_type=\"diag\"):\n",
    "        age_size = len(self.age_voc.idx2word)\n",
    "        diag_size = len(self.diag_voc.idx2word)\n",
    "        med_size = len(self.med_voc.idx2word)\n",
    "        lab_size = len(self.lab_voc.idx2word)\n",
    "\n",
    "        if voc_type == \"diag\":\n",
    "            return [self.n_special_tokens + age_size, self.n_special_tokens + age_size + diag_size]\n",
    "        elif voc_type == \"med\":\n",
    "            return [self.n_special_tokens + age_size + diag_size, self.n_special_tokens + age_size + diag_size + med_size]\n",
    "        elif voc_type == \"lab\":\n",
    "            return [self.n_special_tokens + age_size + diag_size + med_size, self.n_special_tokens + age_size + diag_size + med_size + lab_size]\n",
    "        elif voc_type == \"pro\":\n",
    "            return [self.n_special_tokens + age_size + diag_size + med_size + lab_size, len(self.vocab.idx2word)]\n",
    "    \n",
    "    def token_number(self, voc_type=\"diag\"):\n",
    "        if voc_type == \"diag\":\n",
    "            return len(self.diag_voc.idx2word)\n",
    "        elif voc_type == \"med\":\n",
    "            return len(self.med_voc.idx2word)\n",
    "        elif voc_type == \"lab\":\n",
    "            return len(self.lab_voc.idx2word)\n",
    "        elif voc_type == \"pro\":\n",
    "            return len(self.pro_voc.idx2word)\n",
    "    \n",
    "    def random_token(self, voc_type=\"diag\"):\n",
    "        # randomly sample a token from the vocabulary\n",
    "        if voc_type == \"diag\":\n",
    "            return self.diag_voc.idx2word[np.random.randint(len(self.diag_voc.idx2word))]\n",
    "        elif voc_type == \"med\":\n",
    "            return self.med_voc.idx2word[np.random.randint(len(self.med_voc.idx2word))]\n",
    "        elif voc_type == \"lab\":\n",
    "            return self.lab_voc.idx2word[np.random.randint(len(self.lab_voc.idx2word))]\n",
    "        elif voc_type == \"pro\":\n",
    "            return self.pro_voc.idx2word[np.random.randint(len(self.pro_voc.idx2word))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "361ad713-54e3-4bf1-adad-fe51a1750736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class PretrainEHRDataset(Dataset):\n",
    "    def __init__(self, data_pd, tokenizer: EHRTokenizer, token_type=['diag', 'med', 'pro', 'lab']):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        def transform_data(data):\n",
    "            records, ages = {}, {}\n",
    "            genders = {}\n",
    "            for subject_id in data['subject_id'].unique():\n",
    "                item_df = data[data['subject_id'] == subject_id]\n",
    "                genders[subject_id] = [item_df.head(1)[\"gender\"].values[0]]\n",
    "\n",
    "                patient, age = [], []\n",
    "                for _, row in item_df.iterrows():\n",
    "                    admission = []\n",
    "                    if \"diag\" in token_type:\n",
    "                        admission.append(list(row['icd_code']))\n",
    "                    if \"med\" in token_type:\n",
    "                        admission.append(list(row['ndc']))\n",
    "                    if \"pro\" in token_type:\n",
    "                        admission.append(list(row['pro_code']))\n",
    "                    if \"lab\" in token_type:\n",
    "                        admission.append(list(row['lab_test']))\n",
    "                    patient.append(admission)\n",
    "                    age.append(row['age'])\n",
    "                records[subject_id] = list(patient)\n",
    "                ages[subject_id] = age\n",
    "            return records, ages, genders\n",
    "\n",
    "        self.records, self.ages, self.genders = transform_data(data_pd)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.records)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        subject_id = list(self.records.keys())[item]\n",
    "\n",
    "        input_tokens = ['[CLS]']\n",
    "        visit_positions = [0]\n",
    "        age_tokens = [self.ages[subject_id][0]]\n",
    "        for idx, adm in enumerate(self.records[subject_id]):  # each subject have multiple admissions, idx:visit id\n",
    "            cur_input_tokens = []\n",
    "            for i in range(len(adm)):  # one admission may have many kinds of entities, [[diag], [med], ...]\n",
    "                cur_input_tokens.extend(list(adm[i]))\n",
    "            if idx != len(self.records[subject_id]) - 1:  # add [SEP] token between visits\n",
    "                cur_input_tokens.append('[SEP]')\n",
    "            input_tokens.extend(cur_input_tokens)\n",
    "            visit_positions.extend([idx] * len(cur_input_tokens))\n",
    "            age_tokens.extend([self.ages[subject_id][idx]] * len(cur_input_tokens))\n",
    "\n",
    "        visit_positions = torch.tensor(visit_positions)\n",
    "        age_tokens = torch.tensor(self.tokenizer.convert_tokens_to_ids(age_tokens))\n",
    "\n",
    "        # masked token prediction\n",
    "        non_special_tokens_idx = [idx for idx, x in enumerate(input_tokens) if x != '[CLS]' and x != '[SEP]']\n",
    "        masked_tokens_idx = np.random.choice(non_special_tokens_idx, max(1, int(len(non_special_tokens_idx) * 0.15)))\n",
    "        masked_tokens = [input_tokens[idx] for idx in masked_tokens_idx]\n",
    "        masked_input_tokens = input_tokens.copy()\n",
    "        for idx in masked_tokens_idx:\n",
    "            if np.random.random() < 0.8:\n",
    "                masked_input_tokens[idx] = '[MASK]'\n",
    "            elif np.random.random() < 0.5:\n",
    "                # TODO: sample based on the entity type\n",
    "                masked_input_tokens[idx] = np.random.choice(list(self.tokenizer.diag_voc.word2idx.keys()))\n",
    "        masked_input_ids = torch.tensor(self.tokenizer.convert_tokens_to_ids(masked_input_tokens))\n",
    "        masked_tokens_idx = torch.tensor(masked_tokens_idx)\n",
    "        # TODO: consider all kinds of entities\n",
    "        masked_lm_labels = torch.tensor(self.tokenizer.convert_tokens_to_ids(masked_tokens, voc_type='diag'))\n",
    "        return masked_input_ids, visit_positions, age_tokens, masked_lm_labels, masked_tokens_idx\n",
    "\n",
    "\n",
    "class FinetuneEHRDataset(Dataset):\n",
    "    def __init__(self, data_pd, tokenizer: EHRTokenizer, token_type=['diag', 'med', 'pro', 'lab'], task='death'):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.task = task\n",
    "\n",
    "        def transform_data(data, task):\n",
    "            age_records = {}\n",
    "            hadm_records = {}  # including current admission and previous admissions\n",
    "            genders = {}\n",
    "            labels = {}\n",
    "            for subject_id in data['subject_id'].unique():\n",
    "                item_df = data[data['subject_id'] == subject_id]\n",
    "                patient, ages = [], []\n",
    "                \n",
    "                for _, row in item_df.iterrows():\n",
    "                    admission = []\n",
    "                    hadm_id = row['hadm_id']\n",
    "                    if \"diag\" in token_type:\n",
    "                        admission.append(list(row['icd_code']))\n",
    "                    if \"med\" in token_type:\n",
    "                        admission.append(list(row['ndc']))\n",
    "                    if \"pro\" in token_type:\n",
    "                        admission.append(list(row['pro_code']))\n",
    "                    if \"lab\" in token_type:\n",
    "                        admission.append(list(row['lab_test']))\n",
    "                    patient.append(admission)\n",
    "                    ages.append(row['age'])\n",
    "                    if task in [\"death\", \"stay\", \"readmission\"]:  # binary prediction\n",
    "                        hadm_records[hadm_id] = list(patient)\n",
    "                        age_records[hadm_id] = ages\n",
    "                        genders[hadm_id] = [item_df.head(1)[\"gender\"].values[0]]\n",
    "                        if \"READMISSION\" in row:\n",
    "                            labels[hadm_id] = [row[\"DEATH\"], row[\"STAY_DAYS\"], row[\"READMISSION\"]]\n",
    "                        else:\n",
    "                            labels[hadm_id] = [row[\"DEATH\"], row[\"STAY_DAYS\"]]\n",
    "                    else:  # next diagnosis prediction\n",
    "                        label = row[\"NEXT_DIAG_6M\"] if task == \"next_diag_6m\" else row[\"NEXT_DIAG_12M\"]\n",
    "                        if str(label) != \"nan\":  # only include the admission with next diagnosis\n",
    "                            hadm_records[hadm_id] = list(patient)\n",
    "                            age_records[hadm_id] = ages\n",
    "                            genders[hadm_id] = [item_df.head(1)[\"gender\"].values[0]]\n",
    "                            labels[hadm_id] = list(label)\n",
    "\n",
    "            return hadm_records, age_records, genders, labels\n",
    "        self.records, self.ages, self.genders, self.labels = transform_data(data_pd, task)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.records)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        hadm_id = list(self.records.keys())[item]\n",
    "\n",
    "        input_tokens = ['[CLS]']\n",
    "        visit_positions = [0]\n",
    "        age_tokens = [self.ages[hadm_id][0]]\n",
    "        for idx, adm in enumerate(self.records[hadm_id]):  # each subject have multiple admissions, idx:visit id\n",
    "            cur_input_tokens = []\n",
    "            for i in range(len(adm)):  # one admission may have many kinds of entities, [[diag], [med], ...]\n",
    "                cur_input_tokens.extend(list(adm[i]))\n",
    "            if idx != len(self.records[hadm_id]) - 1:  # add [SEP] token between visits\n",
    "                cur_input_tokens.append('[SEP]')\n",
    "            input_tokens.extend(cur_input_tokens)\n",
    "            visit_positions.extend([idx] * len(cur_input_tokens))\n",
    "            age_tokens.extend([self.ages[hadm_id][idx]] * len(cur_input_tokens))\n",
    "\n",
    "        if self.task == \"death\":\n",
    "            # predict if the patient will die in the hospital\n",
    "            labels = torch.tensor([self.labels[hadm_id][0]]).float()\n",
    "        elif self.task == \"stay\":\n",
    "            # predict if the patient will stay in the hospital for more than 7 days\n",
    "            labels = (torch.tensor([self.labels[hadm_id][1]]) > 7).float()\n",
    "        elif self.task == \"readmission\":\n",
    "            # predict if the patient will be readmitted within 1 month\n",
    "            labels = torch.tensor([self.labels[hadm_id][2]]).float()\n",
    "        else:\n",
    "            # predict the next diagnosis in 6 months or 12 months\n",
    "            input_tokens.extend(['[SEP]', '[MASK]'])\n",
    "            visit_positions.extend([visit_positions[-1]+1, visit_positions[-1]+1])\n",
    "            age_tokens.extend([age_tokens[-1], age_tokens[-1]])\n",
    "            label_ids = torch.tensor(self.tokenizer.convert_tokens_to_ids(self.labels[hadm_id], voc_type='diag'))\n",
    "            labels = torch.zeros(self.tokenizer.token_number(voc_type='diag')).long()\n",
    "            labels[label_ids] = 1  # multi-hop vector\n",
    "\n",
    "        visit_positions = torch.tensor(visit_positions)\n",
    "        age_tokens = torch.tensor(self.tokenizer.convert_tokens_to_ids(age_tokens))\n",
    "        input_ids = torch.tensor(self.tokenizer.convert_tokens_to_ids(input_tokens))\n",
    "\n",
    "        return input_ids, visit_positions, age_tokens, labels\n",
    "\n",
    "\n",
    "def batcher(tokenizer):\n",
    "    def batcher_dev(batch):\n",
    "        input_ids, visit_positions, age_tokens = [feat[0] for feat in batch], [feat[1] for feat in batch], [feat[2] for feat in batch]\n",
    "        if len(batch[0]) > 3:\n",
    "            batch_data = []\n",
    "            for d in range(3, len(batch[0])):\n",
    "                data = [feat[d] for feat in batch]\n",
    "                max_len = max([len(x) for x in data])\n",
    "                data = [F.pad(x, (0, max_len - len(x)), \"constant\", 0) for x in data]\n",
    "                data = torch.stack(data)\n",
    "                batch_data.append(data)\n",
    "\n",
    "        # padding\n",
    "        pad_id = tokenizer.vocab.word2idx['[PAD]']\n",
    "        max_len = max([len(x) for x in input_ids])\n",
    "        input_ids = [F.pad(x, (0, max_len - len(x)), \"constant\", pad_id) for x in input_ids]\n",
    "        visit_positions = [F.pad(x, (0, max_len - len(x)), \"constant\", pad_id) for x in visit_positions]\n",
    "        age_tokens = [F.pad(x, (0, max_len - len(x)), \"constant\", pad_id) for x in age_tokens]\n",
    "        \n",
    "        input_ids = torch.stack(input_ids)\n",
    "        visit_positions = torch.stack(visit_positions)\n",
    "        age_tokens = torch.stack(age_tokens)\n",
    "\n",
    "        if len(batch[0]) > 3:\n",
    "            return input_ids, age_tokens, visit_positions, *batch_data\n",
    "        else:\n",
    "            return input_ids, age_tokens, visit_positions\n",
    "    \n",
    "    return batcher_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a729d01-ccb0-42f6-9bcd-8a63ad946607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pad_sequence(seqs, pad_id=0):\n",
    "    # seqs: a list of tensor [n, m]\n",
    "    max_len = max([x.shape[1] for x in seqs])\n",
    "    return torch.cat([F.pad(x, (0, max_len - x.shape[1]), \"constant\", pad_id) for x in seqs], dim=0)\n",
    "\n",
    "\n",
    "class HBERTPretrainEHRDataset(PretrainEHRDataset):\n",
    "    def __init__(self, data_pd, tokenizer: EHRTokenizer, token_type=['diag', 'med', 'pro', 'lab'], mask_rate=0.15, anomaly_rate=0.1):\n",
    "        super().__init__(data_pd, tokenizer, token_type)\n",
    "        \n",
    "        self.mask_rate = mask_rate\n",
    "        self.anomaly_rate = anomaly_rate\n",
    "        self.token_type = token_type\n",
    "        self.token_type_map = {i:t for i, t in enumerate(token_type)}\n",
    "\n",
    "    def _id2multi_hot(self, ids, dim):\n",
    "        multi_hot = torch.zeros(dim)\n",
    "        multi_hot[ids] = 1\n",
    "        return multi_hot\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        subject_id = list(self.records.keys())[item]\n",
    "\n",
    "        input_tokens, token_types, masked_labels, anomaly_labels = [], [], [None for _ in range(len(self.token_type))], []\n",
    "        for idx, adm in enumerate(self.records[subject_id]):  # each subject have multiple admissions, idx:visit id\n",
    "            adm_tokens, adm_token_types, adm_masked_labels = [str(self.ages[subject_id][idx]) + \"_\" + str(self.genders[subject_id][0])], [0], []  # replace [CLS] token with age\n",
    "            adm_anomaly_labels = []\n",
    "\n",
    "            for i in range(len(adm)):  # one admission have many kinds of entities, [[diag], [med], ...]\n",
    "                cur_tokens = list(adm[i])\n",
    "\n",
    "                # randomly mask tokens\n",
    "                non_special_tokens_idx = [idx for idx, x in enumerate(cur_tokens)]\n",
    "                masked_tokens_idx = np.random.choice(non_special_tokens_idx, max(1, int(len(non_special_tokens_idx) * self.mask_rate)))\n",
    "                masked_tokens = [cur_tokens[idx] for idx in masked_tokens_idx]\n",
    "                masked_tokens_idx_ = set(masked_tokens_idx.tolist())  # for fast lookup\n",
    "                non_masked_tokens = [cur_tokens[idx] for idx in non_special_tokens_idx if idx not in masked_tokens_idx_]\n",
    "\n",
    "                # randomly replace tokens with other tokens\n",
    "                if self.anomaly_rate > 0 and len(non_masked_tokens) > 0:\n",
    "                    candidate_token_idx = [idx for idx, x in enumerate(non_masked_tokens)]\n",
    "                    anomaly_tokens_idx = np.random.choice(candidate_token_idx, max(1, int(len(candidate_token_idx) * self.anomaly_rate)))\n",
    "                    for ano_idx in anomaly_tokens_idx:\n",
    "                        non_masked_tokens[ano_idx] = self.tokenizer.random_token(voc_type=self.token_type_map[i])\n",
    "                        adm_anomaly_labels.append(len(adm_tokens) + ano_idx + 1)  # the position of the anomaly token, +1 for [MASK] tokens\n",
    "\n",
    "                adm_tokens.extend([f\"[MASK{i}]\"] + non_masked_tokens)  # [[MASK1], diag1, diag2, [MASK2], med1, med2]\n",
    "                adm_token_types.extend([i + 1] * (len(non_masked_tokens) + 1))  # [0, 1, 1, 2, 2]\n",
    "                adm_masked_labels.append(masked_tokens)  # [[diag1, diag2], [med1, med2]]\n",
    "\n",
    "            input_tokens.append(torch.tensor([self.tokenizer.convert_tokens_to_ids(adm_tokens)]))\n",
    "            token_types.append(torch.tensor([adm_token_types]))\n",
    "            for i in range(len(self.token_type)):\n",
    "                label_ids = self.tokenizer.convert_tokens_to_ids(adm_masked_labels[i], voc_type=self.token_type_map[i])\n",
    "                label_hop = self._id2multi_hot(label_ids, dim=self.tokenizer.token_number(self.token_type_map[i])).unsqueeze(dim=0)\n",
    "                if masked_labels[i] is None:\n",
    "                    masked_labels[i] = label_hop\n",
    "                else:\n",
    "                    masked_labels[i] = torch.cat([masked_labels[i], label_hop])\n",
    "            \n",
    "            if len(adm_anomaly_labels) > 0:\n",
    "                anomaly_labels.append(self._id2multi_hot(adm_anomaly_labels, dim=len(adm_tokens)).unsqueeze(dim=0))\n",
    "            else:\n",
    "                anomaly_labels.append(torch.zeros(len(adm_tokens)).unsqueeze(dim=0))\n",
    "\n",
    "        visit_positions = torch.tensor(list(range(len(input_tokens))))  # [0, 1, 2, ...]\n",
    "        input_tokens = _pad_sequence(input_tokens, pad_id=self.tokenizer.vocab.word2idx[\"[PAD]\"])\n",
    "        token_types = _pad_sequence(token_types, pad_id=0)\n",
    "        anomaly_labels = _pad_sequence(anomaly_labels, pad_id=0) if len(anomaly_labels) > 0 else None\n",
    "        n_adms = len(input_tokens)\n",
    "        if n_adms > 1:\n",
    "            # create a fully connected graph between admission\n",
    "            edge_index = torch.tensor([[i, j] for i in range(n_adms) for j in range(n_adms)]).t()  # [2, n_adms * n_adms]\n",
    "        else:\n",
    "            edge_index = torch.tensor([])\n",
    "        return input_tokens, token_types, edge_index, visit_positions, masked_labels, anomaly_labels\n",
    "\n",
    "\n",
    "class HBERTFinetuneEHRDataset(FinetuneEHRDataset):\n",
    "    def __init__(self, data_pd, tokenizer, token_type=['diag', 'med', 'pro', 'lab'], task='death'):\n",
    "        super().__init__(data_pd, tokenizer, token_type, task)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        hadm_id = list(self.records.keys())[item]\n",
    "\n",
    "        input_tokens, token_types = [], []\n",
    "        for idx, adm in enumerate(self.records[hadm_id]):  # each subject have multiple admissions, idx:visit id\n",
    "            adm_tokens = [str(self.ages[hadm_id][idx]) + \"_\" + self.genders[hadm_id][0]]  # replace [CLS] token with age\n",
    "            # adm_tokens = [self.ages[hadm_id][idx]]  # replace [CLS] token with age\n",
    "            adm_token_types = [0]\n",
    "\n",
    "            for i in range(len(adm)):\n",
    "                cur_tokens = list(adm[i])\n",
    "                adm_tokens.extend(cur_tokens)\n",
    "                adm_token_types.extend([i + 1] * len(cur_tokens))\n",
    "            \n",
    "            # input_tokens.append(torch.tensor([self.tokenizer.convert_tokens_to_ids(adm_tokens)]))\n",
    "            input_tokens.append(adm_tokens)\n",
    "            # token_types.append(torch.tensor([adm_token_types]))\n",
    "            token_types.append(adm_token_types)\n",
    "\n",
    "        if self.task == \"death\":\n",
    "            # predict if the patient will die in the hospital\n",
    "            labels = torch.tensor([self.labels[hadm_id][0]]).float()\n",
    "        elif self.task == \"stay\":\n",
    "            # predict if the patient will stay in the hospital for more than 7 days\n",
    "            labels = (torch.tensor([self.labels[hadm_id][1]]) > 7).float()\n",
    "        elif self.task == \"readmission\":\n",
    "            # predict if the patient will be readmitted within 1 month\n",
    "            labels = torch.tensor([self.labels[hadm_id][2]]).float()\n",
    "        else:\n",
    "            # predict the next diagnosis in 6 months or 12 months\n",
    "            input_tokens[-1] = [input_tokens[-1][0]] + [\"[MASK0]\"] + input_tokens[-1][1:]\n",
    "            token_types[-1] = [token_types[-1][0]] + [1] + token_types[-1][1:]\n",
    "            label_ids = torch.tensor(self.tokenizer.convert_tokens_to_ids(self.labels[hadm_id], voc_type='diag'))\n",
    "            labels = torch.zeros(self.tokenizer.token_number(voc_type='diag')).long()\n",
    "            labels[label_ids] = 1  # multi-hop vector\n",
    "\n",
    "        visit_positions = torch.tensor(list(range(len(input_tokens))))  # [0, 1, 2, ...]\n",
    "        input_tokens = [torch.tensor([self.tokenizer.convert_tokens_to_ids(x)]) for x in input_tokens]\n",
    "        token_types = [torch.tensor([x]) for x in token_types]\n",
    "        input_tokens = _pad_sequence(input_tokens, pad_id=self.tokenizer.vocab.word2idx[\"[PAD]\"])\n",
    "        token_types = _pad_sequence(token_types, pad_id=0)\n",
    "        n_adms = len(input_tokens)\n",
    "        if n_adms > 1:\n",
    "            # create a fully connected graph between admission\n",
    "            edge_index = torch.tensor([[i, j] for i in range(n_adms) for j in range(n_adms)]).t()  # [2, n_adms * n_adms]\n",
    "        else:\n",
    "            edge_index = torch.tensor([])\n",
    "        \n",
    "        return input_tokens, token_types, edge_index, visit_positions, labels\n",
    "\n",
    "\n",
    "def batcher(tokenizer, n_token_type=3, is_train=True):\n",
    "    def batcher_dev(batch):\n",
    "        raw_input_ids, raw_input_types, raw_edge_indexs, raw_visit_positions, raw_labels = [feat[0] for feat in batch], [feat[1] for feat in batch], [feat[2] for feat in batch], [feat[3] for feat in batch], [feat[4] for feat in batch]\n",
    "\n",
    "        pad_id = tokenizer.vocab.word2idx[\"[PAD]\"]\n",
    "        max_n_tokens = max([x.size(1) for x in raw_input_ids])\n",
    "        input_ids = torch.cat([F.pad(raw_input_id, (0, max_n_tokens - raw_input_id.size(1)), \"constant\", pad_id) for raw_input_id in raw_input_ids], dim=0)\n",
    "\n",
    "        max_n_token_types = max([x.size(1) for x in raw_input_types])\n",
    "        input_types = torch.cat([F.pad(raw_input_type, (0, max_n_token_types - raw_input_type.size(1)), \"constant\", 0) for raw_input_type in raw_input_types], dim=0)\n",
    "\n",
    "        n_cumsum_nodes = [0] + np.cumsum([input_id.size(0) for input_id in raw_input_ids]).tolist()\n",
    "        edge_index = []\n",
    "        for i, raw_edge_index in enumerate(raw_edge_indexs):\n",
    "            if raw_edge_index.shape[0] > 0:\n",
    "                edge_index.append(raw_edge_index + n_cumsum_nodes[i])\n",
    "        edge_index = torch.cat(edge_index, dim=1) if len(edge_index) > 0 else None\n",
    "\n",
    "        visit_positions = torch.cat(raw_visit_positions, dim=0)\n",
    "\n",
    "        if is_train:\n",
    "            labels = []  # [n_token_type, B, n_tokens], each element is a multi-hop label tensor\n",
    "            for i in range(n_token_type):\n",
    "                labels.append(torch.cat([x[i] for x in raw_labels]))\n",
    "            \n",
    "            raw_anomaly_labels = [feat[5] for feat in batch]\n",
    "            if raw_anomaly_labels[0] is not None:\n",
    "                max_n_anomaly_labels = max([x.size(1) for x in raw_anomaly_labels])\n",
    "                anomaly_labels = torch.cat([F.pad(raw_anomaly_label, (0, max_n_anomaly_labels - raw_anomaly_label.size(1)), \"constant\", 0) for raw_anomaly_label in raw_anomaly_labels], dim=0)\n",
    "            else:\n",
    "                anomaly_labels = None\n",
    "            return input_ids, input_types, edge_index, visit_positions, labels, anomaly_labels\n",
    "        else:\n",
    "            labels = torch.stack(raw_labels, dim=0)\n",
    "            labeled_batch_idx = [n - 1 for n in n_cumsum_nodes[1:]]  # indicate the index of the to-be-predicted admission\n",
    "            return input_ids, input_types, edge_index, visit_positions, labeled_batch_idx, labels\n",
    "    \n",
    "    return batcher_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533edec1-7f01-4fd4-8a94-18f14bba6767",
   "metadata": {},
   "source": [
    "## MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "316ae0d2-56c2-47df-958c-2057349a89be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "\n",
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.dropout(gelu(self.fc1(x))))\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.d_k, self.n_heads = config.hidden_size // config.num_attention_heads, config.num_attention_heads\n",
    "\n",
    "        self.W_Q = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.W_K = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.W_V = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.W_output = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "    \n",
    "    def ScaledDotProductAttention(self, Q, K, V, attn_mask):\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / math.sqrt(Q.size(-1)) # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
    "        attn = self.dropout(nn.Softmax(dim=-1)(scores))\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context, attn\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        batch_size = Q.size(0)\n",
    "\n",
    "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n",
    "        k_s = self.W_K(K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n",
    "        v_s = self.W_V(V).view(batch_size, -1, self.n_heads, self.d_k).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n",
    "\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.n_heads, 1, 1) # attn_mask : [batch_size x n_heads x len_q x len_k]\n",
    "\n",
    "        # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        context, attn = self.ScaledDotProductAttention(q_s, k_s, v_s, attn_mask)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_k) # context: [batch_size x len_q x n_heads * d_v]\n",
    "        return self.W_output(context)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(config)\n",
    "        self.pos_ffn = PoswiseFeedForwardNet(config)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.norm_attn = nn.LayerNorm(config.hidden_size)\n",
    "        self.norm_ffn = nn.LayerNorm(config.hidden_size)\n",
    "\n",
    "    def forward(self, x, self_attn_mask):\n",
    "        norm_x = self.norm_attn(x)\n",
    "        x = x + self.dropout(self.self_attn(norm_x, norm_x, norm_x, self_attn_mask))\n",
    "\n",
    "        norm_x = self.norm_ffn(x)\n",
    "        x = x + self.dropout(self.pos_ffn(norm_x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69485332-551b-40ad-beb7-c86bf082ee21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import einsum\n",
    "\n",
    "class EdgeModule(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(EdgeModule, self).__init__()\n",
    "\n",
    "        self.n_types = 4 + 1  # +1 for [CLS]\n",
    "\n",
    "        self.left_transform = nn.Parameter(torch.zeros(self.n_types, config.hidden_size, config.hidden_size))\n",
    "        self.right_transform = nn.Parameter(torch.zeros(self.n_types, config.hidden_size, config.hidden_size))\n",
    "        self.output = nn.Linear(config.hidden_size * 2, config.edge_hidden_size)\n",
    "\n",
    "        self.init_parameters()\n",
    "\n",
    "    def init_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.left_transform)\n",
    "        nn.init.xavier_uniform_(self.right_transform)\n",
    "\n",
    "    def forward(self, token_embs, token_types):\n",
    "        # token_embs: [batch_size, seq_len, hidden_size], token_types: [batch_size, seq_len]\n",
    "        batch_size, seq_len, hidden_size = token_embs.size()\n",
    "\n",
    "        # encode token according to its type\n",
    "        left_trans = self.left_transform[token_types]  # [batch_size, seq_len, hidden_size, hidden_size]\n",
    "        right_trans = self.right_transform[token_types]  # [batch_size, seq_len, hidden_size, hidden_size]\n",
    "\n",
    "        left_embs = einsum(token_embs, left_trans, 'b l d, b l m d -> b l m')  # [batch_size, seq_len, hidden_size]\n",
    "        right_embs = einsum(token_embs, right_trans, 'b l d, b l m d -> b l m')  # [batch_size, seq_len, hidden_size]\n",
    "\n",
    "        edge_embs = torch.cat((left_embs.unsqueeze(dim=2).repeat(1, 1, seq_len, 1), \n",
    "                               right_embs.unsqueeze(dim=1).repeat(1, seq_len, 1, 1)), dim=-1)\n",
    "        return self.output(edge_embs)\n",
    "\n",
    "\n",
    "class MultiHeadEdgeAttention(MultiHeadAttention):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.d_edge = config.edge_hidden_size\n",
    "        self.W_output = nn.Linear(config.hidden_size * 2, config.hidden_size)\n",
    "        self.W_K_edge = nn.Linear(config.edge_hidden_size, config.edge_hidden_size)\n",
    "        self.W_edge = nn.Linear(config.edge_hidden_size, 1)\n",
    "        self.W_edge_output = nn.Linear(self.d_edge * self.n_heads, config.hidden_size)\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask, edge_embs):\n",
    "        batch_size, n_tokens = Q.size(0), Q.size(1)\n",
    "\n",
    "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n",
    "        k_s = self.W_K(K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n",
    "        v_s = self.W_V(V).view(batch_size, -1, self.n_heads, self.d_k).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n",
    "\n",
    "        k_s_edge = self.W_K_edge(edge_embs).view(batch_size, n_tokens, n_tokens, -1)\n",
    "        edge_bias = self.W_edge(edge_embs).view(batch_size, 1, n_tokens, n_tokens)\n",
    "        edge_bias = edge_bias * (2 ** -0.5)\n",
    "\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.n_heads, 1, 1) # attn_mask : [batch_size x n_heads x len_q x len_k]\n",
    "\n",
    "        # bias attention scores with edge representation\n",
    "        scores = torch.matmul(q_s, k_s.transpose(-1, -2)) * ((2 * q_s.size(-1)) ** -0.5)\n",
    "        scores = scores + edge_bias\n",
    "        scores.masked_fill_(attn_mask, -1e9)  # [batch_size, n_heads, n_tokens, n_tokens]\n",
    "        attn = self.dropout(nn.Softmax(dim=-1)(scores))\n",
    "        \n",
    "        # add edge context into aggregated context\n",
    "        context = torch.matmul(attn, v_s)\n",
    "        edge_context = einsum(attn, k_s_edge, 'b h n m, b n m d -> b h n d')  # [batch_size, n_tokens, n_heads, d_k]\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_k) # context: [batch_size x len_q x n_heads * d_v]\n",
    "        edge_context = edge_context.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_edge)\n",
    "        edge_context = self.W_edge_output(edge_context)\n",
    "        return self.W_output(torch.cat([context, edge_context], dim=-1))\n",
    "\n",
    "\n",
    "class EdgeTransformerBlock(TransformerBlock):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.self_attn = MultiHeadEdgeAttention(config)\n",
    "        self.norm_edge = nn.LayerNorm(config.edge_hidden_size)\n",
    "\n",
    "    def forward(self, x, edge_embs, self_attn_mask):\n",
    "        norm_x = self.norm_attn(x)\n",
    "        norm_edge_embs = self.norm_edge(edge_embs)\n",
    "        x = x + self.dropout(self.self_attn(norm_x, norm_x, norm_x, self_attn_mask, norm_edge_embs))\n",
    "\n",
    "        norm_x = self.norm_ffn(x)\n",
    "        x = x + self.dropout(self.pos_ffn(norm_x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "489d32fc-36d6-402f-bfd2-b68f4aec70b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.utils import softmax\n",
    "from torch_scatter import scatter_mean, scatter_sum\n",
    "\n",
    "\n",
    "class DotAttnConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, n_heads=1, n_max_visits=15, temp=1.):\n",
    "        super(DotAttnConv, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.n_heads, self.temp = n_heads, temp\n",
    "\n",
    "        self.pos_encoding = nn.Embedding(n_max_visits, in_channels)\n",
    "        self.W_q = nn.Linear(in_channels, out_channels, bias=False)\n",
    "        self.W_k = nn.Linear(in_channels, out_channels, bias=False)\n",
    "        self.W_v = nn.Linear(in_channels, out_channels, bias=False)\n",
    "        self.W_out = nn.Linear(out_channels, out_channels, bias=False)\n",
    "        self.ln = nn.LayerNorm(out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, visit_pos):\n",
    "        # x: [N, in_channels], edge_index: [2, E]\n",
    "        N, device = x.size(0), x.device\n",
    "        isolated_nodes_mask = ~torch.isin(torch.arange(N).to(x.device), edge_index[1].unique())\n",
    "        isolated_nodes = isolated_nodes_mask.nonzero(as_tuple=False).squeeze()\n",
    "\n",
    "        pos_encoding = self.pos_encoding(visit_pos)\n",
    "        h_q, h_k, h_v = self.W_q(x + pos_encoding), self.W_k(x + pos_encoding), self.W_k(x)\n",
    "        h_q, h_k, h_v = h_q.reshape(N, self.n_heads, -1), h_k.reshape(N, self.n_heads, -1), h_v.reshape(N, self.n_heads, -1)\n",
    "        \n",
    "        attn_scores = torch.sum(h_q[edge_index[0]] * h_k[edge_index[1]], dim=-1) / self.temp  # [N_edges, n_heads]\n",
    "        dst_nodes = torch.cat([edge_index[1] + N*i for i in range(self.n_heads)], dim=0).to(device)\n",
    "        attn_scores = softmax(attn_scores.reshape(-1), dst_nodes, num_nodes=N * self.n_heads).unsqueeze(dim=-1)  # [N_edges * n_heads, 1]\n",
    "\n",
    "        # aggregation\n",
    "        h_v = h_v.permute(1, 0, 2).reshape(N*self.n_heads, -1)\n",
    "        src_nodes = torch.cat([edge_index[0] + N*i for i in range(self.n_heads)], dim=0).to(device)\n",
    "        out = scatter_sum(src=h_v[src_nodes] * attn_scores, index=dst_nodes, dim_size=N * self.n_heads, dim=0)\n",
    "        out = out.reshape(self.n_heads, N, -1).permute(1, 0, 2).reshape(N, -1)\n",
    "\n",
    "        out = self.W_out(self.ln(out)) + x\n",
    "        out[isolated_nodes] = x[isolated_nodes]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cb3ee9-d723-4ec7-a2e2-4c1f4dea0d65",
   "metadata": {},
   "source": [
    "HEART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3efc328e-b05a-4246-b453-a58a0d4ce842",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeEmbeddings(nn.Module):\n",
    "    def __init__(self, config, diag_tree_table, med_tree_table, n_diag_tokens, n_med_tokens, diag_range, med_range):\n",
    "        super(TreeEmbeddings, self).__init__()\n",
    "        # tree_table: [n_diag/n_med, n_level]\n",
    "        self.n_dim = config.hidden_size\n",
    "        self.diag_range, self.med_range = diag_range, med_range\n",
    "        self.diag_tree_table, self.med_tree_table = diag_tree_table, med_tree_table\n",
    "\n",
    "        self.diag_tokens = nn.Embedding(n_diag_tokens, config.hidden_size // diag_tree_table.shape[1])\n",
    "        self.med_tokens = nn.Embedding(n_med_tokens, config.hidden_size // med_tree_table.shape[1])\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=0)  # [PAD] token\n",
    "        self.emb_dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, input_ids, token_types):\n",
    "        B, N = input_ids.shape[0], input_ids.shape[1]\n",
    "\n",
    "        # concat the embedding at each layer\n",
    "        diag_tree_tokens = self.diag_tokens(self.diag_tree_table.to(input_ids.device)).reshape(-1, self.n_dim)\n",
    "        med_tree_tokens = self.med_tokens(self.med_tree_table.to(input_ids.device)).reshape(-1, self.n_dim)\n",
    "\n",
    "        input_ids = input_ids.reshape(-1)\n",
    "        diag_mask = (input_ids >= self.diag_range[0]) * (input_ids < self.diag_range[1])\n",
    "        med_mask = (input_ids >= self.med_range[0]) * (input_ids < self.med_range[1])\n",
    "\n",
    "        words_embeddings = self.word_embeddings(input_ids)\n",
    "        diag_embeddings = diag_tree_tokens[input_ids[diag_mask] - self.diag_range[0]]\n",
    "        med_embeddings = med_tree_tokens[input_ids[med_mask] - self.med_range[0]]\n",
    "\n",
    "        # replace the diagnosis and medication embeddings with tree embeddings\n",
    "        words_embeddings[diag_mask] = diag_embeddings\n",
    "        words_embeddings[med_mask] = med_embeddings\n",
    "        words_embeddings = words_embeddings.reshape(B, N, -1)\n",
    "\n",
    "        # words_embeddings = words_embeddings + self.type_embedding(token_types)\n",
    "\n",
    "        return self.emb_dropout(words_embeddings)\n",
    "\n",
    "\n",
    "class HBERTEmbeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(HBERTEmbeddings, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=0)  # [PAD] token\n",
    "        self.emb_dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, input_ids, token_types):\n",
    "        # embedding the indexed sequence to sequence of vectors\n",
    "        words_embeddings = self.word_embeddings(input_ids)\n",
    "        return self.emb_dropout(words_embeddings)\n",
    "\n",
    "\n",
    "# Hierarchical Transformer\n",
    "class HiTransformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(HiTransformer, self).__init__()\n",
    "        \n",
    "        # multi-layers transformer blocks, deep network\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "            [TransformerBlock(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "        # multi-layers transformer blocks, deep network\n",
    "        if config.gat == \"dotattn\":\n",
    "            self.cross_attentions = nn.ModuleList(\n",
    "                [DotAttnConv(config.hidden_size, config.hidden_size, config.gnn_n_heads, config.max_visit_size, config.gnn_temp) for _ in range(config.num_hidden_layers)])\n",
    "        elif config.gat == \"None\":\n",
    "            self.cross_attentions = None\n",
    "\n",
    "    def forward(self, x, edge_index, mask, visit_positions):\n",
    "        # running over multiple transformer blocks\n",
    "        for i in range(len(self.transformer_blocks)):\n",
    "            x = self.transformer_blocks[i](x, mask)  # [B, L, D]\n",
    "            if edge_index is not None and self.cross_attentions is not None:\n",
    "                x = torch.cat([self.cross_attentions[i](x[:, 0], edge_index, visit_positions).unsqueeze(dim=1), \n",
    "                                x[:, 1:]], dim=1)  # communicate between visits\n",
    "        return x\n",
    "\n",
    "\n",
    "# Hierarchical Transformer with Edge Representation\n",
    "class HiEdgeTransformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(HiEdgeTransformer, self).__init__()\n",
    "\n",
    "        self.edge_module = EdgeModule(config)\n",
    "        \n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "            [EdgeTransformerBlock(config) for _ in range(config.num_hidden_layers)])\n",
    "        \n",
    "        if config.gat == \"dotattn\":\n",
    "            self.cross_attentions = nn.ModuleList(\n",
    "                [DotAttnConv(config.hidden_size, config.hidden_size, config.gnn_n_heads, config.max_visit_size, config.gnn_temp) for _ in range(config.num_hidden_layers)])\n",
    "        elif config.gat == \"None\":\n",
    "            self.cross_attentions = None\n",
    "\n",
    "    def forward(self, x, x_types, edge_index, mask, visit_positions):\n",
    "        edge_embs = self.edge_module(x, x_types)  # [B, L, L, D]\n",
    "        for i in range(len(self.transformer_blocks)):\n",
    "            x = self.transformer_blocks[i](x, edge_embs, mask)  # [B, L, D]\n",
    "            if edge_index is not None and self.cross_attentions is not None:\n",
    "                x = torch.cat([self.cross_attentions[i](x[:, 0], edge_index, visit_positions).unsqueeze(dim=1), \n",
    "                                x[:, 1:]], dim=1)  # communicate between visits\n",
    "        return x\n",
    "\n",
    "\n",
    "class MaskedPredictionHead(nn.Module):\n",
    "    def __init__(self, config, voc_size):\n",
    "        super(MaskedPredictionHead, self).__init__()\n",
    "        self.cls = nn.Sequential(\n",
    "                nn.Linear(config.hidden_size, config.hidden_size), \n",
    "                nn.ReLU(), \n",
    "                nn.Linear(config.hidden_size, voc_size)\n",
    "            )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.cls(input)\n",
    "\n",
    "\n",
    "# binary classification task\n",
    "class BinaryPredictionHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BinaryPredictionHead, self).__init__()\n",
    "        self.cls = nn.Sequential(\n",
    "                nn.Linear(config.hidden_size, config.hidden_size), \n",
    "                nn.ReLU(), \n",
    "                nn.Linear(config.hidden_size, 1)\n",
    "            )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.cls(input)\n",
    "\n",
    "\n",
    "class HBERT_Pretrain(nn.Module):\n",
    "    def __init__(self, config, tokenizer):\n",
    "        super(HBERT_Pretrain, self).__init__()\n",
    "\n",
    "        if config.diag_med_emb == \"simple\":\n",
    "            self.embeddings = HBERTEmbeddings(config)\n",
    "        elif config.diag_med_emb == \"tree\":\n",
    "            diag_tree_table, med_tree_table = tokenizer.diag_tree_table, tokenizer.med_tree_table\n",
    "            n_diag_tokens, n_med_tokens = len(tokenizer.diag_tree_voc.idx2word), len(tokenizer.med_tree_voc.idx2word)\n",
    "            diag_range, med_range = tokenizer.token_id_range(\"diag\"), tokenizer.token_id_range(\"med\")\n",
    "            self.embeddings = TreeEmbeddings(config, diag_tree_table, med_tree_table, \n",
    "                                            n_diag_tokens, n_med_tokens, diag_range, med_range)\n",
    "\n",
    "        # self.loss_fn = torch.nn.BCEWithLogitsLoss\n",
    "        self.loss_fn = F.binary_cross_entropy_with_logits\n",
    "        self.pos_weight = torch.tensor(config.pos_weight)\n",
    "        self.encoder = config.encoder\n",
    "\n",
    "        if config.encoder == \"hi\":\n",
    "            self.transformer = HiTransformer(config)\n",
    "        elif config.encoder == \"hi_edge\":\n",
    "            self.transformer = HiEdgeTransformer(config)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        self.mask_token_id = config.mask_token_id  # {token_type: masked_id}\n",
    "        predicted_token_type = config.predicted_token_type  # [\"diag\", \"med\", \"pro\", \"lab\"]\n",
    "        label_vocab_size = config.label_vocab_size  # {token_type: vocab_size}\n",
    "        for token_type in predicted_token_type:\n",
    "            self.add_module(f\"{token_type}_cls\", MaskedPredictionHead(config, label_vocab_size[token_type]))\n",
    "        if config.anomaly_rate > 0:\n",
    "            self.anomaly_loss_weight = config.anomaly_loss_weight\n",
    "            self.anomaly_detection_head = BinaryPredictionHead(config)\n",
    "\n",
    "    def forward(self, input_ids, token_types, edge_index, visit_positions, masked_labels, anomaly_labels):\n",
    "        device = input_ids.device\n",
    "        pad_mask = (input_ids > 0)\n",
    "        pair_pad_mask = pad_mask.unsqueeze(1).repeat(1, input_ids.size(1), 1)\n",
    "\n",
    "        # embedding the indexed sequence to sequence of vectors\n",
    "        x = self.embeddings(input_ids, token_types)\n",
    "\n",
    "        if self.encoder == \"hi\":\n",
    "            x = self.transformer(x, edge_index, ~pair_pad_mask, visit_positions)\n",
    "        elif self.encoder == \"hi_edge\":\n",
    "            x = self.transformer(x, token_types, edge_index, ~pair_pad_mask, visit_positions)\n",
    "\n",
    "        ave_loss, loss_dict = 0, {}\n",
    "        for i, (token_type, mask_id) in enumerate(self.mask_token_id.items()):\n",
    "            masked_token_emb = x[input_ids == mask_id]\n",
    "            prediction = self._modules[f\"{token_type}_cls\"](masked_token_emb)\n",
    "            loss = self.loss_fn(prediction, masked_labels[i].to(input_ids.device))\n",
    "            # loss = self.loss_fn(prediction.view(-1), masked_labels[i].view(-1).to(device), pos_weight=self.pos_weight.to(device))\n",
    "            ave_loss += loss\n",
    "            loss_dict[token_type] = loss.cpu().item()\n",
    "        \n",
    "        if anomaly_labels is not None:\n",
    "            anomaly_prediction = self.anomaly_detection_head(x)\n",
    "            # anomaly_loss = self.loss_fn(reduction='none')(anomaly_prediction.view(-1), anomaly_labels.view(-1))\n",
    "            anomaly_loss = self.loss_fn(anomaly_prediction.view(-1), anomaly_labels.view(-1), reduction='none')\n",
    "            anomaly_loss = (anomaly_loss * pad_mask.view(-1)).sum() / pad_mask.sum()\n",
    "            ave_loss += self.anomaly_loss_weight * anomaly_loss\n",
    "            loss_dict[\"anomaly\"] = anomaly_loss.cpu().item()\n",
    "        else:\n",
    "            loss_dict[\"anomaly\"] = 0.\n",
    "\n",
    "        return ave_loss / len(loss_dict), loss_dict\n",
    "\n",
    "\n",
    "class HBERT_Finetune(nn.Module):\n",
    "    def __init__(self, config, tokenizer):\n",
    "        super(HBERT_Finetune, self).__init__()\n",
    "\n",
    "        if config.diag_med_emb == \"simple\":\n",
    "            self.embeddings = HBERTEmbeddings(config)\n",
    "        elif config.diag_med_emb == \"tree\":\n",
    "            diag_tree_table, med_tree_table = tokenizer.diag_tree_table, tokenizer.med_tree_table\n",
    "            n_diag_tokens, n_med_tokens = len(tokenizer.diag_tree_voc.idx2word), len(tokenizer.med_tree_voc.idx2word)\n",
    "            diag_range, med_range = tokenizer.token_id_range(\"diag\"), tokenizer.token_id_range(\"med\")\n",
    "            self.embeddings = TreeEmbeddings(config, diag_tree_table, med_tree_table, \n",
    "                                            n_diag_tokens, n_med_tokens, diag_range, med_range)\n",
    "\n",
    "        self.loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "        self.encoder = config.encoder\n",
    "        self.diag_mask_id = 3  # the idx of [MASK0] token\n",
    "        self.task = config.task\n",
    "\n",
    "        if config.encoder == \"hi\":\n",
    "            self.transformer = HiTransformer(config)\n",
    "        elif config.encoder == \"hi_edge\":\n",
    "            self.transformer = HiEdgeTransformer(config)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        if config.task in [\"death\", \"stay\", \"readmission\"]:\n",
    "            self.downstream_cls = BinaryPredictionHead(config)\n",
    "        else:\n",
    "            self.downstream_cls = MaskedPredictionHead(config, config.label_vocab_size)\n",
    "\n",
    "    def load_weight(self, checkpoint_dict):\n",
    "        param_dict = dict(self.named_parameters())\n",
    "        for key in checkpoint_dict.keys():\n",
    "            if key in param_dict:\n",
    "                param_dict[key].data.copy_(checkpoint_dict[key])\n",
    "    \n",
    "    def forward(self, input_ids, token_types, edge_index, visit_positions, labeled_ids):\n",
    "        pad_mask = (input_ids > 0).unsqueeze(1).repeat(1, input_ids.size(1), 1)\n",
    "\n",
    "        # embedding the indexed sequence to sequence of vectors\n",
    "        x = self.embeddings(input_ids, token_types)\n",
    "\n",
    "        if self.encoder == \"hi\":\n",
    "            x = self.transformer(x, edge_index, ~pad_mask, visit_positions)\n",
    "        elif self.encoder == \"hi_edge\":\n",
    "            x = self.transformer(x, token_types, edge_index, ~pad_mask, visit_positions)\n",
    "\n",
    "        if self.task in [\"death\", \"stay\", \"readmission\"]:\n",
    "            prediction = self.downstream_cls(x[labeled_ids][:, 0])\n",
    "        else:\n",
    "            labeled_ids, labeled_x = input_ids[labeled_ids], x[labeled_ids]\n",
    "            masked_pos_embs = labeled_x[labeled_ids == self.diag_mask_id]\n",
    "            prediction = self.downstream_cls(masked_pos_embs)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "418c0904-b569-4c7f-b066-2a359995627e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(args, all_data_path, pretrain_data_path):\n",
    "    ehr_data = pickle.load(open(all_data_path, 'rb'))\n",
    "    diag_sentences = ehr_data[\"icd_code\"].values.tolist()\n",
    "    med_sentences = ehr_data[\"ndc\"].values.tolist()\n",
    "    lab_sentences = ehr_data[\"lab_test\"].values.tolist()\n",
    "    if args.dataset == \"mimic\":\n",
    "        pro_sentences = ehr_data[\"pro_code\"].values.tolist()\n",
    "        gender_set = [[\"M\"], [\"F\"]]\n",
    "        age_gender_set = [[str(c) + \"_\" + gender] for c in set(ehr_data[\"age\"].values.tolist()) for gender in [\"M\", \"F\"]]\n",
    "    else:\n",
    "        pro_sentences = None\n",
    "        gender_set = [[\"Female\"], [\"Male\"], [\"Unknown\"], [\"Other\"]]\n",
    "        age_gender_set = [[str(c) + \"_\" + gender] for c in set(ehr_data[\"age\"].values.tolist()) for gender in [\"Female\", \"Male\", \"Unknown\", \"Other\"]]\n",
    "    age_set = [[c] for c in set(ehr_data[\"age\"].values.tolist())]\n",
    "    \n",
    "    ehr_pretrain_data = pickle.load(open(pretrain_data_path, 'rb'))\n",
    "    tokenizer = EHRTokenizer(diag_sentences, med_sentences, lab_sentences, pro_sentences, gender_set, age_set, age_gender_set, special_tokens=args.special_tokens)\n",
    "    if args.dataset == \"mimic\":\n",
    "        tokenizer.build_tree()\n",
    "    dataset = HBERTPretrainEHRDataset(ehr_pretrain_data, tokenizer, token_type=args.predicted_token_type, mask_rate=args.mask_rate)\n",
    "    dataloader = DataLoader(dataset, batch_size=args.batch_size, collate_fn=batcher(tokenizer, n_token_type=len(args.predicted_token_type)), shuffle=True)\n",
    "    return tokenizer, dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5178401e-c26b-44b0-9441-8ae4b6ec8010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed: int = 42):\n",
    "    import os, random, numpy as np\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    try:\n",
    "        import torch\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    except ImportError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0dbac18a-cdbc-492a-98d3-a9248811012c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seed': 0, 'dataset': 'mimic', 'device': 0, 'batch_size': 32, 'lr': 2e-05, 'epochs': 50, 'use_wandb': False, 'encoder': 'hi_edge', 'mask_rate': 0.7, 'anomaly_rate': 0.05, 'anomaly_loss_weight': 1, 'pos_weight': 1, 'num_hidden_layers': 5, 'num_attention_heads': 6, 'attention_probs_dropout_prob': 0.2, 'hidden_dropout_prob': 0.2, 'edge_hidden_size': 32, 'hidden_size': 288, 'intermediate_size': 288, 'gnn_n_heads': 1, 'gnn_temp': 1, 'gat': 'dotattn', 'diag_med_emb': 'tree'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import wandb\n",
    "import random\n",
    "import pickle\n",
    "import logging\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm  # <--- better for Jupyter\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "LOG_FORMAT = \"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "logging.basicConfig(level=logging.INFO, format=LOG_FORMAT)\n",
    "\n",
    "from argparse import Namespace\n",
    "\n",
    "args = Namespace(\n",
    "    seed=0,\n",
    "    dataset=\"mimic\",\n",
    "    device=0,\n",
    "    batch_size=32,\n",
    "    lr=2e-5,\n",
    "    epochs=50,\n",
    "    use_wandb=False,\n",
    "    encoder=\"hi_edge\",\n",
    "    mask_rate=0.7,\n",
    "    anomaly_rate=0.05,\n",
    "    anomaly_loss_weight=1,\n",
    "    pos_weight=1,\n",
    "    num_hidden_layers=5,\n",
    "    num_attention_heads=6,\n",
    "    attention_probs_dropout_prob=0.2,\n",
    "    hidden_dropout_prob=0.2,\n",
    "    edge_hidden_size=32,\n",
    "    hidden_size=288,\n",
    "    intermediate_size=288,\n",
    "    gnn_n_heads=1,\n",
    "    gnn_temp=1,\n",
    "    gat=\"dotattn\",\n",
    "    diag_med_emb=\"tree\"\n",
    ")\n",
    "\n",
    "print(vars(args))  # See all your parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b1e2f4a-44a2-487b-815d-31b408459e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'seed': 0,\n",
       " 'dataset': 'mimic',\n",
       " 'device': 0,\n",
       " 'batch_size': 32,\n",
       " 'lr': 2e-05,\n",
       " 'epochs': 50,\n",
       " 'use_wandb': False,\n",
       " 'encoder': 'hi_edge',\n",
       " 'mask_rate': 0.7,\n",
       " 'anomaly_rate': 0.05,\n",
       " 'anomaly_loss_weight': 1,\n",
       " 'pos_weight': 1,\n",
       " 'num_hidden_layers': 5,\n",
       " 'num_attention_heads': 6,\n",
       " 'attention_probs_dropout_prob': 0.2,\n",
       " 'hidden_dropout_prob': 0.2,\n",
       " 'edge_hidden_size': 32,\n",
       " 'hidden_size': 288,\n",
       " 'intermediate_size': 288,\n",
       " 'gnn_n_heads': 1,\n",
       " 'gnn_temp': 1,\n",
       " 'gat': 'dotattn',\n",
       " 'diag_med_emb': 'tree'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9bfd7cfa-34fa-4523-95be-1088159c7ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dataset == \"mimic\":\n",
    "    args.max_visit_size = 15\n",
    "    args.predicted_token_type = [\"diag\", \"med\", \"pro\", \"lab\"]\n",
    "    args.mask_token_id = {\"diag\":3, \"med\":4, \"pro\":5, \"lab\":6}\n",
    "    args.special_tokens = (\"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK0]\", \"[MASK1]\", \"[MASK2]\", \"[MASK3]\")\n",
    "    data_path = \"/data/horse/ws/arsi805e-finetune/Thesis/dataset/mimic.pkl\"\n",
    "    pretrain_data_path = \"/data/horse/ws/arsi805e-finetune/Thesis/dataset/mimic_pretrain.pkl\"\n",
    "elif args.dataset == \"eicu\":\n",
    "    args.max_visit_size = 24\n",
    "    args.predicted_token_type = [\"diag\", \"med\", \"lab\"]\n",
    "    args.mask_token_id = {\"diag\":3, \"med\":4, \"lab\":5}\n",
    "    args.special_tokens = (\"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK0]\", \"[MASK1]\", \"[MASK2]\")\n",
    "    data_path = \"/home/username/ehr_bert/dataset/eicu.pkl\"\n",
    "    pretrain_data_path = \"/home/username/ehr_bert/dataset/eicu_pretrain.pkl\"\n",
    "else:\n",
    "    raise ValueError(\"Unknown dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "277c759b-c6db-479d-b577-d64d3b00b152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(args, all_data_path, pretrain_data_path):\n",
    "    ehr_data = pickle.load(open(all_data_path, 'rb'))\n",
    "    diag_sentences = ehr_data[\"icd_code\"].values.tolist()\n",
    "    med_sentences = ehr_data[\"ndc\"].values.tolist()\n",
    "    lab_sentences = ehr_data[\"lab_test\"].values.tolist()\n",
    "    if args.dataset == \"mimic\":\n",
    "        pro_sentences = ehr_data[\"pro_code\"].values.tolist()\n",
    "        gender_set = [[\"M\"], [\"F\"]]\n",
    "        age_gender_set = [[str(c) + \"_\" + gender] for c in set(ehr_data[\"age\"].values.tolist()) for gender in [\"M\", \"F\"]]\n",
    "    else:\n",
    "        pro_sentences = None\n",
    "        gender_set = [[\"Female\"], [\"Male\"], [\"Unknown\"], [\"Other\"]]\n",
    "        age_gender_set = [[str(c) + \"_\" + gender] for c in set(ehr_data[\"age\"].values.tolist()) for gender in [\"Female\", \"Male\", \"Unknown\", \"Other\"]]\n",
    "    age_set = [[c] for c in set(ehr_data[\"age\"].values.tolist())]\n",
    "    \n",
    "    ehr_pretrain_data = pickle.load(open(pretrain_data_path, 'rb'))\n",
    "    tokenizer = EHRTokenizer(diag_sentences, med_sentences, lab_sentences, pro_sentences, gender_set, age_set, age_gender_set, special_tokens=args.special_tokens)\n",
    "    print(f\"Tokenzier: \\n {tokenizer}\")\n",
    "    if args.dataset == \"mimic\":\n",
    "        tokenizer.build_tree()\n",
    "    dataset = HBERTPretrainEHRDataset(ehr_pretrain_data, tokenizer, token_type=args.predicted_token_type, mask_rate=args.mask_rate)\n",
    "    dataloader = DataLoader(dataset, batch_size=args.batch_size, collate_fn=batcher(tokenizer, n_token_type=len(args.predicted_token_type)), shuffle=True)\n",
    "    return tokenizer, dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "48b782cc-f1c0-48ee-b60c-d77021a19576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(seed=0, dataset='mimic', device=0, batch_size=32, lr=2e-05, epochs=50, use_wandb=False, encoder='hi_edge', mask_rate=0.7, anomaly_rate=0.05, anomaly_loss_weight=1, pos_weight=1, num_hidden_layers=5, num_attention_heads=6, attention_probs_dropout_prob=0.2, hidden_dropout_prob=0.2, edge_hidden_size=32, hidden_size=288, intermediate_size=288, gnn_n_heads=1, gnn_temp=1, gat='dotattn', diag_med_emb='tree', max_visit_size=15, predicted_token_type=['diag', 'med', 'pro', 'lab'], mask_token_id={'diag': 3, 'med': 4, 'pro': 5, 'lab': 6}, special_tokens=('[PAD]', '[CLS]', '[SEP]', '[MASK0]', '[MASK1]', '[MASK2]', '[MASK3]'))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1542d41f-48d7-43e2-9fb7-9e855122f137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/horse/ws/arsi805e-finetune/Thesis/dataset/mimic.pkl'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6951c22f-ee0c-40fc-b0db-240e580e4322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenzier: \n",
      " <__main__.EHRTokenizer object at 0x1514053127b0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 17:10:50,347 - INFO - Loaded data from /data/horse/ws/arsi805e-finetune/Thesis/dataset/mimic_pretrain.pkl\n"
     ]
    }
   ],
   "source": [
    "tokenizer, dataloader = read_data(args, data_path, pretrain_data_path)\n",
    "logging.info(f\"Loaded data from {pretrain_data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "173634b6-6cf6-4c12-ae4f-9722f4af416d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 17:10:50,605 - INFO - Initialized model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: \n",
      " HBERT_Pretrain(\n",
      "  (embeddings): TreeEmbeddings(\n",
      "    (diag_tokens): Embedding(2651, 96)\n",
      "    (med_tokens): Embedding(226, 96)\n",
      "    (word_embeddings): Embedding(4398, 288, padding_idx=0)\n",
      "    (emb_dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (transformer): HiEdgeTransformer(\n",
      "    (edge_module): EdgeModule(\n",
      "      (output): Linear(in_features=576, out_features=32, bias=True)\n",
      "    )\n",
      "    (transformer_blocks): ModuleList(\n",
      "      (0-4): 5 x EdgeTransformerBlock(\n",
      "        (self_attn): MultiHeadEdgeAttention(\n",
      "          (W_Q): Linear(in_features=288, out_features=288, bias=True)\n",
      "          (W_K): Linear(in_features=288, out_features=288, bias=True)\n",
      "          (W_V): Linear(in_features=288, out_features=288, bias=True)\n",
      "          (W_output): Linear(in_features=576, out_features=288, bias=True)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "          (W_K_edge): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_edge): Linear(in_features=32, out_features=1, bias=True)\n",
      "          (W_edge_output): Linear(in_features=192, out_features=288, bias=True)\n",
      "        )\n",
      "        (pos_ffn): PoswiseFeedForwardNet(\n",
      "          (fc1): Linear(in_features=288, out_features=288, bias=True)\n",
      "          (fc2): Linear(in_features=288, out_features=288, bias=True)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (norm_attn): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ffn): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_edge): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (cross_attentions): ModuleList(\n",
      "      (0-4): 5 x DotAttnConv(\n",
      "        (pos_encoding): Embedding(15, 288)\n",
      "        (W_q): Linear(in_features=288, out_features=288, bias=False)\n",
      "        (W_k): Linear(in_features=288, out_features=288, bias=False)\n",
      "        (W_v): Linear(in_features=288, out_features=288, bias=False)\n",
      "        (W_out): Linear(in_features=288, out_features=288, bias=False)\n",
      "        (ln): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (diag_cls): MaskedPredictionHead(\n",
      "    (cls): Sequential(\n",
      "      (0): Linear(in_features=288, out_features=288, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=288, out_features=2001, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (med_cls): MaskedPredictionHead(\n",
      "    (cls): Sequential(\n",
      "      (0): Linear(in_features=288, out_features=288, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=288, out_features=140, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (pro_cls): MaskedPredictionHead(\n",
      "    (cls): Sequential(\n",
      "      (0): Linear(in_features=288, out_features=288, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=288, out_features=801, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (lab_cls): MaskedPredictionHead(\n",
      "    (cls): Sequential(\n",
      "      (0): Linear(in_features=288, out_features=288, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=288, out_features=1387, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (anomaly_detection_head): BinaryPredictionHead(\n",
      "    (cls): Sequential(\n",
      "      (0): Linear(in_features=288, out_features=288, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=288, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "set_random_seed(args.seed)\n",
    "\n",
    "args.vocab_size = 7 + len(tokenizer.diag_voc.idx2word) + \\\n",
    "                  len(tokenizer.pro_voc.idx2word) + \\\n",
    "                  len(tokenizer.med_voc.idx2word) + \\\n",
    "                  len(tokenizer.lab_voc.idx2word) + \\\n",
    "                  len(tokenizer.age_voc.idx2word) + \\\n",
    "                  len(tokenizer.gender_voc.idx2word) + \\\n",
    "                  len(tokenizer.age_gender_voc.idx2word)\n",
    "\n",
    "args.label_vocab_size = {\"diag\":len(tokenizer.diag_voc.idx2word), \n",
    "                        \"pro\":len(tokenizer.pro_voc.idx2word), \n",
    "                        \"med\":len(tokenizer.med_voc.idx2word), \n",
    "                        \"lab\":len(tokenizer.lab_voc.idx2word)}  # {token_type: vocab_size}\n",
    "\n",
    "loss_entity = [\"diag\", \"med\", \"pro\", \"lab\", \"anomaly\"]\n",
    "\n",
    "device = torch.device(f\"cuda:{args.device}\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "model = HBERT_Pretrain(args, tokenizer).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr)\n",
    "logging.info(f\"Initialized model\")\n",
    "print(f\"MODEL: \\n {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "02d39af5-e872-4390-b92d-e5a9b6101c56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "273a223fd21e4bb68b383392c6255a71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 17:11:26,763 - INFO - Saved model to /data/horse/ws/arsi805e-finetune/Thesis/ehr_bert/saved_model//pretrained_1.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:001, average loss:0.1519\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b34555ecaef458281dba4685c05f36d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:002, average loss:0.0821\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb3852911ae44319b992e4c17d73097e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:003, average loss:0.0782\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bd5b09d08bf4f299fb52128c88e10c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:004, average loss:0.0757\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8becc9d4c1cb4de2b07078faa83db53c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 17:13:30,735 - INFO - Saved model to /data/horse/ws/arsi805e-finetune/Thesis/ehr_bert/saved_model//pretrained_5.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:005, average loss:0.0733\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a59e396cad684b71bb84082cfca8d276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:006, average loss:0.0714\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de5cb1a8e0b24751aefaf25106f1a5fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:007, average loss:0.0701\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44ee3d17147f48fa9106ad5a844dbf6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:008, average loss:0.0693\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d4ec123ca174122bbaf20bf89af9606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:009, average loss:0.0682\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ec07b08e9fe4bf29dc0a04a6d2f1787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 17:16:06,762 - INFO - Saved model to /data/horse/ws/arsi805e-finetune/Thesis/ehr_bert/saved_model//pretrained_10.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:010, average loss:0.0675\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a8aa4519bf74d03aa427ebc9382cec4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:011, average loss:0.0669\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cab5c82ae014487944930f87c47ecc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:012, average loss:0.0663\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c79f22095d443deb7e137fea4eadab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:013, average loss:0.0658\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37957b311af5447886fcc57d6abd5275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:014, average loss:0.0650\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ea1cd02339e4f219c842bf68ec6d3cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 17:18:42,832 - INFO - Saved model to /data/horse/ws/arsi805e-finetune/Thesis/ehr_bert/saved_model//pretrained_15.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:015, average loss:0.0648\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "040f6fcf84b247daafaa7ca3a960a51c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:016, average loss:0.0642\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9766e24f3a0b4a369f128f0f16251ec4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:017, average loss:0.0639\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5db06a238c014287a05e7f7556fe4f67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:018, average loss:0.0637\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce8e022987b412590e2826356220046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:019, average loss:0.0633\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b102446143c6407ca84ad45a8b20207f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 17:21:18,283 - INFO - Saved model to /data/horse/ws/arsi805e-finetune/Thesis/ehr_bert/saved_model//pretrained_20.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:020, average loss:0.0628\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18263cdaff6042eeb023aa8f40d7a0a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:021, average loss:0.0625\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5c6de3c6e844661a0af02b60842d585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:022, average loss:0.0623\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1894e69c7a5e47908644ca0e55368d1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:023, average loss:0.0617\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e107c71d0c3b43388bb1e2bb56048db9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:024, average loss:0.0616\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8909f6691b974335a24b344ba951757b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 17:23:54,231 - INFO - Saved model to /data/horse/ws/arsi805e-finetune/Thesis/ehr_bert/saved_model//pretrained_25.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:025, average loss:0.0614\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "869227387d624b1ebad572f8724f2d92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:026, average loss:0.0609\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20697d9c4cd44cf9adc173826208ea7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:027, average loss:0.0606\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0504d75b75be4996ab5140a3af709b35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:028, average loss:0.0605\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "122abb631f474a89bb3f255c6f7b4c0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:029, average loss:0.0602\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b64cda43147946378cb6b854df62640e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 17:26:30,660 - INFO - Saved model to /data/horse/ws/arsi805e-finetune/Thesis/ehr_bert/saved_model//pretrained_30.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:030, average loss:0.0601\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "738a33ef0712498c8b0a2081feee02b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:031, average loss:0.0597\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "134b53d360c4408482ac8af4b7567654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:032, average loss:0.0595\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22bfce17ba214682818794beb122a43c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:033, average loss:0.0593\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca1bdbd92a7841e48b9ea919e1d0ef06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:034, average loss:0.0590\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b5d703b8fc4436c98569d26c7847803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 17:29:06,961 - INFO - Saved model to /data/horse/ws/arsi805e-finetune/Thesis/ehr_bert/saved_model//pretrained_35.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:035, average loss:0.0588\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffbddc302d934639987707f3b0effad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:036, average loss:0.0586\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c980689763c49a19ca0fff32f4570b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:037, average loss:0.0584\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f4fa9e4831b405bb15a771aae061463",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:038, average loss:0.0581\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d90ffc5cea543e2be59fb8a6aa0b5e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:039, average loss:0.0579\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abeb9086b2d24c3388d83387c9368387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 17:31:41,841 - INFO - Saved model to /data/horse/ws/arsi805e-finetune/Thesis/ehr_bert/saved_model//pretrained_40.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:040, average loss:0.0578\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a746eeb237614413bebbdb10c9679800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:041, average loss:0.0577\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce2c284a13244f36a659b5d688dba29c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:042, average loss:0.0575\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6332c998347b40b2b67349e60d2b4ae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:043, average loss:0.0575\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "593513d3fd3c4fc38812450bad3c2997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:044, average loss:0.0571\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "339284f9d01b49e680c9f518055727d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 17:34:16,874 - INFO - Saved model to /data/horse/ws/arsi805e-finetune/Thesis/ehr_bert/saved_model//pretrained_45.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:045, average loss:0.0571\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7c05f7f43ac4f718f1268c46d7ba74f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:046, average loss:0.0571\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6af165c1e8c14f3fa8c318ad3bd12289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:047, average loss:0.0568\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcdc8622a1b44ba0b2757a8ea1064ab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:048, average loss:0.0568\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbe3ab27ff0a4aa2a292ea228316ac4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:049, average loss:0.0565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59ddfa4994ab499abe2cb10a1b16ee03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 17:36:52,062 - INFO - Saved model to /data/horse/ws/arsi805e-finetune/Thesis/ehr_bert/saved_model//pretrained_50.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:050, average loss:0.0563\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "if args.use_wandb:\n",
    "    wandb.init(project=\"ehr_bert\", name=\"Pretrain-HBERT\")\n",
    "    wandb.config.update(vars(args))\n",
    "    wandb.watch(model, log='all')\n",
    "\n",
    "save_path = \"/data/horse/ws/arsi805e-finetune/Thesis/ehr_bert/saved_model/\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "for epoch in range(1, 1 + args.epochs):\n",
    "    train_iter = tqdm(dataloader, ncols=140)\n",
    "    model.train()\n",
    "    ave_loss, ave_loss_dict = 0., {token_type: 0. for token_type in loss_entity}\n",
    "\n",
    "    for step, batch in enumerate(train_iter):\n",
    "        batch = [x.to(device) if isinstance(x, torch.Tensor) else x for x in batch]\n",
    "        loss, loss_dict = model(*batch)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_iter.set_description(f\"Epoch:{epoch:03d}, Step:{step:03d}, loss:{loss.item():.4f}\")\n",
    "        ave_loss += loss.item()\n",
    "        ave_loss_dict = {token_type: ave_loss_dict[token_type] + loss_dict[token_type] for token_type in loss_entity}\n",
    "\n",
    "    ave_loss /= (step + 1)\n",
    "    ave_loss_dict = {token_type: ave_loss_dict[token_type] / (step + 1) for token_type in loss_entity}\n",
    "    print(f\"Epoch:{epoch:03d}, average loss:{ave_loss:.4f}\")\n",
    "\n",
    "    if args.use_wandb:\n",
    "        record_dict = {f\"loss\": ave_loss}\n",
    "        record_dict.update({f\"loss_{token_type}\": ave_loss_dict[token_type] for token_type in loss_entity})\n",
    "        wandb.log(record_dict)\n",
    "\n",
    "    if epoch % 5 == 0 or epoch == 1:\n",
    "        torch.save(model.cpu().state_dict(), f\"{save_path}/pretrained_{epoch}.pt\")\n",
    "        logging.info(f\"Saved model to {save_path}/pretrained_{epoch}.pt\")\n",
    "        model.to(device)\n",
    "\n",
    "if args.use_wandb:\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4898d402-79f3-4666-aba4-976be0cc8988",
   "metadata": {},
   "source": [
    "# FINETUNING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9dc164ae-8ce2-4952-8c14-0726d31e210c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namespace(seed=42,\n",
       "          dataset='mimic',\n",
       "          device=0,\n",
       "          task='next_diag_6m',\n",
       "          pretrain_epoch=50,\n",
       "          batch_size=8,\n",
       "          eval_batch_size=8,\n",
       "          encoder='hi_edge',\n",
       "          pretrain_mask_rate=0.5,\n",
       "          pretrain_anomaly_rate=0.05,\n",
       "          pretrain_anomaly_loss_weight=1,\n",
       "          pretrain_pos_weight=1,\n",
       "          lr=0.0001,\n",
       "          epochs=10,\n",
       "          use_wandb=False,\n",
       "          num_hidden_layers=5,\n",
       "          num_attention_heads=6,\n",
       "          attention_probs_dropout_prob=0.2,\n",
       "          hidden_dropout_prob=0.2,\n",
       "          edge_hidden_size=32,\n",
       "          hidden_size=288,\n",
       "          intermediate_size=288,\n",
       "          save_model=True,\n",
       "          gnn_n_heads=1,\n",
       "          gnn_temp=1.0,\n",
       "          gat='dotattn',\n",
       "          diag_med_emb='tree')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score, auc, precision_recall_curve\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import wandb  # optional, skip if not using\n",
    "\n",
    "LOG_FORMAT = \"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "logging.basicConfig(level=logging.INFO, format=LOG_FORMAT)\n",
    "\n",
    "# ------ Setup args as a Namespace or SimpleNamespace -------\n",
    "args = SimpleNamespace(\n",
    "    seed=42,\n",
    "    dataset=\"mimic\",\n",
    "    device=0,\n",
    "    task=\"next_diag_6m\",  # change as needed\n",
    "    pretrain_epoch=50,\n",
    "    batch_size=8,\n",
    "    eval_batch_size=8,\n",
    "    encoder=\"hi_edge\",\n",
    "    pretrain_mask_rate=0.5,\n",
    "    pretrain_anomaly_rate=0.05,\n",
    "    pretrain_anomaly_loss_weight=1,\n",
    "    pretrain_pos_weight=1,\n",
    "    lr=1e-4,\n",
    "    epochs=10,\n",
    "    use_wandb=False,  # change as needed\n",
    "    num_hidden_layers=5,\n",
    "    num_attention_heads=6,\n",
    "    attention_probs_dropout_prob=0.2,\n",
    "    hidden_dropout_prob=0.2,\n",
    "    edge_hidden_size=32,\n",
    "    hidden_size=288,\n",
    "    intermediate_size=288,\n",
    "    save_model=True,\n",
    "    gnn_n_heads=1,\n",
    "    gnn_temp=1.0,\n",
    "    gat=\"dotattn\",\n",
    "    diag_med_emb=\"tree\",\n",
    "    # Add others if needed below\n",
    ")\n",
    "\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2f38eb29-782e-4197-9fb8-15eb4fefc4b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'seed': 42,\n",
       " 'dataset': 'mimic',\n",
       " 'device': 0,\n",
       " 'task': 'next_diag_6m',\n",
       " 'pretrain_epoch': 50,\n",
       " 'batch_size': 8,\n",
       " 'eval_batch_size': 8,\n",
       " 'encoder': 'hi_edge',\n",
       " 'pretrain_mask_rate': 0.5,\n",
       " 'pretrain_anomaly_rate': 0.05,\n",
       " 'pretrain_anomaly_loss_weight': 1,\n",
       " 'pretrain_pos_weight': 1,\n",
       " 'lr': 0.0001,\n",
       " 'epochs': 10,\n",
       " 'use_wandb': False,\n",
       " 'num_hidden_layers': 5,\n",
       " 'num_attention_heads': 6,\n",
       " 'attention_probs_dropout_prob': 0.2,\n",
       " 'hidden_dropout_prob': 0.2,\n",
       " 'edge_hidden_size': 32,\n",
       " 'hidden_size': 288,\n",
       " 'intermediate_size': 288,\n",
       " 'save_model': True,\n",
       " 'gnn_n_heads': 1,\n",
       " 'gnn_temp': 1.0,\n",
       " 'gat': 'dotattn',\n",
       " 'diag_med_emb': 'tree'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50301705-0842-496a-b7ef-6ff1d0d7f4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PATH LOADED SUCCESSFULLY....]\n",
      "MIMIC file loaded from: /data/horse/ws/arsi805e-finetune/Thesis/dataset/mimic.pkl\n"
     ]
    }
   ],
   "source": [
    "# ----- Set file paths here (modify for your data location) -----\n",
    "root = \"/data/horse/ws/arsi805e-finetune/Thesis\"\n",
    "root_ehr_bert = root + '/ehr_bert'\n",
    "args.special_tokens = (\"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK0]\", \"[MASK1]\", \"[MASK2]\", \"[MASK3]\")\n",
    "\n",
    "if args.dataset == \"mimic\":\n",
    "    args.predicted_token_type = [\"diag\", \"med\", \"pro\", \"lab\"]\n",
    "    all_data_path = f\"{root}/dataset/mimic.pkl\"\n",
    "    if args.task == \"next_diag_6m\":\n",
    "        finetune_data_path = f\"{root}/dataset/mimic_nextdiag_6m.pkl\"\n",
    "    elif args.task == \"next_diag_12m\":\n",
    "        finetune_data_path = f\"{root}/dataset/mimic_nextdiag_12m.pkl\"\n",
    "    else:    \n",
    "        finetune_data_path = f\"{root}/dataset/mimic_downstream.pkl\"\n",
    "    args.max_visit_size = 15\n",
    "else:\n",
    "    args.predicted_token_type = [\"diag\", \"med\", \"lab\"]\n",
    "    all_data_path = f\"{root}/dataset/eicu.pkl\"\n",
    "    finetune_data_path = f\"{root}/dataset/eicu_downstream.pkl\"\n",
    "    args.max_visit_size = 24\n",
    "\n",
    "print(\"[PATH LOADED SUCCESSFULLY....]\")\n",
    "print(f\"MIMIC file loaded from: {all_data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "019ee505-1b02-414d-9a1f-e80b7a497432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HBERT-hi_edge-0.5-0.05-1-1-288-32-5-6-0.2-0.2-288-dotattn-1-1.0-tree'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_name = (\n",
    "    \"HBERT\" +\n",
    "    \"-\" + str(args.encoder) +\n",
    "    \"-\" + str(args.pretrain_mask_rate) +\n",
    "    \"-\" + str(args.pretrain_anomaly_rate) +\n",
    "    \"-\" + str(args.pretrain_anomaly_loss_weight) +\n",
    "    \"-\" + str(args.pretrain_pos_weight) +\n",
    "    \"-\" + str(args.hidden_size) +\n",
    "    \"-\" + str(args.edge_hidden_size) +\n",
    "    \"-\" + str(args.num_hidden_layers) +\n",
    "    \"-\" + str(args.num_attention_heads) +\n",
    "    \"-\" + str(args.attention_probs_dropout_prob) +\n",
    "    \"-\" + str(args.hidden_dropout_prob) +\n",
    "    \"-\" + str(args.intermediate_size) +\n",
    "    \"-\" + str(args.gat) +\n",
    "    \"-\" + str(args.gnn_n_heads) +\n",
    "    \"-\" + str(args.gnn_temp) +\n",
    "    \"-\" + str(args.diag_med_emb)\n",
    ")\n",
    "exp_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0898fc11-21d7-4adc-aefa-e9955384e57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ Data loading ------\n",
    "def read_data_FineTune(args, all_data_path, finetune_data_path):\n",
    "    ehr_data = pickle.load(open(all_data_path, 'rb'))\n",
    "    diag_sentences = ehr_data[\"icd_code\"].values.tolist()\n",
    "    med_sentences = ehr_data[\"ndc\"].values.tolist()\n",
    "    lab_sentences = ehr_data[\"lab_test\"].values.tolist()\n",
    "    if args.dataset == \"mimic\":\n",
    "        pro_sentences = ehr_data[\"pro_code\"].values.tolist()\n",
    "        gender_set = [[\"M\"], [\"F\"]]\n",
    "        age_gender_set = [[str(c) + \"_\" + gender] for c in set(ehr_data[\"age\"].values.tolist()) for gender in [\"M\", \"F\"]]\n",
    "    else:\n",
    "        pro_sentences = None\n",
    "        gender_set = [[\"Female\"], [\"Male\"], [\"Unknown\"], [\"Other\"]]\n",
    "        age_gender_set = [[str(c) + \"_\" + gender] for c in set(ehr_data[\"age\"].values.tolist()) for gender in [\"Female\", \"Male\", \"Unknown\", \"Other\"]]\n",
    "    age_set = [[c] for c in set(ehr_data[\"age\"].values.tolist())]    \n",
    "\n",
    "    tokenizer = EHRTokenizer(diag_sentences, med_sentences, lab_sentences, pro_sentences, gender_set, age_set, age_gender_set, special_tokens=args.special_tokens)\n",
    "    if args.dataset == \"mimic\":\n",
    "        tokenizer.build_tree()\n",
    "\n",
    "    train_data, val_data, test_data = pickle.load(open(finetune_data_path, 'rb'))\n",
    "    train_dataset = HBERTFinetuneEHRDataset(train_data, tokenizer, token_type=args.predicted_token_type, task=args.task)\n",
    "    val_dataset = HBERTFinetuneEHRDataset(val_data, tokenizer, token_type=args.predicted_token_type, task=args.task)\n",
    "    test_dataset = HBERTFinetuneEHRDataset(test_data, tokenizer, token_type=args.predicted_token_type, task=args.task)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, collate_fn=batcher(tokenizer, is_train=False), shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=args.eval_batch_size, collate_fn=batcher(tokenizer, is_train=False), shuffle=False)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=args.eval_batch_size, collate_fn=batcher(tokenizer, is_train=False), shuffle=False)\n",
    "\n",
    "    return tokenizer, train_dataloader, val_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e0daf4b0-5803-4dc4-ba6a-a572e8947285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Helper: evaluation -----\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, device, task_type=\"binary\"):\n",
    "    model.eval()\n",
    "    predicted_scores, gt_labels = [], []\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        batch = [x.to(device) if isinstance(x, torch.Tensor) else x for x in batch]\n",
    "        labels = batch[-1]\n",
    "        output_logits = model(*batch[:-1])\n",
    "        predicted_scores.append(output_logits)\n",
    "        gt_labels.append(labels)\n",
    "    \n",
    "    if task_type == \"binary\":\n",
    "        predicted_scores = torch.cat(predicted_scores, dim=0).view(-1)\n",
    "        gt_labels = torch.cat(gt_labels, dim=0).view(-1).cpu().numpy()\n",
    "        scores = predicted_scores.cpu().numpy()\n",
    "        predicted_labels = (predicted_scores > 0).float().cpu().numpy()\n",
    "\n",
    "        precision = (predicted_labels * gt_labels).sum() / (predicted_labels.sum() + 1e-8)\n",
    "        recall = (predicted_labels * gt_labels).sum() / (gt_labels.sum() + 1e-8)\n",
    "        f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "        roc_auc = roc_auc_score(gt_labels, scores)\n",
    "        precision_curve, recall_curve, _ = precision_recall_curve(gt_labels, scores)\n",
    "        pr_auc = auc(recall_curve, precision_curve)\n",
    "\n",
    "        return {\"precision\":precision, \"recall\":recall, \"f1\":f1, \"roc_auc\":roc_auc, \"pr_auc\":pr_auc}\n",
    "    else:\n",
    "        predicted_scores = torch.cat(predicted_scores, dim=0).cpu()  # [B, -1]\n",
    "        gt_labels = torch.cat(gt_labels, dim=0).cpu()\n",
    "\n",
    "        print(f\"Predicted Sores: {predicted_scores}\")\n",
    "        print(f\"Ground truth labels: {gt_labels}\")\n",
    "\n",
    "        print(\"----- Some Analysis -----\")\n",
    "        print(f\"Predicted score shape: {predicted_scores.shape}\")\n",
    "        print(f\"GT label shape: {gt_labels.shape}\")\n",
    "\n",
    "        ave_f1, ave_auc, ave_prauc, ave_recall, ave_precision = [], [], [], [], []\n",
    "        for i in range(predicted_scores.size(0)):\n",
    "            scores, labels = predicted_scores[i].squeeze().clone(), gt_labels[i].squeeze().clone()\n",
    "\n",
    "            predicted_labels = (scores > 0).float().cpu().numpy()\n",
    "            labels = labels.float().cpu().numpy()\n",
    "            precision = (predicted_labels * labels).sum() / (predicted_labels.sum() + 1e-8)\n",
    "            recall = (predicted_labels * labels).sum() / (labels.sum() + 1e-8)\n",
    "            ave_f1.append(2 * precision * recall / (precision + recall + 1e-8))\n",
    "            ave_auc.append(roc_auc_score(labels, scores))\n",
    "            precision_curve, recall_curve, _ = precision_recall_curve(labels, scores)\n",
    "            ave_prauc.append(auc(recall_curve, precision_curve))\n",
    "            ave_recall.append(recall)\n",
    "            ave_precision.append(precision)\n",
    "\n",
    "        ave_f1, ave_auc, ave_prauc, ave_recall, ave_precision = (\n",
    "            np.mean(ave_f1), np.mean(ave_auc), np.mean(ave_prauc),\n",
    "            np.mean(ave_recall), np.mean(ave_precision)\n",
    "        )\n",
    "        return {\"recall\":ave_recall, \"precision\":ave_precision, \"f1\":ave_f1, \"auc\":ave_auc, \"prauc\":ave_prauc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "59c6aa6a-da94-4d2f-bf77-3769b812b5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Training setup ----\n",
    "set_random_seed(args.seed)\n",
    "\n",
    "tokenizer, train_dataloader, val_dataloader, test_dataloader = read_data_FineTune(args, all_data_path, finetune_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b538920-2625-48b0-aef0-4342243df690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "406"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a628da6c-c7d4-486b-8374-f867762ddaeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained weights loaded from file: /data/horse/ws/arsi805e-finetune/Thesis/ehr_bert/saved_model/pretrained_50.pt\n"
     ]
    }
   ],
   "source": [
    "pretrained_weight_path = \"/data/horse/ws/arsi805e-finetune/Thesis/ehr_bert/saved_model/\" + f\"pretrained_{args.pretrain_epoch}.pt\"\n",
    "print(f\"Pretrained weights loaded from file: {pretrained_weight_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "34d4c56c-853b-4859-9499-0647e794bc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "Model initialized....\n",
      "\n",
      "Model: \n",
      " HBERT_Finetune(\n",
      "  (embeddings): TreeEmbeddings(\n",
      "    (diag_tokens): Embedding(2651, 96)\n",
      "    (med_tokens): Embedding(226, 96)\n",
      "    (word_embeddings): Embedding(4398, 288, padding_idx=0)\n",
      "    (emb_dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (loss_fn): BCEWithLogitsLoss()\n",
      "  (transformer): HiEdgeTransformer(\n",
      "    (edge_module): EdgeModule(\n",
      "      (output): Linear(in_features=576, out_features=32, bias=True)\n",
      "    )\n",
      "    (transformer_blocks): ModuleList(\n",
      "      (0-4): 5 x EdgeTransformerBlock(\n",
      "        (self_attn): MultiHeadEdgeAttention(\n",
      "          (W_Q): Linear(in_features=288, out_features=288, bias=True)\n",
      "          (W_K): Linear(in_features=288, out_features=288, bias=True)\n",
      "          (W_V): Linear(in_features=288, out_features=288, bias=True)\n",
      "          (W_output): Linear(in_features=576, out_features=288, bias=True)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "          (W_K_edge): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_edge): Linear(in_features=32, out_features=1, bias=True)\n",
      "          (W_edge_output): Linear(in_features=192, out_features=288, bias=True)\n",
      "        )\n",
      "        (pos_ffn): PoswiseFeedForwardNet(\n",
      "          (fc1): Linear(in_features=288, out_features=288, bias=True)\n",
      "          (fc2): Linear(in_features=288, out_features=288, bias=True)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (norm_attn): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ffn): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_edge): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (cross_attentions): ModuleList(\n",
      "      (0-4): 5 x DotAttnConv(\n",
      "        (pos_encoding): Embedding(15, 288)\n",
      "        (W_q): Linear(in_features=288, out_features=288, bias=False)\n",
      "        (W_k): Linear(in_features=288, out_features=288, bias=False)\n",
      "        (W_v): Linear(in_features=288, out_features=288, bias=False)\n",
      "        (W_out): Linear(in_features=288, out_features=288, bias=False)\n",
      "        (ln): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (downstream_cls): MaskedPredictionHead(\n",
      "    (cls): Sequential(\n",
      "      (0): Linear(in_features=288, out_features=288, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=288, out_features=2001, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "Loaded pretrained model from /data/horse/ws/arsi805e-finetune/Thesis/ehr_bert/saved_model/pretrained_50.pt\n",
      "\n",
      "Finetuned model saved to: /data/horse/ws/arsi805e-finetune/Thesis/ehr_bert/saved_model/Finetune-next_diag_6m-HBERT-hi_edge-0.5-0.05-1-1-288-32-5-6-0.2-0.2-288-dotattn-1-1.0-tree\n",
      "\n",
      "Task Type: l2r\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2260856ae62849998585d02934b74804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sores: tensor([[ -3.6145,  -2.1973,  -3.2359,  ...,  -9.0892,  -9.9497,  -9.9118],\n",
      "        [ -3.1866,  -1.7978,  -3.0658,  ...,  -8.7547, -10.0428, -10.2816],\n",
      "        [ -2.8221,  -1.6359,  -2.6502,  ...,  -8.1609,  -9.2824,  -9.3210],\n",
      "        ...,\n",
      "        [ -3.6656,  -2.1279,  -3.7517,  ...,  -9.4805, -10.8054, -11.3812],\n",
      "        [ -3.5529,  -2.2493,  -3.4050,  ...,  -9.0637,  -9.8782,  -9.9200],\n",
      "        [ -3.6572,  -2.2082,  -3.6477,  ...,  -9.7365, -10.6272, -11.1785]])\n",
      "Ground truth labels: tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "----- Some Analysis -----\n",
      "Predicted score shape: torch.Size([3252, 2001])\n",
      "GT label shape: torch.Size([3252, 2001])\n",
      "Predicted Sores: tensor([[ -2.7997,  -1.3305,  -3.1107,  ...,  -8.4252,  -9.9410, -10.5253],\n",
      "        [ -3.5956,  -2.1410,  -3.7670,  ...,  -9.8894, -11.1102, -11.7762],\n",
      "        [ -3.2721,  -1.8536,  -3.5120,  ...,  -8.9918, -10.6014, -11.2823],\n",
      "        ...,\n",
      "        [ -3.9665,  -2.3083,  -4.0960,  ..., -10.1348, -11.5455, -12.3587],\n",
      "        [ -2.8139,  -1.6439,  -2.6694,  ...,  -7.8560,  -8.9963,  -8.8494],\n",
      "        [ -3.0707,  -1.7435,  -2.7174,  ...,  -8.1955,  -9.4628,  -9.4155]])\n",
      "Ground truth labels: tensor([[1, 1, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "----- Some Analysis -----\n",
      "Predicted score shape: torch.Size([3247, 2001])\n",
      "GT label shape: torch.Size([3247, 2001])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 10:24:25,196 - INFO - save model to /data/horse/ws/arsi805e-finetune/Thesis/ehr_bert/saved_model/Finetune-next_diag_6m-HBERT-hi_edge-0.5-0.05-1-1-288-32-5-6-0.2-0.2-288-dotattn-1-1.0-tree/1.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, average loss:0.0677\n",
      "Val: {'recall': np.float32(0.008708145), 'precision': np.float32(0.0997335), 'f1': np.float32(0.015754258), 'auc': np.float64(0.8645676079355915), 'prauc': np.float64(0.1522231524831325)}\n",
      "Test: {'recall': np.float32(0.008512921), 'precision': np.float32(0.08987784), 'f1': np.float32(0.01518192), 'auc': np.float64(0.862878897148297), 'prauc': np.float64(0.15606537425298653)}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efe9c511fe2d4f8a8caca822d9991a32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sores: tensor([[ -4.5174,  -3.9595,  -4.6507,  ..., -10.4043, -10.0170, -10.1102],\n",
      "        [ -3.5042,  -3.0154,  -2.7237,  ...,  -9.4488, -10.4488, -10.4434],\n",
      "        [ -2.9625,  -2.7444,  -2.4664,  ...,  -8.9354,  -9.9238,  -9.5685],\n",
      "        ...,\n",
      "        [ -4.4686,  -3.7752,  -4.3978,  ..., -10.8625, -10.8472, -11.4905],\n",
      "        [ -4.6091,  -4.0935,  -4.9563,  ..., -10.5095,  -9.7289, -10.2643],\n",
      "        [ -3.8921,  -3.5555,  -3.9436,  ..., -10.6084, -10.4061, -11.1773]])\n",
      "Ground truth labels: tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "----- Some Analysis -----\n",
      "Predicted score shape: torch.Size([3252, 2001])\n",
      "GT label shape: torch.Size([3252, 2001])\n",
      "Predicted Sores: tensor([[ -1.8357,  -1.3216,  -1.5965,  ...,  -8.9128, -10.9110, -10.9077],\n",
      "        [ -3.5857,  -3.5154,  -3.6659,  ..., -10.6778, -11.4703, -11.7915],\n",
      "        [ -2.7858,  -2.1440,  -2.5880,  ...,  -9.5275, -11.4447, -11.8154],\n",
      "        ...,\n",
      "        [ -4.5300,  -3.6859,  -4.2294,  ..., -11.2158, -12.1917, -12.8494],\n",
      "        [ -3.5335,  -2.7552,  -3.1767,  ...,  -8.8163,  -9.6347,  -9.1755],\n",
      "        [ -4.1537,  -3.1220,  -3.2735,  ...,  -9.4238, -10.2781, -10.0292]])\n",
      "Ground truth labels: tensor([[1, 1, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "----- Some Analysis -----\n",
      "Predicted score shape: torch.Size([3247, 2001])\n",
      "GT label shape: torch.Size([3247, 2001])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 10:24:53,092 - INFO - save model to /data/horse/ws/arsi805e-finetune/Thesis/ehr_bert/saved_model/Finetune-next_diag_6m-HBERT-hi_edge-0.5-0.05-1-1-288-32-5-6-0.2-0.2-288-dotattn-1-1.0-tree/2.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02, average loss:0.0345\n",
      "Val: {'recall': np.float32(0.06911934), 'precision': np.float32(0.42247117), 'f1': np.float32(0.112225026), 'auc': np.float64(0.8726785764803058), 'prauc': np.float64(0.19364868743184)}\n",
      "Test: {'recall': np.float32(0.072617956), 'precision': np.float32(0.46204922), 'f1': np.float32(0.11807963), 'auc': np.float64(0.8712624830299209), 'prauc': np.float64(0.19678725640231612)}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "473c8ab0f83d4ad0958b869eb7a32af2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sores: tensor([[ -6.1494,  -5.0640,  -5.5158,  ..., -13.0290, -13.0021, -12.8992],\n",
      "        [ -3.7291,  -2.8153,  -2.5991,  ..., -10.0156, -11.0711, -11.0816],\n",
      "        [ -3.6770,  -3.0100,  -2.7166,  ...,  -9.9831, -11.1621, -10.9490],\n",
      "        ...,\n",
      "        [ -5.9657,  -4.8546,  -5.6999,  ..., -11.4388, -11.1506, -11.8699],\n",
      "        [ -6.4027,  -5.3688,  -6.0335,  ..., -12.7834, -12.3712, -12.7713],\n",
      "        [ -4.8488,  -4.2636,  -4.6142,  ..., -11.2460, -10.9108, -11.9671]])\n",
      "Ground truth labels: tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "----- Some Analysis -----\n",
      "Predicted score shape: torch.Size([3252, 2001])\n",
      "GT label shape: torch.Size([3252, 2001])\n",
      "Predicted Sores: tensor([[ -0.7637,   0.3837,   0.3398,  ...,  -9.5742, -11.6979, -12.1421],\n",
      "        [ -4.2102,  -4.0393,  -4.3584,  ..., -10.4789, -11.1474, -11.8453],\n",
      "        [ -2.6378,  -1.4195,  -2.2304,  ...,  -9.4597, -11.3320, -12.2184],\n",
      "        ...,\n",
      "        [ -5.7132,  -4.5147,  -5.5252,  ..., -11.3486, -12.0943, -12.7081],\n",
      "        [ -4.7501,  -3.1835,  -3.7541,  ..., -10.7950, -12.2907, -11.2814],\n",
      "        [ -5.0948,  -3.2736,  -3.5368,  ..., -11.2879, -12.7975, -12.1593]])\n",
      "Ground truth labels: tensor([[1, 1, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "----- Some Analysis -----\n",
      "Predicted score shape: torch.Size([3247, 2001])\n",
      "GT label shape: torch.Size([3247, 2001])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 10:25:20,806 - INFO - save model to /data/horse/ws/arsi805e-finetune/Thesis/ehr_bert/saved_model/Finetune-next_diag_6m-HBERT-hi_edge-0.5-0.05-1-1-288-32-5-6-0.2-0.2-288-dotattn-1-1.0-tree/3.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03, average loss:0.0333\n",
      "Val: {'recall': np.float32(0.08458793), 'precision': np.float32(0.45441583), 'f1': np.float32(0.13507324), 'auc': np.float64(0.8814322489961537), 'prauc': np.float64(0.2188281335101017)}\n",
      "Test: {'recall': np.float32(0.086001836), 'precision': np.float32(0.4751101), 'f1': np.float32(0.13793363), 'auc': np.float64(0.8792959206077511), 'prauc': np.float64(0.22236822287195504)}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f712955f19449fea964ae32c9777495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sores: tensor([[ -6.4630,  -5.3724,  -5.9918,  ..., -13.2629, -13.7330, -12.7194],\n",
      "        [ -3.6167,  -2.5673,  -3.3195,  ..., -10.7131, -11.9107, -11.8995],\n",
      "        [ -3.4000,  -2.6081,  -3.3774,  ..., -11.1285, -12.8326, -12.3716],\n",
      "        ...,\n",
      "        [ -7.3594,  -5.7484,  -7.1348,  ..., -11.8198, -12.0070, -11.5319],\n",
      "        [ -7.4424,  -5.9737,  -6.8388,  ..., -13.0569, -13.4256, -13.2712],\n",
      "        [ -6.1815,  -5.0549,  -6.4380,  ..., -11.1038, -11.3826, -11.5499]])\n",
      "Ground truth labels: tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "----- Some Analysis -----\n",
      "Predicted score shape: torch.Size([3252, 2001])\n",
      "GT label shape: torch.Size([3252, 2001])\n",
      "Predicted Sores: tensor([[  0.3253,   1.2449,  -0.4281,  ..., -10.7470, -13.8855, -13.1709],\n",
      "        [ -6.0822,  -5.4594,  -7.1142,  ..., -11.1099, -11.4804, -11.3377],\n",
      "        [ -3.4634,  -1.9280,  -4.0731,  ...,  -9.8759, -11.6320, -11.7401],\n",
      "        ...,\n",
      "        [ -7.1265,  -5.4385,  -7.6059,  ..., -12.4787, -12.8639, -12.3077],\n",
      "        [ -3.8489,  -2.3058,  -3.7437,  ..., -11.6566, -13.9959, -12.6663],\n",
      "        [ -4.5263,  -2.8909,  -3.9426,  ..., -12.3716, -14.1492, -12.9772]])\n",
      "Ground truth labels: tensor([[1, 1, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "----- Some Analysis -----\n",
      "Predicted score shape: torch.Size([3247, 2001])\n",
      "GT label shape: torch.Size([3247, 2001])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 10:25:48,418 - INFO - save model to /data/horse/ws/arsi805e-finetune/Thesis/ehr_bert/saved_model/Finetune-next_diag_6m-HBERT-hi_edge-0.5-0.05-1-1-288-32-5-6-0.2-0.2-288-dotattn-1-1.0-tree/4.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04, average loss:0.0323\n",
      "Val: {'recall': np.float32(0.1006805), 'precision': np.float32(0.5277901), 'f1': np.float32(0.16032055), 'auc': np.float64(0.8873185467592815), 'prauc': np.float64(0.239293042546235)}\n",
      "Test: {'recall': np.float32(0.10334113), 'precision': np.float32(0.55508846), 'f1': np.float32(0.16482471), 'auc': np.float64(0.8851736323923826), 'prauc': np.float64(0.24284987175139472)}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2e8b282d64d462ab4c697f7ea555948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sores: tensor([[ -6.8582,  -6.4077,  -5.7406,  ..., -14.5964, -13.6856, -12.8219],\n",
      "        [ -4.5635,  -3.7417,  -3.2570,  ..., -11.5918, -11.9174, -12.4433],\n",
      "        [ -4.7443,  -4.2258,  -3.9980,  ..., -12.4884, -13.3430, -13.5984],\n",
      "        ...,\n",
      "        [ -7.6120,  -5.8315,  -6.5352,  ..., -12.0507, -11.5663, -11.0707],\n",
      "        [ -8.0457,  -6.8380,  -6.5165,  ..., -14.3495, -13.6647, -13.9722],\n",
      "        [ -6.0326,  -5.0253,  -6.0140,  ..., -10.6257, -10.9013, -11.0606]])\n",
      "Ground truth labels: tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "----- Some Analysis -----\n",
      "Predicted score shape: torch.Size([3252, 2001])\n",
      "GT label shape: torch.Size([3252, 2001])\n",
      "Predicted Sores: tensor([[  0.3250,   0.7672,  -0.1590,  ..., -10.7320, -14.1912, -12.8502],\n",
      "        [ -6.6763,  -5.6684,  -7.0010,  ..., -11.5323, -11.5066, -11.5163],\n",
      "        [ -3.3484,  -1.6621,  -3.1489,  ..., -10.2010, -11.8143, -11.8948],\n",
      "        ...,\n",
      "        [ -7.5315,  -5.8098,  -7.4659,  ..., -12.4631, -12.3683, -11.4531],\n",
      "        [ -5.0593,  -3.5894,  -3.8498,  ..., -12.7455, -14.0337, -13.1807],\n",
      "        [ -4.8648,  -4.2506,  -3.9277,  ..., -13.8171, -14.3760, -13.9071]])\n",
      "Ground truth labels: tensor([[1, 1, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "----- Some Analysis -----\n",
      "Predicted score shape: torch.Size([3247, 2001])\n",
      "GT label shape: torch.Size([3247, 2001])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 10:26:16,044 - INFO - save model to /data/horse/ws/arsi805e-finetune/Thesis/ehr_bert/saved_model/Finetune-next_diag_6m-HBERT-hi_edge-0.5-0.05-1-1-288-32-5-6-0.2-0.2-288-dotattn-1-1.0-tree/5.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05, average loss:0.0314\n",
      "Val: {'recall': np.float32(0.12590444), 'precision': np.float32(0.5533016), 'f1': np.float32(0.19400993), 'auc': np.float64(0.8922930981161225), 'prauc': np.float64(0.24871936660572166)}\n",
      "Test: {'recall': np.float32(0.12944558), 'precision': np.float32(0.57222706), 'f1': np.float32(0.19943379), 'auc': np.float64(0.8906072863019037), 'prauc': np.float64(0.255244870431211)}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e56a68a4f4e943d68cf534c8b163bc4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sores: tensor([[ -5.5154,  -5.1422,  -5.0709,  ..., -14.0520, -13.4623, -12.9973],\n",
      "        [ -4.2153,  -2.8065,  -3.3860,  ..., -11.0823, -11.4728, -12.1030],\n",
      "        [ -4.8364,  -3.6759,  -4.5046,  ..., -12.7320, -13.7594, -14.6120],\n",
      "        ...,\n",
      "        [ -7.9207,  -5.2851,  -7.0926,  ..., -11.7644, -11.5949, -11.3437],\n",
      "        [ -7.8884,  -6.2291,  -6.4677,  ..., -14.2223, -13.8600, -15.0756],\n",
      "        [ -5.6288,  -4.4216,  -5.9800,  ...,  -9.8597, -10.7577, -11.5900]])\n",
      "Ground truth labels: tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "----- Some Analysis -----\n",
      "Predicted score shape: torch.Size([3252, 2001])\n",
      "GT label shape: torch.Size([3252, 2001])\n",
      "Predicted Sores: tensor([[  0.9446,   1.7681,   0.0835,  ..., -10.8267, -14.3889, -13.1325],\n",
      "        [ -7.2166,  -5.4313,  -7.3616,  ..., -12.4648, -11.4793, -12.0414],\n",
      "        [ -2.8784,  -0.3383,  -2.4271,  ..., -10.4317, -11.4743, -11.9125],\n",
      "        ...,\n",
      "        [ -7.7418,  -4.9260,  -8.0960,  ..., -12.5489, -12.4394, -11.8160],\n",
      "        [ -4.5289,  -2.5117,  -3.7494,  ..., -13.5614, -15.0853, -14.5159],\n",
      "        [ -3.8801,  -3.0838,  -3.5305,  ..., -13.8535, -14.1551, -14.2462]])\n",
      "Ground truth labels: tensor([[1, 1, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "----- Some Analysis -----\n",
      "Predicted score shape: torch.Size([3247, 2001])\n",
      "GT label shape: torch.Size([3247, 2001])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 10:26:43,599 - INFO - save model to /data/horse/ws/arsi805e-finetune/Thesis/ehr_bert/saved_model/Finetune-next_diag_6m-HBERT-hi_edge-0.5-0.05-1-1-288-32-5-6-0.2-0.2-288-dotattn-1-1.0-tree/6.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06, average loss:0.0307\n",
      "Val: {'recall': np.float32(0.13341683), 'precision': np.float32(0.5620838), 'f1': np.float32(0.20299445), 'auc': np.float64(0.8959654876688778), 'prauc': np.float64(0.25913739480396936)}\n",
      "Test: {'recall': np.float32(0.13657007), 'precision': np.float32(0.583517), 'f1': np.float32(0.20824853), 'auc': np.float64(0.8941724868521465), 'prauc': np.float64(0.2646814254769317)}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fec250361d94dc9bf1315f3a4129588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sores: tensor([[ -5.5977,  -5.0665,  -4.3719,  ..., -14.6600, -14.0313, -12.9975],\n",
      "        [ -4.6273,  -2.6414,  -3.6711,  ..., -11.7055, -12.0137, -12.7368],\n",
      "        [ -5.1199,  -3.9296,  -5.1013,  ..., -13.6919, -15.0952, -15.6021],\n",
      "        ...,\n",
      "        [ -9.2480,  -5.6272,  -7.6742,  ..., -12.1386, -11.9606, -11.3120],\n",
      "        [ -7.8225,  -5.5798,  -5.3377,  ..., -13.7431, -14.5117, -15.1996],\n",
      "        [ -5.2204,  -4.0100,  -5.5247,  ...,  -9.7020, -11.3757, -11.8162]])\n",
      "Ground truth labels: tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "----- Some Analysis -----\n",
      "Predicted score shape: torch.Size([3252, 2001])\n",
      "GT label shape: torch.Size([3252, 2001])\n",
      "Predicted Sores: tensor([[  0.1151,   0.8308,  -1.8219,  ..., -12.1971, -16.4017, -14.8391],\n",
      "        [ -8.7486,  -6.0036,  -8.2906,  ..., -13.5704, -12.4306, -12.2871],\n",
      "        [ -3.9123,  -1.1500,  -3.1761,  ..., -11.7742, -12.8734, -13.1131],\n",
      "        ...,\n",
      "        [ -8.7284,  -5.3127,  -8.2589,  ..., -13.3835, -13.0409, -12.0926],\n",
      "        [ -4.4554,  -2.2444,  -3.6455,  ..., -13.6966, -16.1316, -14.7603],\n",
      "        [ -3.6880,  -2.7895,  -3.6465,  ..., -14.5875, -15.3202, -15.1285]])\n",
      "Ground truth labels: tensor([[1, 1, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "----- Some Analysis -----\n",
      "Predicted score shape: torch.Size([3247, 2001])\n",
      "GT label shape: torch.Size([3247, 2001])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 10:27:11,283 - INFO - save model to /data/horse/ws/arsi805e-finetune/Thesis/ehr_bert/saved_model/Finetune-next_diag_6m-HBERT-hi_edge-0.5-0.05-1-1-288-32-5-6-0.2-0.2-288-dotattn-1-1.0-tree/7.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07, average loss:0.0301\n",
      "Val: {'recall': np.float32(0.143976), 'precision': np.float32(0.5609398), 'f1': np.float32(0.21533673), 'auc': np.float64(0.8988223543471447), 'prauc': np.float64(0.2691411556489929)}\n",
      "Test: {'recall': np.float32(0.14674005), 'precision': np.float32(0.58894706), 'f1': np.float32(0.22038631), 'auc': np.float64(0.8971939317801387), 'prauc': np.float64(0.27315512580137175)}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97fc802e264c49a6971e2b709c590ae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sores: tensor([[ -7.1426,  -5.7206,  -4.7859,  ..., -16.1973, -14.5680, -13.8038],\n",
      "        [ -4.9845,  -2.4967,  -3.2662,  ..., -10.5924, -10.5996, -11.3280],\n",
      "        [ -5.3998,  -4.0680,  -5.0065,  ..., -13.9324, -14.7439, -15.6854],\n",
      "        ...,\n",
      "        [ -9.9958,  -5.5687,  -8.5907,  ..., -12.7839, -13.1363, -11.9336],\n",
      "        [ -8.6132,  -5.8908,  -5.8094,  ..., -15.6885, -15.9015, -16.8325],\n",
      "        [ -6.5986,  -5.1613,  -6.5606,  ..., -10.7697, -11.7442, -12.4037]])\n",
      "Ground truth labels: tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "----- Some Analysis -----\n",
      "Predicted score shape: torch.Size([3252, 2001])\n",
      "GT label shape: torch.Size([3252, 2001])\n",
      "Predicted Sores: tensor([[  0.5597,   1.2163,  -0.7397,  ..., -11.3831, -14.9858, -13.4319],\n",
      "        [ -7.4666,  -4.5903,  -6.2562,  ..., -14.0231, -13.1399, -13.2966],\n",
      "        [ -3.7470,  -1.0098,  -2.9838,  ..., -11.8583, -13.2544, -13.3454],\n",
      "        ...,\n",
      "        [ -9.0739,  -5.2381,  -8.7184,  ..., -13.4144, -13.5186, -12.1231],\n",
      "        [ -4.9098,  -2.6698,  -3.6222,  ..., -13.9684, -14.8977, -14.0912],\n",
      "        [ -4.2285,  -3.1971,  -2.8980,  ..., -14.9868, -14.2864, -14.2875]])\n",
      "Ground truth labels: tensor([[1, 1, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "----- Some Analysis -----\n",
      "Predicted score shape: torch.Size([3247, 2001])\n",
      "GT label shape: torch.Size([3247, 2001])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 10:27:38,846 - INFO - save model to /data/horse/ws/arsi805e-finetune/Thesis/ehr_bert/saved_model/Finetune-next_diag_6m-HBERT-hi_edge-0.5-0.05-1-1-288-32-5-6-0.2-0.2-288-dotattn-1-1.0-tree/8.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08, average loss:0.0295\n",
      "Val: {'recall': np.float32(0.14811444), 'precision': np.float32(0.56238574), 'f1': np.float32(0.22082554), 'auc': np.float64(0.9008671707150797), 'prauc': np.float64(0.27319878001191544)}\n",
      "Test: {'recall': np.float32(0.15123168), 'precision': np.float32(0.5896405), 'f1': np.float32(0.22708263), 'auc': np.float64(0.8995144117942292), 'prauc': np.float64(0.2801306007080481)}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efe052b159e7485da7055bc44cd92cc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sores: tensor([[ -7.3699,  -6.0922,  -4.6081,  ..., -16.1994, -14.4121, -13.9667],\n",
      "        [ -5.5936,  -2.8883,  -3.0530,  ..., -11.4291, -10.9953, -11.9799],\n",
      "        [ -6.1493,  -4.6486,  -5.5485,  ..., -14.0388, -14.9354, -16.2184],\n",
      "        ...,\n",
      "        [-11.1010,  -6.6787,  -9.0507,  ..., -13.9647, -13.4843, -12.6927],\n",
      "        [ -9.5797,  -6.6748,  -6.2637,  ..., -15.4299, -15.5048, -17.1078],\n",
      "        [ -7.9207,  -6.3729,  -7.4805,  ..., -11.5065, -12.1186, -13.4430]])\n",
      "Ground truth labels: tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "----- Some Analysis -----\n",
      "Predicted score shape: torch.Size([3252, 2001])\n",
      "GT label shape: torch.Size([3252, 2001])\n",
      "Predicted Sores: tensor([[  0.5587,   1.6716,  -0.1856,  ..., -11.8280, -15.2589, -13.9680],\n",
      "        [ -7.9994,  -4.5343,  -6.3284,  ..., -15.3845, -13.8694, -14.5999],\n",
      "        [ -4.4984,  -0.7919,  -2.6824,  ..., -11.7303, -12.9309, -13.0454],\n",
      "        ...,\n",
      "        [-10.1598,  -6.0024,  -8.7577,  ..., -14.2465, -13.6634, -12.6257],\n",
      "        [ -5.6456,  -3.3467,  -3.7024,  ..., -14.8635, -15.9921, -15.0398],\n",
      "        [ -4.6314,  -3.5143,  -3.2235,  ..., -15.2095, -13.7920, -14.0186]])\n",
      "Ground truth labels: tensor([[1, 1, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "----- Some Analysis -----\n",
      "Predicted score shape: torch.Size([3247, 2001])\n",
      "GT label shape: torch.Size([3247, 2001])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 10:28:06,538 - INFO - save model to /data/horse/ws/arsi805e-finetune/Thesis/ehr_bert/saved_model/Finetune-next_diag_6m-HBERT-hi_edge-0.5-0.05-1-1-288-32-5-6-0.2-0.2-288-dotattn-1-1.0-tree/9.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09, average loss:0.0289\n",
      "Val: {'recall': np.float32(0.1573699), 'precision': np.float32(0.55600625), 'f1': np.float32(0.2306454), 'auc': np.float64(0.9030000054062169), 'prauc': np.float64(0.27744277263635647)}\n",
      "Test: {'recall': np.float32(0.16003361), 'precision': np.float32(0.58153456), 'f1': np.float32(0.23553608), 'auc': np.float64(0.900786546700267), 'prauc': np.float64(0.28311852886060784)}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50392758f2144ba599317c63301c7b16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sores: tensor([[ -7.1189,  -6.1312,  -5.0557,  ..., -16.2848, -13.3306, -13.2514],\n",
      "        [ -6.3592,  -3.5092,  -4.3789,  ..., -11.3406, -10.9547, -11.8675],\n",
      "        [ -7.1041,  -5.4248,  -6.3510,  ..., -14.7640, -15.3594, -16.7681],\n",
      "        ...,\n",
      "        [-11.2289,  -7.0510,  -9.9360,  ..., -13.5963, -13.3008, -12.8809],\n",
      "        [ -9.2740,  -6.3913,  -6.1975,  ..., -14.8917, -14.2556, -15.9404],\n",
      "        [ -8.3528,  -6.9007,  -8.1331,  ..., -11.4716, -11.7346, -13.1652]])\n",
      "Ground truth labels: tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "----- Some Analysis -----\n",
      "Predicted score shape: torch.Size([3252, 2001])\n",
      "GT label shape: torch.Size([3252, 2001])\n",
      "Predicted Sores: tensor([[ -0.4897,  -0.1799,  -2.1954,  ..., -12.3655, -15.6033, -13.3512],\n",
      "        [ -8.0335,  -5.7194,  -7.0833,  ..., -15.9088, -14.4768, -15.5356],\n",
      "        [ -4.8191,  -2.3805,  -3.8675,  ..., -13.1557, -14.7265, -14.5959],\n",
      "        ...,\n",
      "        [ -9.4082,  -6.3202,  -9.7193,  ..., -13.5699, -13.5375, -12.8721],\n",
      "        [ -5.0003,  -2.6819,  -3.7795,  ..., -14.2188, -14.7807, -13.9625],\n",
      "        [ -4.8141,  -3.6017,  -3.5665,  ..., -15.6359, -13.7276, -14.5387]])\n",
      "Ground truth labels: tensor([[1, 1, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "----- Some Analysis -----\n",
      "Predicted score shape: torch.Size([3247, 2001])\n",
      "GT label shape: torch.Size([3247, 2001])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 10:28:34,455 - INFO - save model to /data/horse/ws/arsi805e-finetune/Thesis/ehr_bert/saved_model/Finetune-next_diag_6m-HBERT-hi_edge-0.5-0.05-1-1-288-32-5-6-0.2-0.2-288-dotattn-1-1.0-tree/10.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, average loss:0.0284\n",
      "Val: {'recall': np.float32(0.15291238), 'precision': np.float32(0.57836694), 'f1': np.float32(0.22681643), 'auc': np.float64(0.9040776082079067), 'prauc': np.float64(0.28434501990170685)}\n",
      "Test: {'recall': np.float32(0.155953), 'precision': np.float32(0.60882443), 'f1': np.float32(0.23367436), 'auc': np.float64(0.9024004351060698), 'prauc': np.float64(0.2899076529444907)}\n",
      "-----------------\n",
      "best val metric: {'recall': np.float32(0.15291238), 'precision': np.float32(0.57836694), 'f1': np.float32(0.22681643), 'auc': np.float64(0.9040776082079067), 'prauc': np.float64(0.28434501990170685)}\n",
      "best test metric: {'recall': np.float32(0.155953), 'precision': np.float32(0.60882443), 'f1': np.float32(0.23367436), 'auc': np.float64(0.9024004351060698), 'prauc': np.float64(0.2899076529444907)}\n"
     ]
    }
   ],
   "source": [
    "args.vocab_size = len(args.special_tokens) + \\\n",
    "                  len(tokenizer.diag_voc.idx2word) + \\\n",
    "                  len(tokenizer.pro_voc.idx2word) + \\\n",
    "                  len(tokenizer.med_voc.idx2word) + \\\n",
    "                  len(tokenizer.lab_voc.idx2word) + \\\n",
    "                  len(tokenizer.age_voc.idx2word) + \\\n",
    "                  len(tokenizer.gender_voc.idx2word) + \\\n",
    "                  len(tokenizer.age_gender_voc.idx2word)\n",
    "\n",
    "args.label_vocab_size = len(tokenizer.diag_voc.idx2word)  # only for diagnosis\n",
    "\n",
    "device = torch.device(f\"cuda:{args.device}\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "model = HBERT_Finetune(args, tokenizer)\n",
    "print(\"Model initialized....\")\n",
    "print(f\"\\nModel: \\n {model}\")\n",
    "\n",
    "if args.pretrain_epoch > 0:\n",
    "    model.load_weight(torch.load(pretrained_weight_path, map_location=device))\n",
    "    print(f\"\\nLoaded pretrained model from {pretrained_weight_path}\")\n",
    "\n",
    "finetune_exp_name = f\"Finetune-{args.task}-{exp_name}\"\n",
    "save_path = f\"{root_ehr_bert}/saved_model/{finetune_exp_name}\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "print(f\"\\nFinetuned model saved to: {save_path}\")\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr)\n",
    "\n",
    "if args.task in [\"death\", \"stay\", \"readmission\"]:\n",
    "    eval_metric = \"f1\"\n",
    "    task_type = \"binary\"\n",
    "    loss_fn = F.binary_cross_entropy_with_logits\n",
    "else:\n",
    "    eval_metric = \"prauc\"\n",
    "    task_type = \"l2r\"\n",
    "    loss_fn = lambda x, y: F.binary_cross_entropy_with_logits(x, y)\n",
    "\n",
    "print(f\"\\nTask Type: {task_type}\")\n",
    "\n",
    "if args.use_wandb:\n",
    "    wandb.init(project=\"ehr_bert\", name=finetune_exp_name)\n",
    "    wandb.config.update(vars(args))\n",
    "    wandb.watch(model, log='all')\n",
    "\n",
    "best_score, best_val_metric, best_test_metric = 0., None, None\n",
    "\n",
    "for epoch in range(1, 1 + args.epochs):\n",
    "    train_iter = tqdm(train_dataloader, ncols=140)\n",
    "    model.train()\n",
    "    ave_loss = 0.\n",
    "\n",
    "    for step, batch in enumerate(train_iter):\n",
    "        batch = [x.to(device) if isinstance(x, torch.Tensor) else x for x in batch]\n",
    "        labels = batch[-1].float()\n",
    "        output_logits = model(*batch[:-1])\n",
    "\n",
    "        loss = loss_fn(output_logits.view(-1), labels.view(-1))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_iter.set_description(f\"Epoch:{epoch: 03d}, Step:{step: 03d}, loss:{loss.item():.4f}\")\n",
    "        ave_loss += loss.item()\n",
    "\n",
    "    ave_loss /= (step + 1)\n",
    "    val_metric = evaluate(model, val_dataloader, device, task_type=task_type)\n",
    "    test_metric = evaluate(model, test_dataloader, device, task_type=task_type)\n",
    "    print(f\"Epoch:{epoch: 03d}, average loss:{ave_loss:.4f}\")\n",
    "    print(\"Val:\", val_metric)\n",
    "    print(\"Test:\", test_metric)\n",
    "\n",
    "    if val_metric[eval_metric] > best_score:\n",
    "        best_score = val_metric[eval_metric]\n",
    "        best_val_metric = val_metric\n",
    "        best_test_metric = test_metric\n",
    "\n",
    "    if args.use_wandb:\n",
    "        record_dict = {f\"loss\": ave_loss}\n",
    "        record_dict.update({f\"val_{k}\":v for k, v in val_metric.items()})\n",
    "        record_dict.update({f\"test_{k}\":v for k, v in test_metric.items()})\n",
    "        wandb.log(record_dict)\n",
    "\n",
    "    if args.save_model:\n",
    "        torch.save(model.cpu().state_dict(), f\"{save_path}/{epoch}.pt\")\n",
    "        logging.info(f\"save model to {save_path}/{epoch}.pt\")\n",
    "        model.to(device)\n",
    "\n",
    "print(\"-----------------\")\n",
    "print(f\"best val metric: {best_val_metric}\")\n",
    "print(f\"best test metric: {best_test_metric}\")\n",
    "\n",
    "if args.use_wandb:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c730ab86-a5ce-43a4-aff9-117fb64c29c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33masn5898\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/horse/ws/arsi805e-finetune/Thesis/wandb/run-20250714_101554-y7w74amy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/asn5898/ehr_bert/runs/y7w74amy' target=\"_blank\">quiet-vortex-1</a></strong> to <a href='https://wandb.ai/asn5898/ehr_bert' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/asn5898/ehr_bert' target=\"_blank\">https://wandb.ai/asn5898/ehr_bert</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/asn5898/ehr_bert/runs/y7w74amy' target=\"_blank\">https://wandb.ai/asn5898/ehr_bert/runs/y7w74amy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/asn5898/ehr_bert/runs/y7w74amy?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x14bdb1e07620>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee38a64-f336-4501-a1b9-f4162fa6dabe",
   "metadata": {},
   "source": [
    "## TEXT MODALITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86991f1e-44a9-4bf9-abf7-a999b3034d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load EHR and Notes\n",
    "ehr_df = pd.read_pickle(\"mimic.pkl\")       # or read_csv/read_feather/...\n",
    "notes_df = pd.read_csv(\"cleaned_notes.csv\")  # or whichever format you use\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002128e3-855d-47f8-949e-2b206214b1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure hadm_id is string or int in both\n",
    "ehr_df['hadm_id'] = ehr_df['hadm_id'].astype(str)\n",
    "notes_df['hadm_id'] = notes_df['hadm_id'].astype(str)\n",
    "\n",
    "# Merge\n",
    "merged_df = ehr_df.merge(notes_df, on=\"hadm_id\", how=\"left\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetune",
   "language": "python",
   "name": "tune"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
