{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5646d3d0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# common_textgen.py\n",
    "import os, re, json, time, logging, math, pickle\n",
    "from typing import List, Dict, Tuple\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    ")\n",
    "from transformers.utils import logging as hf_logging\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "import faiss\n",
    "from rapidfuzz.fuzz import token_set_ratio\n",
    "from transformers import AutoTokenizer as HFTok, AutoModel as HFModel\n",
    "\n",
    "# ---------- logging & env ----------\n",
    "hf_logging.set_verbosity_error()\n",
    "log = logging.getLogger(\"textgen_common\")\n",
    "if not log.handlers:\n",
    "    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# ---------- rank helpers ----------\n",
    "def _env_int(k, default=0):\n",
    "    try: return int(os.environ.get(k, default))\n",
    "    except: return default\n",
    "\n",
    "def is_main_process(): return _env_int(\"RANK\", 0) == 0\n",
    "def world_size(): return _env_int(\"WORLD_SIZE\", 1)\n",
    "def local_rank(): return _env_int(\"LOCAL_RANK\", 0)\n",
    "\n",
    "# ------------------ utilities ------------------\n",
    "TEXT_COLS_SAFE = [\n",
    "    \"Chief Complaint\",\"History of Present Illness\",\"Past Medical History\",\n",
    "    \"Family History\",\"Physical Exam\",\"Pertinent Results\",\n",
    "    \"Brief Hospital Course\",\"Medications on Admission\"\n",
    "]\n",
    "\n",
    "def clean_text(x):\n",
    "    if x is None: return \"\"\n",
    "    try:\n",
    "        s = \" \".join(map(str, x.tolist())) if isinstance(x, (np.ndarray, pd.Series)) else str(x)\n",
    "    except Exception:\n",
    "        s = str(x)\n",
    "    s = s.replace(\"\\x00\",\" \").replace(\"\\r\",\" \")\n",
    "    s = re.sub(r\"_+\",\" \", s)\n",
    "    return re.sub(r\"\\s+\",\" \", s).strip()\n",
    "\n",
    "def to_list(x) -> List[str]:\n",
    "    if x is None: return []\n",
    "    if isinstance(x, (list, tuple, set, np.ndarray, pd.Series)):\n",
    "        it = x.tolist() if hasattr(x, \"tolist\") else x\n",
    "        out=[]\n",
    "        for v in it:\n",
    "            if v is None: continue\n",
    "            if isinstance(v, float) and np.isnan(v): continue\n",
    "            sv = str(v).strip()\n",
    "            if sv and sv.lower() not in (\"nan\",\"none\"): out.append(sv)\n",
    "        return out\n",
    "    s = str(x).strip()\n",
    "    if not s or s.lower() in (\"nan\",\"none\"): return []\n",
    "    if s.startswith(\"[\") and s.endswith(\"]\"):\n",
    "        try:\n",
    "            import ast\n",
    "            v = ast.literal_eval(s)\n",
    "            if isinstance(v, (list, tuple, set)): return [str(t).strip() for t in v if str(t).strip()]\n",
    "        except Exception: pass\n",
    "    return [t for t in re.split(r\"[,\\s]+\", s) if t]\n",
    "\n",
    "def norm_text(s: str) -> str:\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[^a-z0-9\\s\\-]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def format_icd9(code: str) -> str:\n",
    "    code = re.sub(r\"\\s+\",\"\", str(code)).upper().rstrip(\".\")\n",
    "    if not code: return \"\"\n",
    "    if code[0].isdigit():\n",
    "        if len(code)>3 and \".\" not in code: return code[:3]+\".\"+code[3:]\n",
    "        return code\n",
    "    if code[0] == \"V\":\n",
    "        if len(code)>3 and \".\" not in code: return code[:3]+\".\"+code[3:]\n",
    "        return code\n",
    "    if code[0] == \"E\":\n",
    "        if len(code)>4 and \".\" not in code: return code[:4]+\".\"+code[4:]\n",
    "        return code\n",
    "    return code\n",
    "\n",
    "def is_valid_icd9(code: str) -> bool:\n",
    "    if not code: return False\n",
    "    c = code.upper()\n",
    "    if c[0].isdigit(): return bool(re.match(r\"^\\d{3}(\\.\\d{1,2})?$\", c))\n",
    "    if c[0]==\"V\":      return bool(re.match(r\"^V\\d{2}(\\.\\d{1,2})?$\", c))\n",
    "    if c[0]==\"E\":      return bool(re.match(r\"^E\\d{3}(\\.\\d{1})?$\", c))\n",
    "    return False\n",
    "\n",
    "def get_icd9_parent(code: str) -> str:\n",
    "    code = str(code).upper().strip()\n",
    "    if not code or len(code) < 3:\n",
    "        return code\n",
    "    # Numeric ICD-9: 3-digit category\n",
    "    if code[0].isdigit():\n",
    "        return code.split('.')[0][:3]\n",
    "    # V-codes: 3-digit category (e.g., V10.x -> V10)\n",
    "    if code.startswith('V'):\n",
    "        base = code.split('.')[0]\n",
    "        return base[:3]\n",
    "    # E-codes: 4-digit category (e.g., E812.x -> E812)\n",
    "    if code.startswith('E'):\n",
    "        base = code.split('.')[0]\n",
    "        return base[:4] if len(base) >= 4 else base\n",
    "    return code\n",
    "\n",
    "def serialize_structured_readable(row: pd.Series) -> str:\n",
    "    ndc  = \" \".join(to_list(row.get(\"ndc\", []))[:24])\n",
    "    proc = \" \".join(to_list(row.get(\"pro_code\", []))[:24])\n",
    "    labs = \" \".join(to_list(row.get(\"lab_test\", []))[:48])\n",
    "    parts=[]\n",
    "    parts.append(f\"DEMOGRAPHICS: gender={row.get('gender','')} age_group={row.get('age','')}\")\n",
    "    if ndc:  parts.append(f\"MEDICATIONS: {ndc}\")\n",
    "    if proc: parts.append(f\"PROCEDURES: {proc}\")\n",
    "    if labs: parts.append(f\"LAB TESTS: {labs}\")\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "def serialize_notes(row: pd.Series) -> str:\n",
    "    parts=[]\n",
    "    for col in TEXT_COLS_SAFE:\n",
    "        if col in row:\n",
    "            t = clean_text(row[col])\n",
    "            if t: parts.append(f\"{col}: {t}\")\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "def token_len(tok, text: str) -> int:\n",
    "    return int(tok(text, add_special_tokens=False, return_length=True)[\"length\"][0])\n",
    "\n",
    "def chat_token_len(tok, msgs: List[Dict], add_generation_prompt: bool) -> int:\n",
    "    txt = tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=add_generation_prompt)\n",
    "    return token_len(tok, txt)\n",
    "\n",
    "def build_textgen_prompt_budgeted(row: pd.Series, tok, max_len: int,\n",
    "                                  min_assist_tokens: int, N_max_terms: int\n",
    "                                  ) -> Tuple[str, Dict[str, int]]:\n",
    "    head = []\n",
    "    head.append(f\"[VISIT] subject_id={row.get('subject_id_x','?')} hadm_id={row.get('hadm_id','?')}\")\n",
    "    head.append(serialize_structured_readable(row))\n",
    "    notes_full = serialize_notes(row)\n",
    "\n",
    "    tail = [\n",
    "        \"[TASK] List the final clinical diagnoses for this admission.\",\n",
    "        \"[FORMAT]\",\n",
    "        \"- One diagnosis per line\",\n",
    "        \"- Avoid abbreviations if possible\",\n",
    "        \"- No ICD codes or explanations\",\n",
    "        f\"- Maximum: {N_max_terms} lines\",\n",
    "        \"[OUTPUT]\",\n",
    "    ]\n",
    "    def assemble(notes_text: str) -> str:\n",
    "        parts = [*head]\n",
    "        if notes_text: parts.append(notes_text)\n",
    "        parts.extend(tail)\n",
    "        return \"\\n\".join([p for p in parts if p])\n",
    "\n",
    "    prompt = assemble(notes_full)\n",
    "    prompt_len_tokens = chat_token_len(tok, [{\"role\":\"user\",\"content\":prompt}], add_generation_prompt=True)\n",
    "    if prompt_len_tokens <= max_len - min_assist_tokens:\n",
    "        return prompt, {\"prompt_tokens\": prompt_len_tokens, \"notes_kept_chars\": len(notes_full), \"notes_trimmed\": 0}\n",
    "\n",
    "    # binary search to fit\n",
    "    left, right = 0, len(notes_full)\n",
    "    best_prompt, best_len, best_mid = None, None, 0\n",
    "    while left <= right:\n",
    "        mid = (left + right) // 2\n",
    "        trial_notes = notes_full[:mid]\n",
    "        trial_prompt = assemble(trial_notes)\n",
    "        L = chat_token_len(tok, [{\"role\":\"user\",\"content\":trial_prompt}], add_generation_prompt=True)\n",
    "        if L <= max_len - min_assist_tokens:\n",
    "            best_prompt, best_len, best_mid = trial_prompt, L, mid\n",
    "            left = mid + 1\n",
    "        else:\n",
    "            right = mid - 1\n",
    "    if best_prompt is None:\n",
    "        trial_prompt = assemble(\"\")\n",
    "        L = chat_token_len(tok, [{\"role\":\"user\",\"content\":trial_prompt}], add_generation_prompt=True)\n",
    "        best_prompt, best_len, best_mid = trial_prompt, L, 0\n",
    "    kept_chars = best_mid\n",
    "    return best_prompt, {\"prompt_tokens\": best_len, \"notes_kept_chars\": kept_chars, \"notes_trimmed\": len(notes_full)-kept_chars}\n",
    "\n",
    "# ------------------ datasets ------------------\n",
    "class SFTTextGenDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, tokenizer,\n",
    "                 label_col: str,\n",
    "                 target_mode: str,\n",
    "                 icd_index_dir: str,\n",
    "                 max_len: int,\n",
    "                 N_max_terms: int,\n",
    "                 min_assistant_tokens: int):\n",
    "        self.tok = tokenizer\n",
    "        self.label_col = label_col\n",
    "        self.target_mode = target_mode\n",
    "        self.max_len = max_len\n",
    "        self.N_max_terms = N_max_terms\n",
    "        self.min_assistant_tokens = max(1, int(min_assistant_tokens))\n",
    "\n",
    "        self.code2title = {}\n",
    "        if target_mode == \"icd_titles\":\n",
    "            try:\n",
    "                with open(os.path.join(icd_index_dir, \"code2title.json\"), \"r\") as f:\n",
    "                    self.code2title = json.load(f)\n",
    "                if is_main_process():\n",
    "                    log.info(f\"Loaded {len(self.code2title)} ICD-9 titles\")\n",
    "            except Exception as e:\n",
    "                if is_main_process():\n",
    "                    log.warning(f\"Could not load code2title.json: {e}\")\n",
    "\n",
    "        inputs, targets, kept_idx = [], [], []\n",
    "        dropped_empty_targets = 0\n",
    "        dropped_truncated = 0\n",
    "        trimmed_notes = 0\n",
    "        prompt_tok_lens, assistant_tok_lens = [], []\n",
    "\n",
    "        for idx, row in df.reset_index(drop=True).iterrows():\n",
    "            if target_mode == \"icd_titles\":\n",
    "                codes = [format_icd9(c) for c in to_list(row.get(label_col, [])) if c]\n",
    "                codes = [c for c in codes if is_valid_icd9(c)]\n",
    "                titles = []\n",
    "                for c in codes:\n",
    "                    t = self.code2title.get(c, \"\").strip()\n",
    "                    if len(t) > 3:\n",
    "                        titles.append(f\"- {t}\")\n",
    "                target = \"\\n\".join(titles)\n",
    "                has_supervision = len(titles) > 0\n",
    "            else:\n",
    "                target_raw = clean_text(row.get(\"Discharge Diagnosis\",\"\"))\n",
    "                target = target_raw if len(target_raw) >= 5 else \"\"\n",
    "                has_supervision = len(target) > 0\n",
    "\n",
    "            if not has_supervision:\n",
    "                dropped_empty_targets += 1\n",
    "                continue\n",
    "\n",
    "            prompt, stat = build_textgen_prompt_budgeted(\n",
    "                row, self.tok, self.max_len, self.min_assistant_tokens, self.N_max_terms\n",
    "            )\n",
    "            if stat.get(\"notes_trimmed\", 0) > 0:\n",
    "                trimmed_notes += 1\n",
    "\n",
    "            user_msg = {\"role\": \"user\", \"content\": prompt}\n",
    "            asst_msg = {\"role\": \"assistant\", \"content\": target}\n",
    "\n",
    "            prompt_text = self.tok.apply_chat_template([user_msg], tokenize=False, add_generation_prompt=True)\n",
    "            full_text   = self.tok.apply_chat_template([user_msg, asst_msg], tokenize=False, add_generation_prompt=False)\n",
    "\n",
    "            prompt_ids = self.tok(prompt_text, return_tensors=\"pt\", truncation=True, max_length=self.max_len).input_ids[0]\n",
    "            full       = self.tok(full_text,   return_tensors=\"pt\", truncation=True, max_length=self.max_len)\n",
    "            input_ids  = full.input_ids[0]\n",
    "            labels     = input_ids.clone()\n",
    "\n",
    "            prompt_len = min(len(prompt_ids), len(input_ids))\n",
    "            labels[:prompt_len] = -100  # ignore prompt\n",
    "\n",
    "            assistant_len = int((labels != -100).sum().item())\n",
    "            if assistant_len < 1:\n",
    "                dropped_truncated += 1\n",
    "                continue\n",
    "\n",
    "            inputs.append(input_ids)\n",
    "            targets.append(labels)\n",
    "            kept_idx.append(idx)\n",
    "            prompt_tok_lens.append(int(prompt_len))\n",
    "            assistant_tok_lens.append(int(assistant_len))\n",
    "\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.kept_idx = kept_idx\n",
    "\n",
    "        if is_main_process():\n",
    "            log.info(\n",
    "                f\"SFT dataset: kept={len(self.inputs)} \"\n",
    "                f\"dropped_empty_targets={dropped_empty_targets} \"\n",
    "                f\"dropped_truncated_targets={dropped_truncated} \"\n",
    "                f\"trimmed_notes={trimmed_notes} (mode={target_mode})\"\n",
    "            )\n",
    "            if prompt_tok_lens and assistant_tok_lens:\n",
    "                p = np.array(prompt_tok_lens); a = np.array(assistant_tok_lens)\n",
    "                log.info(\n",
    "                    \"Token stats (kept): \"\n",
    "                    f\"prompt_mean={p.mean():.1f}, p95={np.percentile(p,95):.0f} | \"\n",
    "                    f\"assistant_mean={a.mean():.1f}, p95={np.percentile(a,95):.0f}\"\n",
    "                )\n",
    "\n",
    "    def __len__(self): return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.inputs[idx]\n",
    "        labels = self.targets[idx]\n",
    "        attn = torch.ones_like(input_ids, dtype=torch.long)\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attn, \"labels\": labels, \"idx\": self.kept_idx[idx]}\n",
    "\n",
    "def pad_collate(features, tok):\n",
    "    pad_id = tok.pad_token_id\n",
    "    max_len = max(len(f[\"input_ids\"]) for f in features)\n",
    "    B = len(features)\n",
    "    input_ids = torch.full((B, max_len), pad_id, dtype=torch.long)\n",
    "    attention_mask = torch.zeros((B, max_len), dtype=torch.long)\n",
    "    labels = torch.full((B, max_len), -100, dtype=torch.long)\n",
    "    for i, f in enumerate(features):\n",
    "        L = len(f[\"input_ids\"])\n",
    "        input_ids[i,:L] = f[\"input_ids\"]\n",
    "        attention_mask[i,:L] = f[\"attention_mask\"]\n",
    "        labels[i,:L] = f[\"labels\"]\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "\n",
    "# ------------------ LLM & LoRA ------------------\n",
    "def load_llm_with_lora(model_name):\n",
    "    if torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8:\n",
    "        dtype = torch.bfloat16\n",
    "    elif torch.cuda.is_available():\n",
    "        dtype = torch.float16\n",
    "    else:\n",
    "        dtype = torch.float32\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    if tok.pad_token_id is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    tok.padding_side = \"right\"\n",
    "\n",
    "    base = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, torch_dtype=dtype, low_cpu_mem_usage=True\n",
    "    )\n",
    "    base.config.pad_token_id = tok.pad_token_id\n",
    "    base.config.use_cache = False  # for grad checkpointing\n",
    "\n",
    "    lora_cfg = LoraConfig(\n",
    "        r=16, lora_alpha=32, lora_dropout=0.05, bias=\"none\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    model = get_peft_model(base, lora_cfg)\n",
    "    if hasattr(model, \"enable_input_require_grads\"):\n",
    "        model.enable_input_require_grads()\n",
    "    else:\n",
    "        try:\n",
    "            model.get_input_embeddings().weight.requires_grad_(True)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if is_main_process():\n",
    "        model.print_trainable_parameters()\n",
    "    if hasattr(model, \"generation_config\"):\n",
    "        gc = model.generation_config\n",
    "        for a in (\"temperature\",\"top_p\",\"top_k\"):\n",
    "            if hasattr(gc, a): setattr(gc, a, None)\n",
    "        if hasattr(gc, \"do_sample\"): gc.do_sample = False\n",
    "    return model, tok\n",
    "\n",
    "# ------------------ generation ------------------\n",
    "def build_generate_kwargs(\n",
    "    decoding:str, max_new:int,\n",
    "    eos_id:int, pad_id:int,\n",
    "    num_beams:int=1, temperature:float=1.0, top_p:float=1.0, top_k:int=0,\n",
    "    no_repeat_ngram:int=0\n",
    "):\n",
    "    decoding = decoding.lower()\n",
    "    if decoding == \"beam\":\n",
    "        return dict(\n",
    "            max_new_tokens=max_new,\n",
    "            do_sample=False,\n",
    "            num_beams=max(2, num_beams),\n",
    "            eos_token_id=eos_id,\n",
    "            pad_token_id=pad_id,\n",
    "            no_repeat_ngram_size=no_repeat_ngram,\n",
    "            return_dict_in_generate=True,\n",
    "        )\n",
    "    if decoding == \"sample\":\n",
    "        return dict(\n",
    "            max_new_tokens=max_new,\n",
    "            do_sample=True,\n",
    "            temperature=max(0.1, float(temperature)),\n",
    "            top_p=min(1.0, max(0.0, float(top_p))),\n",
    "            top_k=max(0, int(top_k)),\n",
    "            num_beams=1,\n",
    "            eos_token_id=eos_id,\n",
    "            pad_token_id=pad_id,\n",
    "            no_repeat_ngram_size=no_repeat_ngram,\n",
    "            return_dict_in_generate=True,\n",
    "        )\n",
    "    # greedy\n",
    "    return dict(\n",
    "        max_new_tokens=max_new,\n",
    "        do_sample=False,\n",
    "        num_beams=1,\n",
    "        eos_token_id=eos_id,\n",
    "        pad_token_id=pad_id,\n",
    "        no_repeat_ngram_size=no_repeat_ngram,\n",
    "        return_dict_in_generate=True,\n",
    "    )\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_terms(model, tokenizer, prompts: List[str],\n",
    "                   max_len: int, gen_kwargs: dict, batch_size: int):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    out_all = []\n",
    "\n",
    "    for i in range(0, len(prompts), batch_size):\n",
    "        batch_prompts = prompts[i:i+batch_size]\n",
    "        prompt_texts = [\n",
    "            tokenizer.apply_chat_template(\n",
    "                [{\"role\":\"user\",\"content\":p}],\n",
    "                tokenize=False, add_generation_prompt=True\n",
    "            ) for p in batch_prompts\n",
    "        ]\n",
    "        enc = tokenizer(\n",
    "            prompt_texts, return_tensors=\"pt\",\n",
    "            padding=True, truncation=True, max_length=max_len\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.amp.autocast('cuda', enabled=(device.type==\"cuda\")):\n",
    "            gen = model.generate(**enc, **gen_kwargs)\n",
    "\n",
    "        seq = gen.sequences  # [B, L_in + L_new]\n",
    "        in_lens = enc[\"attention_mask\"].sum(dim=1).tolist()\n",
    "        for b in range(seq.size(0)):\n",
    "            new_tokens = seq[b, in_lens[b]:]\n",
    "            text = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "            # normalize to list of lines\n",
    "            terms=[]\n",
    "            for line in text.split(\"\\n\"):\n",
    "                t = re.sub(r\"^[\\-\\*\\u2022]+\\s*\", \"\", line).strip()\n",
    "                if t: terms.append(t)\n",
    "            out_all.append(terms)\n",
    "    return out_all\n",
    "\n",
    "# ------------------ HF SapBERT mean encoder ------------------\n",
    "class HFMeanEncoder:\n",
    "    def __init__(self, model_name: str):\n",
    "        self.tokenizer = HFTok.from_pretrained(model_name)\n",
    "        self.model = HFModel.from_pretrained(model_name)\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device).eval()\n",
    "\n",
    "    def _mean_pool(self, last_hidden, attention_mask):\n",
    "        mask = attention_mask.unsqueeze(-1).expand(last_hidden.size()).float()\n",
    "        s = torch.sum(last_hidden * mask, dim=1)\n",
    "        d = torch.clamp(mask.sum(dim=1), min=1e-9)\n",
    "        return s / d\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode(self, texts: List[str], batch_size=32) -> np.ndarray:\n",
    "        if not texts: return np.zeros((0,768), dtype=np.float32)\n",
    "        chunks=[]\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            enc = self.tokenizer(batch, padding=True, truncation=True, max_length=128, return_tensors=\"pt\").to(self.device)\n",
    "            out = self.model(**enc)\n",
    "            emb = self._mean_pool(out.last_hidden_state, enc.attention_mask)\n",
    "            emb = torch.nn.functional.normalize(emb, p=2, dim=1)\n",
    "            chunks.append(emb.cpu().numpy())\n",
    "        return np.vstack(chunks)\n",
    "\n",
    "# ------------------ Mapper ------------------\n",
    "class ICDMapper:\n",
    "    def __init__(self, index_dir, encoder_model_cli=None,\n",
    "                 tau_cos=0.40, tau_final=0.60,\n",
    "                 w_cos=0.6, w_fuz=0.4,\n",
    "                 faiss_rows=20):\n",
    "        self.dir = index_dir\n",
    "        self.faiss_rows = faiss_rows\n",
    "        self.tau_cos = tau_cos\n",
    "        self.tau_final = tau_final\n",
    "        self.w_cos = w_cos\n",
    "        self.w_fuz = w_fuz\n",
    "        self.last_stats=[]\n",
    "\n",
    "        idx_path = os.path.join(self.dir, \"icd.faiss\")\n",
    "        if not os.path.exists(idx_path):\n",
    "            raise FileNotFoundError(f\"FAISS index not found: {idx_path}\")\n",
    "        self.index = faiss.read_index(idx_path)\n",
    "\n",
    "        rows_path = os.path.join(self.dir, \"rows.json\")\n",
    "        if not os.path.exists(rows_path):\n",
    "            raise FileNotFoundError(f\"rows.json not found: {rows_path}\")\n",
    "        with open(rows_path, \"r\") as f:\n",
    "            self.rows = json.load(f)\n",
    "        if not isinstance(self.rows, list):\n",
    "            raise ValueError(\"rows.json must be a list of {text, code}\")\n",
    "        if len(self.rows) != self.index.ntotal:\n",
    "            raise ValueError(f\"rows.json length ({len(self.rows)}) must match FAISS ntotal ({self.index.ntotal})\")\n",
    "\n",
    "        meta = {}\n",
    "        meta_path = os.path.join(self.dir, \"meta.json\")\n",
    "        if os.path.exists(meta_path):\n",
    "            with open(meta_path,\"r\") as f:\n",
    "                meta = json.load(f)\n",
    "        self.metric = meta.get(\"metric\",\"ip\").lower()  # \"ip\" or \"l2\"\n",
    "        enc_name = meta.get(\"encoder_model\", encoder_model_cli) or \"cambridgeltl/SapBERT-from-PubMedBERT-fulltext\"\n",
    "        self.encoder = HFMeanEncoder(enc_name)\n",
    "\n",
    "        if is_main_process():\n",
    "            log.info(f\"FAISS index loaded: {self.index.ntotal} rows (metric={self.metric})\")\n",
    "            log.info(f\"Encoder model: {enc_name}\")\n",
    "\n",
    "    def map_terms(self, term_lists: List[List[str]]):\n",
    "        self.last_stats=[]\n",
    "        all_codes=[]\n",
    "        for terms in term_lists:\n",
    "            mapped=set()\n",
    "            if not terms:\n",
    "                all_codes.append([])\n",
    "                self.last_stats.append((0,0))\n",
    "                continue\n",
    "\n",
    "            embs = self.encoder.encode(terms)\n",
    "            n_mapped=0\n",
    "            for t_idx, term in enumerate(terms):\n",
    "                if not term or len(term) < 3: continue\n",
    "                norm_t = norm_text(term)\n",
    "                if len(norm_t) < 3: continue\n",
    "\n",
    "                D, I = self.index.search(embs[t_idx:t_idx+1], self.faiss_rows)  # (1,k)\n",
    "                D, I = D[0], I[0]\n",
    "\n",
    "                cand=[]\n",
    "                for j, row_idx in enumerate(I):\n",
    "                    if row_idx < 0: continue\n",
    "                    entry = self.rows[row_idx]\n",
    "                    cand_text = entry.get(\"text\",\"\")\n",
    "                    cand_code = entry.get(\"code\",\"\")\n",
    "                    if not cand_text or not cand_code: continue\n",
    "\n",
    "                    if self.metric == \"ip\":\n",
    "                        cos = float(D[j])  # inner product on normalized vectors ≈ cosine\n",
    "                    else:\n",
    "                        cos = 1.0 - float(D[j]) / 2.0  # L2 on normalized vectors -> cosine-like\n",
    "\n",
    "                    if cos < self.tau_cos: continue\n",
    "\n",
    "                    fuzzy = token_set_ratio(norm_t, norm_text(cand_text)) / 100.0\n",
    "                    score = self.w_cos * cos + self.w_fuz * fuzzy\n",
    "                    if score >= self.tau_final:\n",
    "                        cand.append((cand_code, score))\n",
    "\n",
    "                if cand:\n",
    "                    cand.sort(key=lambda x: x[1], reverse=True)\n",
    "                    mapped.add(cand[0][0]); n_mapped += 1\n",
    "\n",
    "            self.last_stats.append((len(terms), n_mapped))\n",
    "            all_codes.append(sorted(mapped))\n",
    "        return all_codes\n",
    "\n",
    "# ------------------ eval helpers ------------------\n",
    "def build_eval_labels(train_gold_lists, head_k=0):\n",
    "    counter = Counter([c for codes in train_gold_lists for c in codes])\n",
    "    if head_k and head_k > 0:\n",
    "        return [c for c,_ in counter.most_common(head_k)]\n",
    "    return sorted(counter.keys())\n",
    "\n",
    "def restrict_to(codes_lists, allowed):\n",
    "    S=set(allowed)\n",
    "    return [[c for c in codes if c in S] for codes in codes_lists]\n",
    "\n",
    "def multihot(codes_lists, labels):\n",
    "    idx = {c:i for i,c in enumerate(labels)}\n",
    "    Y = np.zeros((len(codes_lists), len(labels)), dtype=np.int32)\n",
    "    for i, lst in enumerate(codes_lists):\n",
    "        for c in lst:\n",
    "            j = idx.get(c)\n",
    "            if j is not None: \n",
    "                Y[i,j]=1\n",
    "    return Y\n",
    "\n",
    "def eval_pack(y_true, y_pred):\n",
    "    return {\n",
    "        \"precision_micro\": float(precision_score(y_true, y_pred, average='micro', zero_division=0)),\n",
    "        \"recall_micro\":    float(recall_score(y_true, y_pred, average='micro', zero_division=0)),\n",
    "        \"f1_micro\":        float(f1_score(y_true, y_pred, average='micro', zero_division=0)),\n",
    "        \"precision_macro\": float(precision_score(y_true, y_pred, average='macro', zero_division=0)),\n",
    "        \"recall_macro\":    float(recall_score(y_true, y_pred, average='macro', zero_division=0)),\n",
    "        \"f1_macro\":        float(f1_score(y_true, y_pred, average='macro', zero_division=0)),\n",
    "        \"precision_samples\": float(precision_score(y_true, y_pred, average='samples', zero_division=0)),\n",
    "        \"recall_samples\":    float(recall_score(y_true, y_pred, average='samples', zero_division=0)),\n",
    "        \"f1_samples\":        float(f1_score(y_true, y_pred, average='samples', zero_division=0)),\n",
    "    }\n",
    "\n",
    "def add_parent_macro_f1(metrics, gold_lists, pred_lists):\n",
    "    g = [[get_icd9_parent(c) for c in lst] for lst in gold_lists]\n",
    "    p = [[get_icd9_parent(c) for c in lst] for lst in pred_lists]                                                                \n",
    "    labels = sorted({x for lst in g for x in lst})\n",
    "    Yg = multihot(g, labels); Yp = multihot(p, labels)\n",
    "    metrics[\"f1_macro_parent\"] = float(f1_score(Yg, Yp, average=\"macro\", zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49da50a2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# In [1]\n",
    "import os, sys, json, re, math, pickle\n",
    "from collections import defaultdict, Counter\n",
    "from typing import Dict, List, Set, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# allow importing your helper utilities\n",
    "PROJECT_DIR = \"/data/horse/ws/arsi805e-finetune/Thesis/MasterThesis\"\n",
    "sys.path.append(PROJECT_DIR)\n",
    "\n",
    "# === Assets (adjust if needed) ===\n",
    "KG_NODES_CSV = f\"{PROJECT_DIR}/KG/kg_output4/kg_nodes.csv\"\n",
    "KG_EDGES_CSV = f\"{PROJECT_DIR}/KG/kg_output4/kg_edges.csv\"\n",
    "\n",
    "ICD9_PROC_MAP = f\"{PROJECT_DIR}/KG/kg_output4/code2cui_icd9_proc.pkl\"\n",
    "LOINC_MAP     = f\"{PROJECT_DIR}/KG/kg_output4/code2cui_loinc.pkl\"\n",
    "ATC_MAP       = f\"{PROJECT_DIR}/KG/kg_output4/code2cui_atc.pkl\"\n",
    "\n",
    "CODE2NAME_PKL    = f\"{PROJECT_DIR}/gen/withKG/RAG/kg_recommender/code2name.pkl\"  # primary titles\n",
    "ICD9_PROFILES_JS = f\"{PROJECT_DIR}/gen/withKG/RAG/kg_recommender/icd9_profiles.json\"  # optional fallback\n",
    "\n",
    "DATA_PKL    = f\"{PROJECT_DIR}/dataset/final_data/test_df.pkl\"\n",
    "BASE_MODEL  = f\"{PROJECT_DIR}/models/Llama-3.1-8B-Instruct\"  # tokenizer only for budgeting\n",
    "\n",
    "# Prompt budgets (align with your SLURM script)\n",
    "MAX_LEN          = 4096\n",
    "KG_HINT_BUDGET   = 600   # tokens reserved for [KG CONTEXT]\n",
    "N_MAX_TERMS      = 12    # lines to parse after [OUTPUT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dd5af9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------- misc text helpers -------------------------\n",
    "def _strip(x: str) -> str:\n",
    "    return str(x or \"\").strip().upper().replace(\" \", \"\")\n",
    "\n",
    "def count_tokens(tok, text: str) -> int:\n",
    "    if not text:\n",
    "        return 0\n",
    "    enc = tok(text, add_special_tokens=False, return_length=True)\n",
    "    return int(enc[\"length\"][0])\n",
    "\n",
    "def trim_to_token_budget(tok, text: str, max_tokens: int) -> str:\n",
    "    if max_tokens <= 0 or not text:\n",
    "        return \"\"\n",
    "    if count_tokens(tok, text) <= max_tokens:\n",
    "        return text\n",
    "    lo, hi, best = 0, len(text), \"\"\n",
    "    while lo <= hi:\n",
    "        mid = (lo + hi) // 2\n",
    "        cand = text[:mid]\n",
    "        if count_tokens(tok, cand) <= max_tokens:\n",
    "            best = cand\n",
    "            lo = mid + 1\n",
    "        else:\n",
    "            hi = mid - 1\n",
    "    return best\n",
    "\n",
    "# ------------------------- load KG from CSVs -------------------------\n",
    "def load_kg(nodes_csv: str, edges_csv: str):\n",
    "    \"\"\"\n",
    "    Build a directed KG:\n",
    "    - Nodes: CUI with attributes {name, semantic_type}\n",
    "    - Edges: u -> v with attributes {rel, rela}\n",
    "    Also, collect CUI -> ICD9 codes present on node rows where sab contains ICD9CM.\n",
    "    \"\"\"\n",
    "    nodes_df = pd.read_csv(nodes_csv)\n",
    "    edges_df = pd.read_csv(edges_csv)\n",
    "\n",
    "    # Normalize expected columns (from your sample)\n",
    "    # nodes: ['cui', 'name', 'sab', 'code', 'semantic_type']\n",
    "    # edges: ['cui_start','name_start','sab_start','codes_start','rel','rela',\n",
    "    #         'sab_relation','cui_target','name_target','sab_target','codes_target']\n",
    "\n",
    "    G = nx.DiGraph()\n",
    "    node_name, node_sem = {}, {}\n",
    "    cui_to_icd9 = defaultdict(set)\n",
    "\n",
    "    for _, r in nodes_df.iterrows():\n",
    "        cui = str(r.get(\"cui\",\"\")).strip()\n",
    "        nm  = str(r.get(\"name\",\"\")).strip()\n",
    "        sem = str(r.get(\"semantic_type\",\"\")).strip()\n",
    "        sab = str(r.get(\"sab\",\"\")).strip().upper()\n",
    "        code= str(r.get(\"code\",\"\")).strip()\n",
    "        if not cui:\n",
    "            continue\n",
    "        if cui not in G:\n",
    "            G.add_node(cui)\n",
    "        node_name[cui] = nm or node_name.get(cui, \"\")\n",
    "        node_sem[cui]  = sem or node_sem.get(cui, \"\")\n",
    "        # anchor ICD-9 codes if SAB suggests ICD9 (diagnosis)\n",
    "        if sab == \"ICD9CM\" and code:\n",
    "            # many rows carry ranges or variants; keep raw for now\n",
    "            cui_to_icd9[cui].add(code)\n",
    "\n",
    "    for _, r in edges_df.iterrows():\n",
    "        u = str(r.get(\"cui_start\",\"\")).strip()\n",
    "        v = str(r.get(\"cui_target\",\"\")).strip()\n",
    "        if not u or not v:\n",
    "            continue\n",
    "        if u not in G:\n",
    "            G.add_node(u)\n",
    "        if v not in G:\n",
    "            G.add_node(v)\n",
    "        rel  = str(r.get(\"rel\",\"\") or \"\").strip()\n",
    "        rela = str(r.get(\"rela\",\"\") or \"\").strip()\n",
    "        # store only one edge; if multi-edges exist in CSV, DiGraph keeps last attrs\n",
    "        G.add_edge(u, v, rel=rel, rela=rela)\n",
    "\n",
    "    return G, node_name, node_sem, {k: sorted(v) for k,v in cui_to_icd9.items()}\n",
    "\n",
    "# ------------------------- evidence → CUIs -------------------------\n",
    "def format_icd9_proc_from_pro(c: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert tokens like PRO_5491 or 5491 to ICD-9-Proc format '54.91'\n",
    "    \"\"\"\n",
    "    s = _strip(c)\n",
    "    if s.startswith(\"PRO_\"):\n",
    "        s = s[4:]\n",
    "    digits = re.sub(r\"[^0-9]\", \"\", s)\n",
    "    if not digits:\n",
    "        return \"\"\n",
    "    if len(digits) >= 3:\n",
    "        return digits[:2] + \".\" + digits[2:]\n",
    "    return digits\n",
    "\n",
    "def to_list(x):\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return []\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if isinstance(x, (set, tuple)):\n",
    "        return list(x)\n",
    "    # comma-separated or space-separated\n",
    "    if isinstance(x, str):\n",
    "        if '|' in x:\n",
    "            return [p.strip() for p in x.split('|') if p.strip()]\n",
    "        if ',' in x:\n",
    "            return [p.strip() for p in x.split(',') if p.strip()]\n",
    "        return [x.strip()] if x.strip() else []\n",
    "    return [x]\n",
    "\n",
    "def visit_evidence_cuis(row: pd.Series,\n",
    "                        icd9_proc_map: Dict[str, List[str]],\n",
    "                        loinc_map: Dict[str, List[str]],\n",
    "                        atc_map: Dict[str, List[str]]) -> Tuple[Dict[str,List[str]], Set[str]]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      src2cuis: { \"ATC:XXX\": [...], \"LNC:YYY\":[...], \"PROC:12.34\":[...] }\n",
    "      ev_union: set(CUIs)\n",
    "    \"\"\"\n",
    "    src2cuis, ev = {}, set()\n",
    "\n",
    "    # ATC from 'ndc'\n",
    "    for c in to_list(row.get(\"ndc\", [])):\n",
    "        key = _strip(c)\n",
    "        cuis = atc_map.get(key, [])\n",
    "        if cuis:\n",
    "            src2cuis[f\"ATC:{key}\"] = cuis\n",
    "            ev.update(cuis)\n",
    "\n",
    "    # LOINC from 'lab_test'\n",
    "    for c in to_list(row.get(\"lab_test_loinc\", [])):\n",
    "        key = _strip(c)\n",
    "        cuis = loinc_map.get(key, [])\n",
    "        if cuis:\n",
    "            src2cuis[f\"LNC:{key}\"] = cuis\n",
    "            ev.update(cuis)\n",
    "\n",
    "    # procedures from 'pro_code'\n",
    "    for c in to_list(row.get(\"pro_code\", [])):\n",
    "        pc = format_icd9_proc_from_pro(c)\n",
    "        if not pc:\n",
    "            continue\n",
    "        cuis = icd9_proc_map.get(pc, [])\n",
    "        if cuis:\n",
    "            src2cuis[f\"PROC:{pc}\"] = cuis\n",
    "            ev.update(cuis)\n",
    "\n",
    "    return src2cuis, ev\n",
    "\n",
    "# ------------------------- note cues (type heuristics) -------------------------\n",
    "_HISTORY_PAT  = re.compile(r\"\\b(history of|family history)\\b\", re.I)\n",
    "_EXTERNAL_PAT = re.compile(r\"\\b(accident|trauma|injury|fall|assault)\\b\", re.I)\n",
    "\n",
    "def extract_note_cues(text: str) -> Set[str]:\n",
    "    s = (text or \"\").lower()\n",
    "    cues=set()\n",
    "    if _HISTORY_PAT.search(s): cues.add(\"history\")\n",
    "    if _EXTERNAL_PAT.search(s): cues.add(\"external\")\n",
    "    return cues\n",
    "\n",
    "# ------------------------- titles for ICD-9 anchors -------------------------\n",
    "def load_profiles_json(path: str) -> Dict[str, str]:\n",
    "    if not path or not os.path.exists(path):\n",
    "        return {}\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def _profile_title_fallback(code: str, profiles: Dict[str,str]) -> str:\n",
    "    \"\"\"\n",
    "    Given a line like '003.0 :: Salmonella gastroenteritis ; [H1] ...',\n",
    "    return the part after '::' and before the first ';' as a title.\n",
    "    \"\"\"\n",
    "    raw = profiles.get(code, \"\")\n",
    "    if not raw:\n",
    "        return \"\"\n",
    "    # expect \"CODE :: Title ; ...\"\n",
    "    if \"::\" in raw:\n",
    "        after = raw.split(\"::\", 1)[1].strip()\n",
    "        return after.split(\";\", 1)[0].strip()\n",
    "    return raw.strip()\n",
    "\n",
    "def short_title_for_code(code: str, code2name: Dict[str,str], profiles: Dict[str,str]) -> str:\n",
    "    t = code2name.get(code)\n",
    "    if t:\n",
    "        return t\n",
    "    return _profile_title_fallback(code, profiles) or \"\"\n",
    "\n",
    "# ------------------------- H1/H2 mining -------------------------\n",
    "def mine_paths_h1_h2(\n",
    "    G: nx.DiGraph,\n",
    "    node_name: Dict[str,str],\n",
    "    node_sem: Dict[str,str],\n",
    "    cui_to_icd9: Dict[str, List[str]],\n",
    "    ev_cuis: Set[str],\n",
    "    notes_text: str,\n",
    "    rel_whitelist: Set[str] = None,\n",
    "    rela_whitelist: Set[str] = None,\n",
    "    h1_per_src: int = 3,\n",
    "    h2_per_h1: int = 3,\n",
    "    prefer_dx_like: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Return dict with:\n",
    "      - H1: list of dicts with fields: src_cui, rel, rela, nbr_cui, src_name, nbr_name\n",
    "      - H2: list of dicts with fields: u, v, w, rel_uv, rela_uv, rel_vw, rela_vw, u_name, v_name, w_name, w_icd9 (list)\n",
    "    Preference:\n",
    "      - optionally prioritize v or w nodes that have ICD-9 anchors (for display anchoring)\n",
    "    \"\"\"\n",
    "    cues = extract_note_cues(notes_text)\n",
    "    H1, H2 = [], []\n",
    "\n",
    "    for u in list(ev_cuis):\n",
    "        if u not in G:\n",
    "            continue\n",
    "        # H1 neighbors\n",
    "        ctr = 0\n",
    "        for v in G.successors(u):\n",
    "            d = G[u][v]\n",
    "            rel, rela = (d.get(\"rel\",\"\") or \"\").strip(), (d.get(\"rela\",\"\") or \"\").strip()\n",
    "            if rel_whitelist and rel not in rel_whitelist:\n",
    "                continue\n",
    "            if rela_whitelist and rela not in rela_whitelist:\n",
    "                continue\n",
    "            H1.append({\n",
    "                \"src_cui\": u,\n",
    "                \"rel\": rel,\n",
    "                \"rela\": rela,\n",
    "                \"nbr_cui\": v,\n",
    "                \"src_name\": node_name.get(u,\"\"),\n",
    "                \"nbr_name\": node_name.get(v,\"\"),\n",
    "            })\n",
    "            ctr += 1\n",
    "            if ctr >= h1_per_src:\n",
    "                break\n",
    "\n",
    "        # H2: u -> v -> w\n",
    "        h2_ctr = 0\n",
    "        for v in G.successors(u):\n",
    "            if v not in G:\n",
    "                continue\n",
    "            # enumerate w\n",
    "            cands = []\n",
    "            for w in G.successors(v):\n",
    "                duv = G[u][v]; dvw = G[v][w]\n",
    "                cand = {\n",
    "                    \"u\": u, \"v\": v, \"w\": w,\n",
    "                    \"rel_uv\": (duv.get(\"rel\",\"\") or \"\").strip(),\n",
    "                    \"rela_uv\":(duv.get(\"rela\",\"\") or \"\").strip(),\n",
    "                    \"rel_vw\": (dvw.get(\"rel\",\"\") or \"\").strip(),\n",
    "                    \"rela_vw\":(dvw.get(\"rela\",\"\") or \"\").strip(),\n",
    "                    \"u_name\": node_name.get(u,\"\"),\n",
    "                    \"v_name\": node_name.get(v,\"\"),\n",
    "                    \"w_name\": node_name.get(w,\"\"),\n",
    "                    \"w_icd9\": cui_to_icd9.get(w, []),\n",
    "                }\n",
    "                # filter by whitelist\n",
    "                if rel_whitelist and (cand[\"rel_uv\"] not in rel_whitelist or cand[\"rel_vw\"] not in rel_whitelist):\n",
    "                    continue\n",
    "                if rela_whitelist and (cand[\"rela_uv\"] not in rela_whitelist or cand[\"rela_vw\"] not in rela_whitelist):\n",
    "                    continue\n",
    "                cands.append(cand)\n",
    "\n",
    "            # prioritize ‘diagnosis-like’ anchors for w (has ICD-9)\n",
    "            if prefer_dx_like:\n",
    "                cands.sort(key=lambda z: (len(z[\"w_icd9\"]) > 0, len(z[\"w_name\"]) > 0), reverse=True)\n",
    "\n",
    "            # keep up to h2_per_h1\n",
    "            for c in cands[:h2_per_h1]:\n",
    "                H2.append(c)\n",
    "                h2_ctr += 1\n",
    "            if h2_ctr >= h2_per_h1:\n",
    "                # per “v” cap; remove to be more exhaustive\n",
    "                pass\n",
    "\n",
    "    return {\"H1\": H1, \"H2\": H2}\n",
    "\n",
    "# ------------------------- render [KG CONTEXT] -------------------------\n",
    "def render_kg_context_paths(\n",
    "    tok,\n",
    "    paths: Dict[str, list],\n",
    "    code2name: Dict[str, str],\n",
    "    profiles: Dict[str, str],\n",
    "    budget_tokens: int = 600,\n",
    "    h2_first_ratio: float = 0.7\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Renders:\n",
    "      [KG CONTEXT — H2 PATHS]\n",
    "      - u --rela_uv/rel_uv--> v --rela_vw/rel_vw--> w [ICD-9: code — title | ...]\n",
    "      [KG CONTEXT — H1 NEIGHBORS]\n",
    "      - u --rela/rel--> v\n",
    "\n",
    "    All output is clipped to `budget_tokens`. Uses:\n",
    "      - short_title_for_code (code2name / profiles)\n",
    "      - count_tokens / trim_to_token_budget\n",
    "    \"\"\"\n",
    "\n",
    "    def _rela_or_rel(rela: str, rel: str) -> str:\n",
    "        rela = (rela or \"\").strip()\n",
    "        rel  = (rel  or \"\").strip()\n",
    "        return rela if rela else (rel if rel else \"\")\n",
    "\n",
    "    if budget_tokens <= 0:\n",
    "        return \"\"\n",
    "\n",
    "    # split budget between H2 and H1\n",
    "    h2_budget = int(budget_tokens * h2_first_ratio)\n",
    "    h1_budget = budget_tokens - h2_budget\n",
    "\n",
    "    # ---------------- H2 block with relation labels ----------------\n",
    "    h2_lines = [\"[KG CONTEXT — H2 PATHS]\"]\n",
    "    seen_h2 = set()\n",
    "    for c in paths.get(\"H2\", []):\n",
    "        # decide labeled arrows\n",
    "        r1 = _rela_or_rel(c.get(\"rela_uv\", \"\"), c.get(\"rel_uv\", \"\"))\n",
    "        r2 = _rela_or_rel(c.get(\"rela_vw\", \"\"), c.get(\"rel_vw\", \"\"))\n",
    "        arrow1 = f\" --{r1}--> \" if r1 else \" → \"\n",
    "        arrow2 = f\" --{r2}--> \" if r2 else \" → \"\n",
    "\n",
    "        # anchors (ICD-9 codes attached to w)\n",
    "        anchors = []\n",
    "        for code in c.get(\"w_icd9\", []) or []:\n",
    "            title = short_title_for_code(code, code2name, profiles)\n",
    "            anchors.append(f\"{code} — {title}\" if title else f\"{code}\")\n",
    "        anchor_str = \" | \".join(anchors) if anchors else \"-\"\n",
    "\n",
    "        u_disp = c.get(\"u_name\") or c.get(\"u\") or \"\"\n",
    "        v_disp = c.get(\"v_name\") or c.get(\"v\") or \"\"\n",
    "        w_disp = c.get(\"w_name\") or c.get(\"w\") or \"\"\n",
    "\n",
    "        line = f\"- {u_disp}{arrow1}{v_disp}{arrow2}{w_disp} [ICD-9: {anchor_str}]\"\n",
    "\n",
    "        # de-dup exact lines to avoid spam\n",
    "        if line in seen_h2:\n",
    "            continue\n",
    "        trial = \"\\n\".join(h2_lines + [line])\n",
    "        if count_tokens(tok, trial) <= h2_budget:\n",
    "            h2_lines.append(line)\n",
    "            seen_h2.add(line)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    h2_block = \"\\n\".join(h2_lines) if len(h2_lines) > 1 else \"\"\n",
    "\n",
    "    # ---------------- H1 block with relation labels ----------------\n",
    "    h1_lines = [\"[KG CONTEXT — H1 PATHS]\"]\n",
    "    seen_h1 = set()\n",
    "    for c in paths.get(\"H1\", []):\n",
    "        rel_lab = _rela_or_rel(c.get(\"rela\", \"\"), c.get(\"rel\", \"\"))\n",
    "        arrow = f\" --{rel_lab}--> \" if rel_lab else \" → \"\n",
    "        u_disp = c.get(\"src_name\") or c.get(\"src_cui\") or \"\"\n",
    "        v_disp = c.get(\"nbr_name\") or c.get(\"nbr_cui\") or \"\"\n",
    "        line = f\"- {u_disp}{arrow}{v_disp}\"\n",
    "\n",
    "        if line in seen_h1:\n",
    "            continue\n",
    "        trial = \"\\n\".join(h1_lines + [line])\n",
    "        if count_tokens(tok, trial) <= h1_budget:\n",
    "            h1_lines.append(line)\n",
    "            seen_h1.add(line)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    h1_block = \"\\n\".join(h1_lines) if len(h1_lines) > 1 else \"\"\n",
    "\n",
    "    # ---------------- glue + final clamp ----------------\n",
    "    combo = \"\\n\".join([b for b in (h2_block, h1_block) if b])\n",
    "    if count_tokens(tok, combo) > budget_tokens:\n",
    "        combo = trim_to_token_budget(tok, combo, budget_tokens)\n",
    "    return combo\n",
    "\n",
    "# ------------------------- prompt tail -------------------------\n",
    "def build_tail(N_max_terms:int) -> str:\n",
    "    lines = [\n",
    "        \"[TASK] List the final clinical diagnoses for this admission.\",\n",
    "        \"[FORMAT]\",\n",
    "        \"- One diagnosis per line\",\n",
    "        \"- Avoid abbreviations if possible\",\n",
    "        \"- No ICD codes or explanations\",\n",
    "        f\"- Maximum: {N_max_terms} lines\",\n",
    "        \"[OUTPUT]\"\n",
    "    ]\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e788e49",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# In [3]\n",
    "G, node_name, node_sem, cui_to_icd9 = load_kg(KG_NODES_CSV, KG_EDGES_CSV)\n",
    "print(f\"KG: {G.number_of_nodes():,} nodes, {G.number_of_edges():,} edges\")\n",
    "print(f\"CUIs with ICD9 anchors: {sum(1 for v in cui_to_icd9.values() if v)}\")\n",
    "\n",
    "with open(ICD9_PROC_MAP, \"rb\") as f: icd9_proc_map = pickle.load(f)\n",
    "with open(LOINC_MAP, \"rb\") as f: loinc_map = pickle.load(f)\n",
    "with open(ATC_MAP, \"rb\") as f:   atc_map   = pickle.load(f)\n",
    "\n",
    "with open(CODE2NAME_PKL, \"rb\") as f: code2name = pickle.load(f)\n",
    "profiles = load_profiles_json(ICD9_PROFILES_JS) if os.path.exists(ICD9_PROFILES_JS) else {}\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "if tok.pad_token_id is None:\n",
    "    tok.pad_token = tok.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04062677",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# In [4]\n",
    "try:\n",
    "    df = pd.read_pickle(DATA_PKL)\n",
    "except Exception:\n",
    "    with open(DATA_PKL, \"rb\") as f:\n",
    "        df = pickle.load(f)\n",
    "\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "i = 0   # try different indices\n",
    "row = df.iloc[i]\n",
    "row.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b720e5ea",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# In [5]\n",
    "src2cuis, ev_cuis = visit_evidence_cuis(row, icd9_proc_map, loinc_map, atc_map)\n",
    "print(\"Structured → CUIs\")\n",
    "for k,v in src2cuis.items():\n",
    "    print(f\"  {k}: {v[:10]}{' ...' if len(v)>10 else ''}\")\n",
    "print(\"Union ev CUIs:\", len(ev_cuis))\n",
    "\n",
    "notes_text = serialize_notes(row)\n",
    "\n",
    "paths = mine_paths_h1_h2(\n",
    "    G, node_name, node_sem, cui_to_icd9,\n",
    "    ev_cuis, notes_text,\n",
    "    rel_whitelist=None,   # you can set a whitelist set([...]) to restrict\n",
    "    rela_whitelist=None,\n",
    "    h1_per_src=3,\n",
    "    h2_per_h1=3,\n",
    "    prefer_dx_like=True\n",
    ")\n",
    "\n",
    "print(f\"H1 paths: {len(paths['H1'])}, H2 paths: {len(paths['H2'])}\")\n",
    "print(\"Sample H2 entry:\", paths[\"H2\"][0] if paths[\"H2\"] else \"(none)\")\n",
    "print(\"Sample H1 entry:\", paths[\"H1\"][0] if paths[\"H1\"] else \"(none)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c120aee1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# In [6]\n",
    "kg_text = render_kg_context_paths(\n",
    "    tok, paths, code2name, profiles,\n",
    "    budget_tokens=KG_HINT_BUDGET,\n",
    "    h2_first_ratio=0.7\n",
    ")\n",
    "print(kg_text[:1500])\n",
    "print(\"\\n[KG CONTEXT tokens]:\", count_tokens(tok, kg_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12236f37",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# In [7]\n",
    "header = f\"[VISIT] subject_id={row.get('subject_id_x','?')} hadm_id={row.get('hadm_id','?')}\\n\" + serialize_structured_readable(row)\n",
    "\n",
    "# leave room for KG + tail\n",
    "tail = build_tail(N_MAX_TERMS)\n",
    "room_for_notes = MAX_LEN - (KG_HINT_BUDGET + count_tokens(tok, header) + count_tokens(tok, tail) + 32)\n",
    "notes_trim = trim_to_token_budget(tok, serialize_notes(row), max(0, room_for_notes))\n",
    "\n",
    "raw_prompt = \"\\n\".join([p for p in (header, notes_trim, tail) if p])\n",
    "kg_prompt  = \"\\n\".join([p for p in (header, notes_trim, kg_text, tail) if p])\n",
    "\n",
    "print(\"RAW prompt tokens:\", count_tokens(tok, raw_prompt))\n",
    "print(\"KG  prompt tokens:\", count_tokens(tok, kg_prompt))\n",
    "\n",
    "print(\"\\n=== RAW PROMPT (head) ===\\n\", raw_prompt[:1200])\n",
    "print(\"\\n=== KG  PROMPT (head) ===\\n\", kg_prompt[:1400])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
